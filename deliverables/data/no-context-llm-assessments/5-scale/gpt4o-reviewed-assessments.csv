paper_id,comment,analyst,sentiment
2795,"This manuscript was submitted as 'full paper' and should be reviewed along the usual dimensions for research contributions which include (1) originality, (2) significance of the results, and (3) quality of writing. Please also assess the data file provided by the authors under “Long-term stable URL for resources”. In particular, assess (A) whether the data file is well organized and in particular contains a README file which makes it easy for you to assess the data, (B) whether the provided resources appear to be complete for replication of experiments, and if not, why, (C) whether the chosen repository, if it is not GitHub, Figshare or Zenodo, is appropriate for long-term repository discoverability, and (4) whether the provided data artifacts are complete. Please refer to the <a href=""/swj/reviewers"" target=""_blank"">reviewer instructions</a> and the <a href=""/swj/faq"" target=""_blank"">FAQ</a> for further information.

The authors present a compact, focused experiment on applying ShEx validation to libraries' datasets to foster data re-use, with four exemplifying use cases on datasets provided by three individual libraries (and one non-library data). The presented methodology is quite straightforward application of ShEx. From purely technological perspective, the originality and significance of the contribution is not particularly high, but especially for researchers and practitioners working with (linked) data in GLAM institutions the paper would be relevant.

Compared to the previous version of the paper: the authors have made improvements on the paper, extending it sufficiently on sections that needed further discussion. The comments I made in my previous review have been addressed sufficiently. The quality of the writing is good.

The data file provided by the authors under “Long-term stable URL for resources” (A) is well organized and contains a README file, (B) appears to be complete for replication of experiments (based on the README file, file listing, and looking at couple of individual data files), (C) is stored on Zenodo, and (4) appears to provide complete data artifacts (based on the README file, file listing, and looking at couple of individual data files).

I have one comment:

- Page 14 ""Regarding the NLF dataset, a common problem is related with the property rdf:langString used for language-tagged string values that are validated against xsd:string"" - In such cases, why did you constrain the property value's datatype to ""xsd:string"" - instead of ""LITERAL"" (https://shex.io/shex-semantics/index.html#shexc) in the ShEx definition? For example, concerning NLF dataset's class Person, you have constrained the property schema:name's value to xsd:string (https://github.com/hibernator11/ShEx-DLs/blob/1.1/nlf/nlf-person.shex#L12). In the NLF data model the range of schema:name is defined as ""Literal"" (https://www.kiwi.fi/display/Datacatalog/Fennica+RDF+data+model), and in the schema.org vocabulary the range of schema:name is defined loosely: ""schema:name schema:rangeIncludes schema:Text"" (https://schema.org/version/latest/schemaorg-current-https.ttl). I would suggest loosening the constraint.

Minor remarks:

- Page 14: ""Table 6 provides an overview of the data quality evaluation. All the assessed repositories obtained a high score, notably the BNB and the BnF."" - Based on Table 6, NLF obtained as high score (mconRelat) as BnF. Mention NLF as well?

- Page 14: ""Regarding the NLF dataset, a common problem is related with the property rdf:langString used for language-tagged string values that are validated against xsd:string"" - rdf:langString is not a property, but a datatype.","gpt-4o-2024-11-20, Jon Wasky",'slightly positive'
2801,"This is the second revision of the article “Urban IoT Ontologies for Sharing and Electric Mobility”. 

The paper describes a modular suite of ontologies representing data gathered from Urban IoT devices used in urban mobility, finally producing a conceptual model to harmonize data exchanges between municipalities and service providers, with a specific focus on sharing and electric mobility domains. The paper also describes the methodology followed for the development of the ontology and contains a set of examples and references to additional materials to better understand installation, queries, and evaluation of the model.

In the previous revision I emphasized the clarity of the exposition, and the relevance of the covered topics. In fact, no major changes to the structure of the paper were required, since it was already well organized. As far as the writing is concerned, therefore, authors basically corrected the typos and missing references that had been highlighted in the review. 
However, in my previous review, I underlined some shortcomings in the section of result evaluation dealing with Completeness. The authors have now added  the file CompetencyQuestion_Completeness.xlsx to the repository, including numerous Competency Question examples on most of the classes of the ontology. This is indeed a useful tool to test the Completeness of the model, therefore my only advice is to replace the full file with a link to a google sheet in read-only mode. I suggest this because from git hub it is not possible to view an xlsx file directly, unless you install git hub desktop: you can only download the excel locally, which may not be the ideal solution for users and information security. 
As for the long-term stable URL for resources, these are all organized in the git hub repository, introduced by a bilingual read_me. Moreover, the subfolders contain read_me description in English and/or properly commented code. The contents of each file are clear and consistent with the descriptions. I give a more than positive assessment to the material provided, and the same suggestion made for the  CompetencyQuestion_Completeness.xlsx file applies to all the other xlsx files. 
This new paper also complies with all my other suggestions.

In conclusion, my final recommendation is to accept the paper. Indeed, such an example of integration of ontologies and the use of existing vocabularies meets the needs of the users it refers to and clearly describes the context of reference, resulting in a good quality and relevance model. Moreover, its modularity also allows for future extension. 
Therefore, although its originality is not extraordinary, I appreciate not only the effort at the technical level, but also the detail and care in organizing heterogeneous data in the repository, and in explaining the problems and challenges faced and the results obtained. In fact, the work is complete and well documented.

","gpt-4o-2024-11-20, Jon Wasky",extremely positive
2804,"Prediction of Adverse Biological Effects of
Chemicals Using Knowledge Graph Embeddings

(1) originality:
This manuscript provides details of the improvements built upon authors’ previous publications on a knowledge graph in Ecotoxicology domain.

The paper starts with explanation of ecotoxicology definition, and its importance. Challenges of datasets related to the field are listed as interoperability from various data sources.
Knowledge graphs(rdf) and semantic web technologies are suggested as a solution of orchestration of these datasets.
For the sake of completeness the manuscript provides details. On the other hand, this makes following the paper difficult, especially in the methods part. The general quality of the manuscript is appropriate.
Main contribution of the work is investigation of KG embedding methods and adding new datasets to previously published KG.  The overall quality is acceptable.

(2) significance of the results,
The manuscript provides appropriate details on KG embeddings on proposed KG. The details of the results are enough.

(3) quality of writing:

The manuscript is acceptable in terms of writing quality.

Contributions of the work:
1. Consolidation of relevant information to ecotoxicology domain as knowledge graph. Integration includes tabular data, ref files, sparql queries over public linked datasets such as Wikidata and log map.
Biological :Ecotox, 1M experiments, 12K chemicals, and 13kK species.
Chemical : Ecotox,Wikidata pubchem, chembl mesh,
Taxonomy : Ecotox, NCBI
Species Traits Enc. of Life,

2. Implemented a prediction model using MLP (multi)and KG embedding models are presented.

3. Manuscript investigates prediction performance of various embeddings namely
Decomposition Models : dismay, complEx, Hole
Geometic Models: TransE, RotatE, pRotatE, HAKE
Convolutional Models: Cons KB, ConvE.

","gpt-4o-2024-11-20, Jon Wasky",'neutral'
2816,"The manuscript “LegalNERo: A linked corpus for named entity recognition in the Romanian legal domain” is a data description paper. It describes the LegalNERo dataset, which is a manually annotated corpus for named entity recognition in the Romanian legal domain. The dataset is made available through Zenodo and also through a SPARQL endpoint hosted on a server of the research centre in which the dataset was developed. The dataset itself is available in a number of different formats including BRAT, RDF Turtle and CoNLL-U Plus.

The paper follows the typical structure of a dataset paper (Introduction, Related Work, Annotation Process, Corpus Description, Using the RDF Version, Corpus Usage, Conclusions). All in all, the descriptions are sufficient to enable re-use and adaptation of the dataset. 

There are, however, a number of issues with this data description paper. 

First and foremost, surprisingly, while the title of the paper implies a certain relationship to the legal domain, neither the paper nor the dataset specifically tackles any aspects related to NER in the legal domain except for adding one additional entity category to the typical person, organisation, location structure and that’s “legal resources” or “legal document references”. In the Related Work section the authors do cite and acknowledge the many different types of entities that are specific to the legal domain but they decided not to introduce any of these specific types or categories themselves (except, as mentioned, “legal document references”). This decision needs to be explained and motivated since the paper and especially the dataset claim a certain focus on the legal domain, which does not exist except for the documents used, which are indeed from the legal domain. The size of the dataset is rather small (370 documents with 265k tokens, 8k sentences and a total of 54k annotated tokens).

In various parts of the paper “time” is mentioned as an entity type. Time expressions are simply time expressions. As explicit time expressions can be recognised easily with a number of regular expressions (like currency expressions), they are often used/implemented in NER tools but it’s simply incorrect to call time expressions “named entities”.

Some additional comments:

Page 1, line 27: suggest to change “international project” to “EU project”.

Page 1, line 29: please explain “comparable” in the context of this paper (or delete it)

Page 1, line 30: “7 languages” should be written as “seven languages”. There are other sentences in which one-figure numbers are written as actual numbers – these should all be changed into the corresponding words (see, among others, page 2, lines 10 and 20).

Page 1, lines 38/39: “All these annotations were realised using automatic processes.” My understanding of the paper is that all annotations were performed by human annotators. 

Page 1, 2, lines 45 ff.: The relevance of this paragraph in Section 1 is unclear. It should probably be moved into Section 2.

Page 2, lines 21 ff.: Section 2 is missing in the summary paragraph.

Page 2, line 23 ff.: Please include a link to the annotation guidelines or include the annotation guidelines in the dataset on Zenodo.

Page 3, lines 1/2: I don’t understand why hiding certain information about the annotation process helps with the computation of the inter-annotator agreement.

Page 3, lines 15 and 32: It’s “Cohen’s Kappa” (not Coehn’s Kappa)

Page 3, line 17: The Cohen’s Kappa of 0.87 is surprisingly low given that the annotation task is so simple. The revised metric (0.89) is still low so where exactly are the actual disagreements?

Page 6: In Section 6 the development of various NER models is mentioned but the evaluation of these models is missing.

Finally, the paper is in need of a thorough round of revisions, there are many typos, missing words etc.","gpt-4o-2024-11-20, Jon Wasky",slightly negative
2816,"This manuscript was submitted as 'Data Description' and should be reviewed along the following dimensions: Linked Dataset Descriptions - short papers (typically up to 10 pages) containing a concise description of a Linked Dataset. The paper shall describe in concise and clear terms key characteristics of the dataset as a guide to its usage for various (possibly unforeseen) purposes. In particular, such a paper shall typically give information, amongst others, on the following aspects of the dataset: name, URL, version date and number, licensing, availability, etc.; topic coverage, source for the data, purpose and method of creation and maintenance, reported usage etc.; metrics and statistics on external and internal connectivity, use of established vocabularies (e.g., RDF, OWL, SKOS, FOAF), language expressivity, growth; examples and critical discussion of typical knowledge modeling patterns used; known shortcomings of the dataset. Papers will be evaluated along the following dimensions: (1) Quality and stability of the dataset - evidence must be provided. (2) Usefulness of the dataset, which should be shown by corresponding third-party uses - evidence must be provided. (3) Clarity and completeness of the descriptions. Papers should usually be written by people involved in the generation or maintenance of the dataset, or with the consent of these people. We strongly encourage authors of dataset description paper to provide details about the used vocabularies; ideally using the 5 star rating provided here <http://www.semantic-web-journal.net/content/five-stars-linked-data-vocabulary-use>. Please also assess the data file provided by the authors under “Long-term stable URL for resources”. In particular, assess (A) whether the data file is well organized and in particular contains a README file which makes it easy for you to assess the data, (B) whether the provided resources appear to be complete for replication of experiments, and if not, why, (C) whether the chosen repository, if it is not GitHub, Figshare or Zenodo, is appropriate for long-term repository discoverability, and (4) whether the provided data artifacts are complete. Please refer to the <a href=""/reviewers"" target=""_blank"">reviewer instructions</a> and the <a href=""/faq"" target=""_blank"">FAQ</a> for further information.



Summary:
The paper presents a manually annotated corpus LegalNERo for named entity recognition in the Romanian Legal domain. The authors considered classical entity types such as the organizations, persons, locations, time expressions together with legal references to the documents such as laws, government decisions, orders, etc. The corpus consisting of 370 documents are manually annotated by 5 human annotators. Each annotator was assigned 100 documents, out of which 30 documents were shared with two other annotators. Inter-Annotator Agreement (IAA) between each pair of annotator has been calculated using Coehn’s Kappa measure. The corpus is made available as raw text, span-based annotations, token-based annotations, and linked data RDF. 
The idea of NER in the legal domain is interesting, however, there are certain concerns:

1. The paper lacks a running example which reduces the readability and understandability of the proposed work. I would recommend the authors provide an excerpt from the original document and show the annotated named entities in the text. (Probably, with the corresponding English translation of the sentences and the annotated named entities because the original documents are in Romanian).
2. The same example or a different example should be used to explain the span-based and token-based annotations.
3. Since, the authors provide the raw corpus as well it would be recommended to have a comparison of the accuracy, precision and recall measure of the annotations using the existing NER tools. This would further help to motivate the need for manual annotation of Legal corpus.
4. The RDF linked data needs further explanation along with Fig 1. The domain and range of the RDF schema are not explained. Also, the different classes are not explained well. For e.g., what does powla: Node denote? Give an example which also would explain its properties hasParent, next, etc. Therefore, I would highly recommend adding a section in the paper explaining each class, the properties and hierarchy of the classes. Furthermore, the SPARQL endpoint link provided in the webpage doesn’t work, however, the link with the example SPARQL queries work. The authors might need to cross-check that.
5. However, the end goal of having the data in form of Linked Data is missing. How this data can be used? What kind of interesting insights can we get from the data. The SPARQL queries provided are basic ones and it fails to give any insights into the data.


","gpt-4o-2024-11-20, Jon Wasky",slightly negative
2819,"In this paper, authors introduce a novel approach which encompasses the aspect of interoperability to quality assessment of RDF data by identifying eleven interoperability dimensions by aligning a list of standardized dimensions regarding data quality to the “I” principles out of other FAIR data principles.

 

Authors also presented a tool by claiming its benefit for successfully identifying interoperability problems and could provide suggestions for resolving them.

 

It is a well written article and easy to read but following few points can be considered:

 

·      There is no discussion around the validation and evaluation of the approach. It would be great to see the precision and recall of the defined approach especially in terms of complexity analysis and accuracy of the approach as the size of the data grows i.e larger datasets.

 

·      As a proof of concept authors performed the quality assessment on a registry dataset about Addison’s disease and two synthetic datasets respectively about personal information and diagnosis from the European Joint Programme on Rare Disease (EJP RD). Addison's dataset was converted from a synthetic tabular dataset to an RDF dataset claiming the conversion scenario as typical/ useful for quality assessment.

Typically, in most of the cases converting relational databases/ tabular databases to RDF requires a good amount of domain knowledge especially when it comes to medical/ biomedical data sources. Considering the inherent feedback mechanism from domain experts during the conversion process the quality assessment and the results may differ on a case to case basis.

How realistic would it be to comment on the quality dimension based on metrics defined in this paper at the generalised level i.e, on other RDF datasets. Would the same approach, dimensions and metrics work for any RDF resource?

Nevertheless, it would be great to see results in real world settings (on existing RDF data sources) irrespective of domain, complexity, size and coverage in order to consider the quality metrics defined in this paper for interoperability dimensions.

 

·      Most of the data sources are dynamic in nature these days. Does any of the quality metrics defined for interoperability dimension consider the assessment for such scenarios?  

 

·    Twenty-nine failure types were defined in the Interoperability Failure Case (IFC) vocabulary which seems comprehensive but it is not explicitly clear if IFC is one of the contributions by authors or the re-use of existing vocab named “IFC”.

 

·      Would be good to see the false positive results while considering the failure matches based on the defined technique. Any analysis based on computed results would be great to see.

 

·      Some of the possible future directions are highlighted in the last paragraph of section 5 and would be good to present them as separate sections/ subsections for better visibility.

 

·      Clear statements regarding the limitation for the approach, data used and the developed tool should also be highlighted.","gpt-4o-2024-11-20, Jon Wasky",neutral
2819,"This paper present an approach to quantify interoperability of datasets using metrics that relate to existing frameworks, notably FAIR and related quality measurement (and representation initiative).

The approach is interesting. In particular the authors do a good deal of relation to existing work, making their proposal a nice fit into a series of other proposals. 

However I am convinced that the work is not yet mature enough for a journal publication, especially SWJ.

First, as a general comment, I must say that I do not buy entirely the parlance in the introduction about the ""fitness for use"" vs ""fitness for exchange"". In the end a lot of the interoperability metrics proposed in table 3 are about general data issues, sometimes very low level (like misused types, or existence of contradictions), so it's hard to identify whether interoperability is something really specific. From the paper it seems that every quality dimension can contribute to interoperability, going beyond what is in scope for I1, I2, I3. This is perhaps true, but then it makes the ground of the paper (especially table 1) a bit shaky. I guess here it is the 'dissection' of 3.3 that would need more substance in the paper.

In a similar line of work of the authors' work is not easily accessible/visible. Where are ""The dimensions were represented in RDF"" (in 3.3)? Is it in section 4.1? Maybe, but then I do not see the point in making such a separation in the paper. The generic title of section 4 ('overview of the approach') doesn't help. In fact the paper is apparently not really consistently written, in the sense that it announce (relevant) pieces of work in some parts but they are hard to identify in others. For example section 3 announced that some metrics were re-used from Zaveri, and DQM, but this is not visible later, e.g., in table 3.

Then it is a bit frustrating to find that this variety of dimensions is not proven by implementation work. Notably, dimensions that are notoriously hard to assess (semantic accuracy and trustworthiness) are not implemented in the current state. Readers should see more evidence that these areas can indeed be implemented to prove the author's inclusion of them in their framework.

More worrying is the lack of large scale experiment on real data. It is good that the authors apply their approach on three datasets. But only one is real. I am not sure why a journal paper should have only one real application, especially when it seems quite easy to apply the developed framework to other real dataset. 

Finally, the resources built around the papers are full of RDF issues that indicate lack of maturity and review. Things that may not be formal errors, but that will not help the readers see the proposal in a good light. Basically things that would make the proposal more interoperable!
Notably it would be really good if the authors could follow consistently good practices for naming classes (starting with upper case) and instances (with lower case). In the IFC vocabulary, ifc:failureCase is a lower-cased class and all its instances are upper-cased - well, one is not (ifc:mailformedLiterals, by the way there's quite a typo here!) which looks even more odd. 
Giving a good thought about these conventions would perhaps help avoid issues with the use of classes instead of instances. Especially, in IQM, all metrics are formally defined as classes, which is not in line with DQV. I.e. iqm:objectiveMetric a dqv:Metric
is a correct statement, making iqm:objectiveMetric an instance. 
But then 
dqm:MisplacedClassesOrPropertiesMetric rdfs:subClass iqm:objectiveMetric
renders both dqm:MisplacedClassesOrPropertiesMetric and iqm:objectiveMetric to be RDFS classes, which is not in line with DQV, and actually does not make much sense in general.
Plus, this file does not use the right RDFS property (it is rdfs:subPropertyOf).
In the same line, dqv:isMeasuredOf is not a DQV property (https://github.com/sxzhang1201/Interoperable-Supportive-Tool/blob/main/shex/report.shex)
and in the IFC file (https://github.com/sxzhang1201/Interoperable-Supportive-Tool/blob/main/vocab/interoperability-failure-case.ttl)
The title of the vocabulary is ""Interoperable Failure Case"" not ""Interoperabality Failure Case"" as it should be.
I also do not get why ""interpretions"" in table 2 are expressed with dct:description in IKP, skos:definition in IQD and rdfs:comment in IQM. Interoperability would probably require using one property here. Or deviations should be explained.
It could be that there are other technical problems. Frankly I have stopped checking at this stage, because I feel that this is work that the authors should do. Or reviewers of a smaller chunks of the authors' work that would be submitted to workshops or conferences, allowing the whole framework to mature at a more regular rhythm!


Minor comments:
- in 3.1 the paper hints that the notions of quality dimensions and metrics originate in Zaveri et al, but these have been also formalized in the daQ work (Debattista et al), which I think predates it.
- ""materials"" is a strange title for 3.2
- 3.1 could be presented before the related work.
- in 3.2 I am not sure it's right to call LDQ the ""linked data version of DQV"". Granted, it introduces some linked data quality aspects, but a lot of its contribution regards assessment processes which are not specifically LD-specific
- I do not understand how RDFS can be used for ""representing annotations"" in 3.4. RDFS is very generic...
- I do not understand the real value of using ShEx in 3.6. It looks like the authors use it to control the data that makes a report. But because the authors control the way the report are generated, I don't see how the report data could fail to meet their expectations.
- in 37 one of the 'synthetic' is written 'synetic'","gpt-4o-2024-11-20, Jon Wasky",'slightly negative'
2830,"This is the first revision of the manuscript I am reviewing.  Therefore, I am judging only the manuscript itself, not the comments on the revision.

This manuscript presents COBRA, an approach for storing RDF archives in a way that supports several versioned query patterns (introduced in Section 2.1).  The authors' previous OSTRICH implemented, which already serves the same purpose, is extended from unidirectional delta chains to bidirectional ones in an attempt to reduce both storage size and ingestion time.  This is measured in a number of evaluations using the BEAR benchmark.  The hypothesis cannot be confirmed universally, but in a majority of situations.

Section 1 briefly introduces the problem.  Section 2 introduces the basics of versioned queries, different storage strategies and discusses related work.  Related approaches are classified w.r.t. storage strategy.  Section 3 states the problem and phrases the research hypotheses.  Section 4 introduces the bidirectional delta chain approach, for the case of subsequently ingesting additional versions during the lifetime of a dataset, and for the case of ingesting all past versions of a dataset at once, the latter of which can be performed out of order.  Section 5 presents the results of evaluating OSTRICH and COBRA in different BEAR settings.  Section 6 draws conclusions, also providing concise guidance w.r.t. what storage approach to employ in what practical setting.  The implementation is open source and accompanied with everything needed to reproduce the experiments.  At least it seems so – I did not try.  Just the creation of the BEAR input datasets is not exactly reproduceable, as the evaluation scripts assume that the respective data already exists on a server donizetti.labnet.

The following aspects need improvement  (cf. the annotated PDF at https://www.dropbox.com/s/bs96w03ab2ikiu2/swj2830.pdf?dl=0 for details):
* Algorithm 2 is introduced as showing the fix-up algorithm.  However, that's actually what Algorithm 1 shows.  
* Algorithm 2 is said to assume that n is even, but the shown implementation, which uses Math.floor, does not.
* There are multiple references to the OSTRICH article [5].  These references would be easier to use if they pointed to specific individual sections of that article.
* In Section 4.6.2, the text about delta materialization says that ""the results from the two queries are sorted"".  Why are they sorted, and by what? Do you mean that the (unsorted) results of the first query come first, and then the (once more unsorted) results of the second query?
* In Section 4.6.3, Version queries are defined as ""results being annotated with the version in which they occur"".  However, is the version always unique?  (That's what this phrasing seems to assume.)
* The setting shown in Subfigure 2.2 does not involve bidirectionality.
* In Section 5.2, why do you restrict your scope to ""at most two delta chains""?  In other words, does this not mean that you assume that the threshold for a chain that's ""too long"" is ""(number of versions) / 2""? Would your approach not perform even better with more delta chains?
* In Section 5.3.1, what do you mean by ""ingesting a raw representation""?
* Regarding Figure 3: Before reading your reminder about the reverse order of ingestion on the next page, it's hard to understand that we have a zero value in the middle. You could facilitate understanding by showing an arrow that indicates the order of ingestion.
* clarity of phrasing (cf. PDF annotations and comments)
* multiple minor linguistic issues (cf. PDF)","gpt-4o-2024-11-20, Jon Wasky",neutral
2832,"The application report systematizes the findings from more almost two decades of research on the Sampo portals, which focus on the domains of cultural heritage (CH) and the digital humanities (DH). As the main conceptual contribution, the author proposes (1) a set of design principles for semantic portals and (2) a description of three technology generations. The article also provides an excellent entry point to the literature on the Sampo portals. 

The article extends a short paper published in the DH Nordic conference proceedings. There is little overlap with this prior publication, however. Both texts present an overview of selected Sampo projects, but the article covers more projects and gives additional technical details. More importantly, the discussion of the design principles does not appear in the short paper. The submitted article is also sufficiently independent from other publications on the Sampo model to justify its publication.

As an application report, the submission is not typical since it describes several related semantic portals instead of a single system. This raises the question of the common technological ground of the portals, which the article addresses convincingly by formulating the Sampo design principles. The author mentions that the Sampo model “has evolved gradually over time” but does not not discuss how the principles evolved over time. Nevertheless, it seems possible to extract a timeline by combining the information given in section 3 on individual projects with the dates from table 2. 

P1 (= data linking) in 2004, MuseumFinland
P2 (= shared ontologies) and P4 (= multiple perspectives) in 2008, CultureSampo 
P6 (= separation of data services and UI) in 2015, WarSampo 
P5 (= faceted search) and P3 (= knowledge discovery) in 2019 BiographySampo

I am not sure whether this timeline is correct. In any case, it would be interesting for the reader to learn in which order the principles have been adopted and how the principles map onto the three technology generations of semantic portals. P1 and P2 are general principles implemented by virtually all LOD projects – not just the Sampo portals. That makes them candidates for first generation systems (data aggregation and exploration). In contrast, P3 is the principle introduced most recently and least explored in the Sampo portal series. P3 also marks ""the next step ahead"" in research, that is, the third generation of semantic portals (knowledge discovery and AI). If such a correspondence between the principles and the technology generations exists, the article should describe it. Optionally, the principles could be renumbered to reflect their chronological position. The current P3 would become the new P6, for instance. 

While this is my main observation, there are minor points that would profit from further clarification.

(1) The author mentions related design principles such as FAIR, but I missed a discussion of the nature of this relation. This is especially relevant with respect to the four LD principles and the 5 star principles. Are P1 and P2 equivalent to the four LD principles? Where exactly do the Sampo principles go beyond the LD or 5 star principles? The four LD principles refer to specific technologies whereas the Sampo principles are stated in technology agnostic form. Do the Sampo principles intend to cover non-RDF knowledge graph technologies as well?

(2) The Sampo model has been designed for CH and DH applications. It is yet difficult to see how the principles reflect that origin. I asked myself whether different principles would have emerged from a series of projects focusing on, say, genetic epidemiology. To put it differently: what in the Sampo model is specific to the way humanity scholars collect, systematize and analyze historical sources? The article should mention if the design principles of the Sampo model address research challenges in the humanities.

(3) With 20 years of operation, the Sampo series of semantic portals provides a unique opportunity to study issues of LD preservation. Have there been lessons learned beyond the fact that data needs to curation? What are the challenges (and costs) of maintenance of the portals? 
The application report is well structured, points to the relevant references and gives an adequate discussion of related work. The author clearly succeeds in demonstrating impact, which is a central requirement for systems described in a SWJ application report. There are few LD systems in the domains of CH and DH that have attracted a larger user base over such a long period than the Sampo portals.

In summary, I believe that all points raised can addressed by minor revisions.


Open Science Data

The SemanticComputing Github provides the long-term stable link to the resources. Some of the projects mentioned in table 2 do not appear in this Github, however. In addition, an access link is missing to some of the semantic portals or the link given in the footnote is defective. This should be resolved before publication. 

Not in the SemanticComputing Github
MuseumFinland, CultureSampo, BookSampo, Norssit Alumni, Mapping Manuscript Migrations

no link given
Norssit Alumni, U.S. Legislator Proso[po]grapher

404 error
WarVictimSampo (footnote 28), AcademySampo (footnote 31)


Typos

Please, check carefully for spelling errors, as the following list is incomplete.

page 1, line 23, abstract : War[V]ictimSampo
page 2, line 41: infra[structure]
page 4, line 35 : m[o]del
page 7, line 14, table 2 : Proso[po]grapher

","gpt-4o-2024-11-20, Jon Wasky",'slightly positive'
2833,"Despite the consideration made by the authors in the cover letter, this paper must be reviewed as ""Tools and Systems Report"". The paper is far from a full research paper and does not add relevant scientific contributions to the Semantic Web field; instead, it uses existing tools to create an event-based knowledge graph.

The topic addressed in the paper is very relevant, timely and well- introduced and motivated, inspiring its reading. On the other hand, the related work section is long, does not explain the paper's contributions in relation to the state-of-the-art and is difficult to follow.

The authors mention ""first step for building a knowledge graph was to decide the source of the documents, since there are important differences among jurisdictions, even when they share the language"". A suggestion to improve readability is to provide an example. I can only guess what the differences are.

The authors also mention ""From the analysis performed in the EventsMatter corpus, we can confirm the importance of the sections in identifying which events are relevant and which are not"". It would be better if the authors would let the readers 'confirm' by providing in-depth analysis and supporting data. The figures presented do not seem enough to confirm the importance of the sections rather than the frequency of facts.

The authors also mention that the ""Structure Extractor is currently able to handle the structure of the ECHR and ECJ documents, but in such a way that a new document type can be easily added"". Looking at the code and the lack of documentation/comments, it does not seem trivial to change a single line in the code provided. Paths are hard-coded, filenames are hard-coded and no design pattern was found to handle the extensibility claimed in this paragraph. An example is the method ""parseAndTag"". In general, section 3 is confusing and terms are mentioned without a proper introduction. For example, the authors say the EventsMatter corpus and only in a subsection it is 'properly' explained (same for the FrameNet). Section 3 requires restructuring. As mentioned before, a running example would help the authors to explain the process.

The presented ontology is also not well-explained. For example, the class ""ComposedTemporalExpression"" can be represented in other ontologies. Simplification can lead to other issues. Again, examples would allow readers to see the 'importance' of such class. 

The remaining sections suffer from similar issues.

Another main concern about the paper is the evaluation of the tool. The authors did not provide any evaluation/experiments—a critical point for a tool.

As previously mentioned, the code is available but not easy to run,  lacks documentation and coding standards are non-existent (https://www.oracle.com/java/technologies/javase/codeconventions-contents.html).

","gpt-4o-2024-11-20, Jon Wasky",'slightly negative'
2833,"The authors propose a pipeline which, given a set of documents describing legal cases, generates a knowledge graph about the events mentioned in such cases. The different steps of this pipeline - rule-based event extraction, a time ontology, a conversion between XML and the time ontology, and the knowledge graphs are described. While this is an impressive amount of work and a well-thought combination of several approaches (partly already described in the authors' previous work), I see several issues with respect to the motivation, the paper structure and the overuse of technical details. Moreover, the suggested tools are not yet easily ready to use due to missing code documentation.

== Paper ==

1) Motivation: The paper starts with a reference to Wittgenstein about the importance of events which is kind of nice but also very high-level and not very close to your actual goals. After that paragraph, you immediately jump into the definition of event knowledge graphs. There is no proper motivation for why the legal domain plays an important role in knowledge modelling and what are the actual goals.

2) Intuition: Personally, it was not easy for me to follow the idea of the paper mainly because of my missing domain knowledge in the legal domain and the missing examples. Throughout the whole article, you are not giving a single example of a law case. Later, when mentioning the EventsMatter corpus, I kind of get an idea of what it is about, but it is still rather vague (even in Section 3.1, you hide the actual example behind ""Decision"" placeholders). I would appreciate seeing an extensive example (e.g., as part of the introduction) of what such documents look like and what you actually want to achieve by modelling them as knowledge graphs.

3) Overview: The article introduces a pipeline consisting of several components/tools and input and output data. A clear overview of them and their dependencies is not provided, though. Specifically, the ""WhenTheFact"" component is not even named before the caption of Table 1. I would suggest extending Fig. 17 and show/describe it even before Section 3 starts. Similarly, Fig. 7 should be shown at the beginning of Section 3. Then, it is more clear how the sub-components (structure extractor, the trained resources) are connected.

4) Use of the knowledge graph: At the end of Section 6 (i.e., too late for providing intuition), you mention several use cases for the extracted knowledge graph, e.g., specific queries (e.g., ""Give me cases ... where the driver was a man"") . Later in the conclusion, you, however give the impression that such queries are not yet executable given the lack of semantic annotation in the knowledge graph (""Once this is achieved, queries will be able to retrieve for instance the timeline of one actor's involvement in a case""). Clear information about what is possible and what is not yet possible would be great. To do so, you might even add a section that discusses the current possibilities of using your tools vs the current challenges and how to solve them in the future.

5) Technical details: The paper mentions many technical details during the ""approach"" sections. Examples include Java data types (""HashMap"", ""ArrayList""), Java classes (""readFrames.java""), file names (""events.ser"", ""frames.ser""), file types (""a txt file"") and libraries (""nltk"", ""framenet""). This distracts from the actual ideas and research that should be independent of the code. I would like to see more abstraction here. Implementation details should be provided in a different section.

6) Structure Extraction: The structure extraction Section 3.1. has four large figures which do not contribute much to the understanding and the approach behind the structure extraction. The actual method behind that subtask is conveyed only in two brief steps discussed at the end of Section 3.1, which do not actually explain how it works (""detects the structure and divides it into parts"" and ""looks for the most relevant section""). More detail about how this works is needed.

7) FrameNet training: You write that ""we found the most general ones [frames]"" ""to our task"". But what is the task here, and how did you proceed to select the frames?

8) FT3 Ontology: Before describing the developed time ontology, I would appreciate some clear examples and reasons why the existing ontologies are not sufficient for your specific case.

9) Legal knowledge graph: This section should provide information about a knowledge graph that can be extracted with your tools: the actual data used for the creation, the number of generated triples, example queries, etc.

10) Event types: Can you describe in more detail what are the ""procedure"" and the ""circumstance"" event types? How come some events such as ""marry"" (see Fig. 5) are seen in both types? What are you doing with this information; how do you decide which ""ft3:hasType"" you use in the end?

11) Simple sentences: At the beginning of Section 3.2.1, you mention that you generate ""simple sentences"", some information regarding specific words (""lodge""), and the inclusion of the ""They"" term. Without any examples, this procedure is rather unclear to me.

Figures: Several figures need an update:
- Fig. 1 - Fig. 4: no axis labels
- Fig. 2: y axis has too many overlapping numbers
- Fig. 7: What does ""WORD"" mean? Why the capital letters? I do not understand the ""NO"" branch from ""special case?"" to ""deppar"" and from ""has event?"" to ""deppar"".
- Fig. 8: too small`
- Fig. 9: too small

Minor:
- Abstract: ""acces[s]ible""
- Section 1: ""Different[ from]..."" (twice)
- 1: Explain ""journalistic event""?
- 1: ""corpus temporal annotation work""?
- 2.1.1: ""a[] specific""
- 2.1.1: You could remove the part about TIMEX2 if it not used anyway.
- 2.1.1: ""literature[ ][13, 14]
- 2.1.1: ""and focus[es] on""
- 2.1.1: ""MUC[ ][21]""
- 2.1.1: ""based [o]n"" (twice)
- 2.1.2: The section starts with ontologies about time but then also discusses events. Make that clear at the beginning of the section.
- 2.1.2: ""LKIF[ ][32]""
- 2.1.2: ""LegalRuleML[]19""
- 2.1.2: ""knowledge.[ ]It""
- 2.2: ""reviewed 150 events extracted 18 sentences from""?
- Fig. 2: ""Number[]""
- 3: ""Based on a previous work[]""
- 3.2.1: ""adding has generic subject 'they'""?
- 3.2.2: ""all [this] information""
- 4: ""On the [other] hand""
- 4.2: ""[All] these examples are discussed""
- 4.2: ""the problematic existing between""?
- 4.2: The link does not break the line.
- 5: ""in a different format[]""
- 5: ""in ou[r] ontology""
- Table 4: ""information contained different types""?
- 6: ""The junction ... allow[s]""
- 6: ""WhenTheFact process[es]""
- 6: ""events f[ro]m a specific year""
- 6: ""exploit the [k]nowledge""

== Code and Data ==

Your main contribution of the paper is the provision of services/tools. However, the website mainly focusses on demo use cases and the tools are not properly documented and structured to be ready to use for a broader audience (even though you state ""The code ... can be freely adapted"").

a) The GitHub repository (https://github.com/mnavasloro/FromTimeToTime/) does not have any documentation in the readme file.
b) The code documentation could be extended. 
c) There are no dedicated entry points to the tools (e.g., Main class with CLI input). The class oeg.eventextractor.Main is empty.
d) File paths are personalised (see https://github.com/mnavasloro/FromTimeToTime/blob/main/whenthefact-core/src/main/java/oeg/tagger/main/Main.java).
e) It would be good to have human-readable versions of the frames.ser and events.ser file so users can easily update it and use it in the code.
f) Resources are duplicated across projects (e.g., the events.ser).
g) The knowledge graph itself is not available for download.
h) The text about footnote 25 mentions that the zenodo link also contains the documentation, but it does not.

== Website ==

The website looks nice and gives a good impression of the work. But for the same reasons I mentioned about the code, also the website is important for the provision of tools and needs to be updated:
i) The ""QUERY"" part does not work (Error message: ""JSON.parse: unexpected end of data at line 2 column 1 of the JSON data"").
j) The GitHub link on the bottom links to the Annotador repository.
k) The Pipeline page (footnote 6) is not linked on the website. In general, I like that one can test the whole pipeline, but I don't think the result should be published in the public knowledge graph.
l) It would be nice to have an introductory paragraph about the whole idea/goal of the website before jumping into the tools (WHENTHEFACT etc).
","gpt-4o-2024-11-20, Jon Wasky",'slightly negative'
2846,"The paper presents a PyTorch implementation of the RGCN model, published in a 2018 paper. The authors replicate several experiments in the original paper to compare the results obtained with the first and the new implementation. Results suggest that the new implementation is a reliable implementation of the model. In addition, while re-implementing the model, the authors propose new approaches to optimize training using PyTorch as well as new variants of node classification and link prediction models based on RGCN

The paper has several merits but also limitations, which are listed below. 

PROS

-	Making it available a PyTorch implementation of RGCN is valuable. Convolutions in Knowledge Graphs are definitely valuable and interesting to exploit the connections in the graph to inform the representation of each node. Combining convolutions and other approaches (e.g., link prediction) is promising. 


-	The technical solutions proposed in the new implementation look (eventually) well-motivated and principled. 

-	The code page on GitHub is well organized and documented. 


 
-	The original model is complex as well as its usage in downstream applications. A more detailed description of the model is valuable for researchers. The paper is reasonably well-written (except for some improvements needed, as further highlighted below) 

-	The new implementation required a significant amount of work and effort, which means that there is a substance in this work, and the introduction of variants and tricks to improve training  


CONS

-	The main limitation of the paper as a full (research) paper submission lays in its novelty, being admittedly a new implementation of a method proposed before.

-	For a research paper, the evaluation has several critical points, especially in the link prediction settings. If the authors admit the limitations in this task compared with more recent Knowledge Graph Embeddings (KGE) approaches, why not trying to find a novel way to combine KGE approaches and RGCN? Why not discussing the comparison between results on node classifications and newer (if any) approaches evaluated on the same datasets? Is RGCN complementary to KGEs (optimized for link prediction) and, therefore, can RGCN be combined with more competitive models as of today in addition to DistMult, or is RGCN simply not helpful in link prediction? I would expect some more conclusive comparative results in a research paper (e.g., why not trying to combine RGCN with newer state-of-the-art approaches to demonstrate complementarity and impact? Why not comparing results with top results as reported in recent papers?). Instead, the reader does not come home with a clear message from the paper whether RGCN is helpful and/or complementary with respect to other KGE methods (e.g., for link prediction)

-	In a paper that focuses on implementation – thus being primarily motivated by practical concerns – avoiding the replication of experiments carried out before (on link prediction, where experiments with larger facebook data are omitted) because apparently too much resource-demanding is a significant limitation and requires further discussion (is it because the previous implementation worked with a bigger infrastructure, or is it because the previous implementation was more efficient?). Anyhow, limitations can be accepted provided that they are clearly discussed in terms of size (nodes, edges, number of relations) that can be somehow handled (as a function of the available infrastructure) by the current implementation. 

-	In a fast-pacing research field, more effort must be spent in convincing about the relevance of RGCN today in terms of impact. This requires a better related work section and stronger arguments (I am convinced that the model is relevant, but the paper must improve on its arguments). 

-	Some sections must be improved to help the reader understand the choices made at the right time (see detailed comments for such improvement). 

SUMMARY OF RECOMMENDATION

I do value the work described in this paper and believe it could deserve to be published. However, I do not think that the paper meets the novelty bar required for a full (i.e., research) paper in this journal. In order to be accepted as a research paper, I think that experiments must be extended along the directions suggested above, and a better comparison with state-of-the-art models must be provided. 

Otherwise, I think that this paper could be submitted as a system paper in the Semantic Web Journal without extending the experiments, after the recommended improvements in the text (I would have recommended a minor/major revision if the paper was submitted as a system paper). This is not to suggest shrinking the paper or removing the nice background sections; instead, I suggest just adapting the argument and apply the suggested improvements. I have already notified the editor about my recommendation for a system paper and, I am available to review such a submission to favor a faster publication process. 

Detailed comments. 

Terminology

“Relational graph” is not properly defined in the paper. We somehow know what we refer to with “knowledge graph”, and “graph” but the presentation switch between KG and relational graph without never really defining both and their relationships.  I think that more precise definitions are useful here to organize and structure better the introduction and also discuss related work (especially considered the two research streams related to KG embeddings (KGE) and graph neural networks, possibly considering multi-relational graphs - see below) 

Related work

The related work section is not much informative. From this section I would expect to be:

-	more convinced that RGCN is still competitive and useful today (there are few pointers to a few papers, but I am not sure this is enough in a research field that is running so fast)

-	informed about the essential differences between KGE methods and graph neural networks, and where RGCN is at (see comment on the terminology)
Overall I found the related work section far from being systematic and providing an insightful introduction to the role of the discussed model

References:

Some references lack proper details, e.g., [6] → please check all the bib entries 

In-line comments (please note that notes have been taken while reading; some questions emerged have found answers later on; I kept the notes to highlight that some explanations must be anticipated):

Page 3

“a matrix computed by row-normalizing4 the adjacency matrix of the graph G  ” → Does this use the in degree? 

Page 4
“and this called“ →  and is called

Page 9

Fig. 4: “Stacked of adjacency matrix” → Remove “of” 

Page 11

“we can prune away the unused nodes from the graph” → Ok but as far as the paper focus on the implementation, I think that it would be interesting to report about the scalability and which is the size of the graphs that can be handled. 

Page 12

Table 2: Since the original paper is from 2018, I think that in addition to showing that the results are comparable with the original paper you should also report results of top performing approaches on these datasets as of today. It is sufficient to track papers using these datasets (where possible) and report results as claimed in the original papers. At the end of the day, the reader would like to know - after reading this paper - how the model is still competitive today on this task.
 
We begin by sampling […] → I didn’t find this explanation very clear. Why the naïf approach where embeddings are computed with RGCN on the whole graph and then feed to initialize the embeddings of DistMult? I am sure there is a good reason but I think it is worth explaining the rationale behind the proposed choice 

“for each triple we generate 10 negative training examples  ” → Training for which model?    


Page 13

Fig. 6: “Can be seen” → Can be seen as

Used an input → Used as input

“Node embeddings E” → It is not clear what these embeddings refer to and how are they computed (with DistMult?) 

Page 14

Freebase (FB-15k and FB15k-237)  → It is not clear why changing these settings and using facebook toy. Is it because the implementation does not handle graphs as large as the ones used in the original paper? This is a potential limitation, and, again, I stress that having an idea of the graph size (nodes, edges, number of relations) supported by this implementation is important in this paper. 
(ADDITION: I found the reason to use the toy data later (but it should be anticipated here) - yet, it is not clear why the other larger datasets were not used). 

Page 15

“The link prediction architecture presented in [4] does not represent a realistic competitor ” → Ok, but please anticipate this at the beginning of this section. It would avoid plenty of questions in the mind of the reader. 

Page 18

“Traditional Knowledge Graph Embeddings (KGE) models, such as TransE and DistMult, lack the ability to perform node classification.“ →   Can you support this by pointing to some references? 

“The c-RGCN has several advantages over the regular RGCN link predictor: 1) c-RGCN does not require sampling edges, because it is able to process the entire graph in full-batch “ →  This observation comes too late, while this reason for adding edge sampling was not explained when sampling was introduced. 
","gpt-4o-2024-11-20, Jon Wasky",neutral
2846,"This paper presents and discusses a new implementation of the known Relational Graph Convolutional Networks (RGCNs) with the Deep Learning framework PyTorch. The authors reproduced the experiments of the original paper achieving similar results, released the source code of the new implementation, developed a more efficient algorithm for the message passage layer, developed two new configurations for RGCNs with less parameters and stated useful lessons learnt during this reimplementation.

The paper is well written and clearly shows that this new implementation brings similar results to the original one. Moreover, the authors extended the original version of RGCNs with a new one with less parameters and, therefore, with a faster training time. I also appreciated the lessons learnt about the small dataset and the parameter statistic between the new implementation and the original one. However, this manuscript was submitted as 'full paper' but, as the focus is on the implementation of a known architecture and the originality is limited to the parameters reduction (achieving lower performance with respect to the new implementation) and a new efficient algorithm for the message passing layer, I suggest to submit again the paper as a 'Reports on tools and systems' category. I think that the paper already satisfies the requirements for this category.

I also recommend addressing the following concerns mostly related to the presentation and the results.

The presentation could be improved by better describing the idea of a message passing algorithm and how it is implemented with the convolution operation in a Neural Network before the presentation of the equations. Examples would ease the comprehension of the main concepts. To this extent, I think the paper would benefit if lines 5-8 at page 4 and lines 12-15 at page 18 will be moved before Eq. 1.

The comparison of the two implementation would be stronger, and fairer, if the results for the link prediction task would include also the original datasets FB-15k and FB15k-237.

In addition, the appendix is short and therefore can be included in the rest of the paper. 

Regarding the results, I would include in tables 2, 5, 6 also the number of parameters of the models and the computational training time. I am aware that TF-RGCN and Torch-RGCN did not run on the same hardware and therefore are not comparable. However, this stresses the fact that Torch-RGCN is useful as it fully leverages all the GPUs potential. I would also include some results from the state-of-the-art such as the reader can better recognize that RGCNs for link prediction have good performance but are still too costly. I appreciate the idea underlying Table 4, however, comparing the two implementation using statistics is not easy nor effective. Statistical significance tests would be more effective in this perspective.

Other concerns:
• Page 1, line 38: older platforms -> old platforms
• Related work: I would expand this section maybe adding other works that use RGCN [1,2,3] or other works for KGE with different methods such as Neural Tensor Networks with Logical Knowledge [4].
• Page 4, line 23: can extended -> can be extended.
• Page 8, line 42: \mathcal{AX} -> AX.
• Page 10,  line 42: please uniform the notation K or C for the number of classes: K is used in Eq 10 but Fig 5 uses C.
• Page 12, line 50: Did you sampled the 30,000 edges from WN18? And how many samples from FB-Toy? Why this specific number?
• Page 13, line 21: used an input -> used as an input.
• Page 13, Fig 6: it is not clear where you took the relational embeddings. As they are not an output of RGCN, did you train another KGE? Are these embeddings provided with the datasets?
• Section 6.2: why did you use 7,000 epochs and no early stopping?
• Page 14, line 35: WordNet the graph -> WordNet, the graph

[1] Mylavarapu, S., Sandhu, M., Vijayan, P., Krishna, K. M., Ravindran, B., & Namboodiri, A. (2020, January). Towards accurate vehicle behaviour classification with multi-relational graph convolutional networks. In 2020 IEEE Intelligent Vehicles Symposium (IV) (pp. 321-327). IEEE.
[2] Hu, X., Fan, H., Noskov, A., Wang, Z., Zipf, A., Gu, F., & Shang, J. (2021). Room semantics inference using random forest and relational graph convolutional networks: A case study of research building. Transactions in GIS, 25(1), 71-111.
[3] Chen, J., Pan, L., Wei, Z., Wang, X., Ngo, C. W., & Chua, T. S. (2020, April). Zero-shot ingredient recognition by multi-relational graph convolutional network. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 34, No. 07, pp. 10542-10550).
[4] Donadello, I., & Serafini, L. (2019, July). Compensating supervision incompleteness with prior knowledge in semantic image interpretation. In 2019 International Joint Conference on Neural Networks (IJCNN) (pp. 1-8). IEEE.","gpt-4o-2024-11-20, Jon Wasky",slightly positive
2852,"1. Originality:
This paper introduces a neural network model where attention mechanisms are exploited to learn common object properties from the OWL2 ontology standard. 
Similar to what some of the co-authors proposed for the IterE model, this paper exploits axiom properties throughout its learning, but instead of learning rules guided by those properties, it uses those rules to implement an attention mechanism affecting the model embeddings.
In conclusion, it exploits recent deep learning techniques, trough attention mechanism, along with Ontology formalisms, which represents an original way of mixing deep learning and symbolic AI.

2. Significance of the results:
This paper provides tangible proofs about the value of ontologies' axiom information, in the learning process of knowledge graph embeddings.
Information on test results and the method used to produce them are well presented.
Results on the WN18RR with Hits@1 metric, being a very difficult task, are very good, and show potential advantage of this method for tasks where precision is needed in noisy (real life) settings.
The significance of those results is satisfactory but somewhat modest considering additional tests and evaluation would be needed to fully assess the level of improvement brought by this method on knowledge graph embeddings and its applicability to different type of datasets.
Nevertheless, it truly confirms the validity of the approach and applicability to certain use cases.

3. Quality of writing:
The quality of the writing is very basic, many sentences are missing words or using the wrong word, verb tense, etc., which makes reading unpleasant but does not affect the technical comprehension.

4. Paper Code/Resources:
The github reference was created but is empty (at least the public branch), which makes it impossible to review the solution code quality, reproduce results and confirm whether artifacts will be available.","gpt-4o-2024-11-20, Jon Wasky",neutral
2854,"This paper is about publishing Public Transport data in a cost-efficient and flexible manner using Semantic Web technologies (Linked Connections approach). The paper studies what are the performance tradeoffs for route query planning when publishing data and compares it with ""traditional"" implementation with a ""fat"" server instead of pushing some computation to intelligent clients.

DISCLAIMER: Is the first time I review this paper, that I see has been previously submitted. In line with what editors of this journal have asked me in similar situations, I read the paper with fresh eyes, without evaluating or assessing changes from previous versions.

(1) originality: The approach is original, there is enough difference with previous work from same authors. 

(2) significance of the results: Not groundbreaking, but in my opinion just sufficient. Indeed, the paper concedes that in its current form the approach is still far from practical for larger networks. Plus, Linked Connection's inherent bandwith overhead means that mobile apps, which I think are the most common client applications, would be slower and more expensive. That is relatively bad news for the SemWeb community, but still a valid scientific result vis-a-vis the methodology followed. 

(3) quality of writing: Can be improved, further comments below.

(4) Resources: GitHub repository, the repo also includes an external link to an institutional repository that I assume complies with the requirements of research data deposit.


Detailed comments:

Abstract: 

You say ""fragmentation size influences route planning query performance and converges on an optimal fragment size per network, in function of its size, density and connection"". From the results shown in section 6.1 I can't see where is that function. I was expecting you derived something F(S,D,C) -> Fragment Size.

Introduction:

Motivates well with respect to ""Open Data"", but jumps to ""Public Transport Data"" without explaining why the client-servers cost tradeoffs are important for that domain. What is wrong with current PT open data publishing? 

The contribution ""Shows how Semantic Web technologies can be applied not only to describe domain specific data, but also interfaces that enable applications to consume it, whose principles could be re-used towards more generic, domain-independent and autonomous data applications"" is quite fuzzy. I'm not certain what is mean with ""generic application"", or ""autonomous application"" and how what is presented here  helps to that. What is presented here is for the PT domain (as stated in the immediately previous sentence), therefore don't see how your contribution creates ""domain-independent"" applications.

Related Work:

In section 2.2 it is mentioned that the approach ultimately lowers the cost for data publishers. Please provide references to this, is there something where this has been quantified? I believe previous work from some of the authors have shown the load balancing part, but not the cost for publishers. I also wonder that in the context of PT, the publisher is usually the Transport agency or provider that has a mandate (or a business interest) in developing a client application too, how does the cost balance works there?

The same remark appears in section 3, where you mention the tradeoff of ""increased implementation complexity on the client"". You mention a mitigation strategy at that point, but this should be expanded in the discussion section.

In terms of contributions, I can see what a ""general architecture is"", but the adjective ""integrated"" does not add anything. 

""An study of the factors that influence route planning query performance"" it should be specified that is query planning under the data publication conditions imposed by your approach.

Section 3:

The AVL tree is nice, but it is part of the implementation, I don't understand why is considered part of the ""LC architecture"". If your architecture is ""general"", then the Live Data Manager is a component that does something, and the AVL is just your implementation.


Section 4:

Minor: ""set heterogeneous"" -> set of heterogeneous

The word ""observed"" does not compile to me, it seems ""measured"" is more appropriate.

To me, this section should be about the datases and metrics used for the experimentation. You include here the choice of modeling as TVG, which I believe is part of your approach, and specifically about your reference implementation of the general architecture.

There is no explanation of why the 22 PT networks were chosen, were they the ones available? You mention ""representative in terms of modes of transport and geographical coverage"" Did you consider a larger set and then discarded some?  Did you choose them to have a variety of sizes/degrees/densities? (does not seem the case) What timeframes were considered and why? In the caption of Table 2 you mention ""number of active stops during their busiest day"", what means ""busiest"" and how it was established? 


Section 5:

The formulation of hypothesis is not consistent with the questions. RQ1 is formulated as ""What is..."" but the H1 is ""There is..."". For RQ2, the question is ""What is"", but the hypothesis is not concrete enough, what is the ""specific set of topological characteristics""? That you hypothesise?  It seems both questions need to be rewritten as ""is there"" questions. Another thing that makes noise to me about writing these as hypothesis is that if you formulate them as statistically testable, then you need an actual statistic test, which you only have for RQ2.

Unclear what the assumption ""PT route planning queries will be normally evaluated within the span of one day"". After reading other parts of the paper, this seems to mean that queries are done for travel on the same day.

I'm confused about the relationship between the paragraph on ""Smallest fragment possible"" where you talk about ""number of connections allowed per document"" and that ""with this lower bound we were able to fragment the rest of the collection in fragments containing similar number of connections and hence a similar size"",  and the ""fragmentation sets"" where you talk about ""connections per fragment"". If you use that lower bound as guide for the size of the fragments, then how does it make sense to  then vary in fixed sizes? It seems to me the last sentence of ""Smallest fragment possible"" may be poorly written.

In table 3, the term ""query length"" has not been used before, what does it mean?

Overall, this section has a lower writing quality than the others, and would benefit from additional proof-reading.


Discussion and conclusion:

MINOR: neglible -> negligible

Can you elaborate why Spain-Renfe has the optimal point?

With respect to historical data, on p6 you mention that the main issue is that this data is not currently being published. Let's assume that a publisher willing to publish that (instead of hiding it for business reasons), what is the advantage of using your approach over a data dump? You mention Machine Learning algorithms as beneficiaries, wouldn't those require a full dump? A statistical analysis would need the same. If I got it right, you use in your experimentation for historical data the same queries than with the Live setting, but are queries for historical use cases the same as for Live use cases? It is not clear for me, and I would say they aren't.

You mention that optimal fragmentation size is ""related to the average scanned connections of the query set"". There is no mention in section 6.1 or Figure 6 of E(SCQ), just some references to table 2 for values of K and D (that are quite heavy for a reader to go check) what is the support of this statement? I think you need some visualisation of this in section 6.1.

You stress a lot the ""cost-efficiency"" of your approach for publishers (presumably small agencies in a budget. You even mention  on p27 ""...more expensive servers will be needed with OpenTripPlanner than with LC Server"", but I'm missing at least an estimation of how much more (in money), based perhaps on current average cloud or web server costs. 

Following on my remark on statistical tests in section 5, I don't think you can write ""accept hypothesis"" on RQ1 and RQ3 in the conclusion.





 




","gpt-4o-2024-11-20, Jon Wasky",slightly negative
2860,"The paper proposes a methodology for aligning Wikidata statements with natural language sentences from Wikipedia. In general, the paper is well-written, and most parts of the proposed pipeline are clearly described. I also appreciate the authors' effort to offer a mathematical formulation for the entire process of building the resulting dataset. The authors have already made their data openly available at a persistent URL on GitHub (i.e. at https://github.com/tahoangthang/Wikidata2Text). The repository looks in good shape and sufficient documentation about the resources is provided.

My concern regarding the paper is mostly associated: (i) with how it relates to other existing works that seek to generate similarly-minded datasets, and (ii) with how the quality of the resulting corpus is evaluated. The work is introducing a methodology for obtaining high-quality alignments between English Wikipedia sentences and Wikidata quads (i.e. subject-property-object triples along with their corresponding qualifier information). However, I believe the provided Corpus Evaluation section does not offer sufficient evidence about the \textit{closeness} of the resulting quads with their corresponding sentences and their annotations.

Furthermore, the data collection involved quads from triples consisting of six different properties (P26, P39, P54, P69, P108 and P166). To a substantial extent, the work in the paper is motivated by its usefulness for training Natural Language Generation systems. Nonetheless, I believe that developing a system focused on the realisation of only six properties would vastly hinder its lexical variability. Consequently, the authors should elaborate further on their decision to narrow their experiments to only these properties, and how feasible would it be for their data collection process to be extended to less popular properties in the knowledge graph.

Literature review focuses mostly on research articles that propose Natural Language Generation systems, and briefly describes their proposed solutions. However, there has been extensive line of works that sought to build datasets that align knowledge base triples with texts. The works by Toutanova et al. (2015), Gardent et al. (2017), Elsahar et al. (2018) and Vougiouklis et al. (2020) are only some representative examples, which I believe should be included in the literature review. Furthermore, while the work by Mrabet et al. (2015) is included in this section, there is not enough information about how the proposed methodology relates to their work. I believe the paper would greatly benefit from a paragraph clarifying further how the proposed dataset creation process differs from other works that provide alignments of DBpedia or Wikidata triples with Wikipedia sentences, in particular, the ones that are the most relevant to it (Mrabet et al., 2015; Gardent et al., 2017, Elsahar et al., 2018). Including a table with comparative statistics wrt. various dataset characteristics, such as number of dataset instances, and entity and property coverage, would be very useful as well.

While I appreciate the provided technical details in Section 5.3, I believe the paper would also benefit from a clearer description regarding the input and output of each algorithm used during the data collection process. A figure or an example that is extended across the entire section would be very helpful towards this direction. I also believe some of the provided algorithms focus too much into development details. I would recommend to have some of these details included in a separate Appendix, allowing this section to highlight the aspects of greater scientific merit in the work.

More details should be provided about the setup used for the evaluation of the mapping in Section 6.1. My understanding is that we want to explore how well can a system for Named Entity Recognition (NER) can match the subjects and objects from the quad in the corresponding sentence. It is not clear from the manuscript how the five different NER systems are leveraged for Type Matching, and which part of the dataset is used for training and testing (i.e. in the case of tr-******-sc* setups). I believe $N_u$ is defined as the set of matches that were made incorrectly by the Entity Linking algorithm. If that is the case, this should be explained in a clearer manner in the second paragraph of Section 6.1.


I would also recommend a re-structuring of Section 6.3. The authors should first introduce the purpose of this evaluation that should go beyond which clustering algorithm works better for the dataset, and explain how the quality of the resulting dataset is determined by the provided clustering metrics. Afterwards, the baselines for noise-filtering should be introduced (i.e. w/ clustering algorithms and by setting a threshold wrt. the maximum number of redundant words). I believe a discussion regarding the results of these experiments should be placed in a separate subsection, which would highlight the important findings and how these relate to the quality of the resulting dataset. The paper would also benefit by a qualitative evaluation section focusing on potential problematic alignment cases in the corpus, and when these are likely to occur.


Some further notes on the provided manuscript:

Minor error in the first sentence of Introduction; a trivial change could be as follows: ""... will go further than what has been done in the past in order to turn Wikimedia into a fundamental ecosystem of free knowledge..."" 

In the fourth paragraph of Section 4.1, there is a reference to ""Mario Kirev"" who is not mentioned in Figure 1--maybe the reference should have been to Simone Loria in Figure 3 (i.e. Simone Loria is mentioned with the ""He"" pronoun in the figure), but this should become clearer in the manuscript.

There is set of minor typos across the entire manuscript, and some further proof-reading is advised.


Some notes on the accompanying repository:

Please include a link to the GitHub repository in the provided manuscript.

To facilitate easier processing of the output data in the repository, I would urge the authors to provide token position and offset values (i.e. position of the tokens in the input) for each annotation included in the label_sentence_1 and label_sentence_2 fields of the resulting CSV files.


References

K. Toutanova, D. Chen, P. Pantel, H. Poon, P. Choudhury, and M. Gamon, “Representing Text for Joint Embedding of Text and Knowledge Bases"", in Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, Lisbon, Portugal, 2015, pp. 1499–1509, doi: 10.18653/v1/D15-1174.
Y. Mrabet et al., “Aligning Texts and Knowledge Bases with Semantic Sentence Simplification,” in Proceedings of the 2nd International Workshop on Natural Language Generation and the Semantic Web (WebNLG 2016), Edinburgh, Scotland, 2016, pp. 29–36.
C. Gardent, A. Shimorina, S. Narayan, and L. Perez-Beltrachini, “Creating Training Corpora for NLG Micro-Planners,” in Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Vancouver, Canada, 2017, pp. 179–188.
H. Elsahar et al., ""T-REx: A Large Scale Alignment of Natural Language with Knowledge Base Triples"", in Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan, 2018.
P. Vougiouklis, E. Maddalena, J. Hare, and E. Simperl, ""Point at the Triple: Generation of Text Summaries from Knowledge Base Triples"", J. Artif. Int. Res., vol. 69, pp. 1–31, 2020, doi: https://doi.org/10.1613/jair.1.11694.","gpt-4o-2024-11-20, Jon Wasky",'slightly positive'
2861,"Review swj2861


This paper describes parts of a pipeline aimed to achieve a formal understanding of commands given in natural language. 
The topic is interesting to SWJ and the paper addresses problems that have received attention in robotics for many years, like symbol grounding, or in more recent research, like the use of image schema for planning. 
The research question is explicitly stated: “how can we use ontological knowledge to extract and evaluate parameters from a natural language instruction in order to simulate it formally?” and the evaluation of the result is done via simulation in a virtual environment previously developed, at least in part, by the authors themselves. This kind of evaluation is suitable for the given research question and the paper content. Yet, more information should be provided to make the material accessible and reusable to the community. I must confess that at times I'm confused about what is assumed to be true in general and what is true because of the way the system elaborates the available data. This might explain some of the points I make below.

The paper’s title and the research question address instructions in natural language but the examples in the paper are about commands. The capability of the pipeline to manage instructions in a proper sense is unclear. For example, a food recipe is made of instructions but goes beyond the cases discussed in the paper: usually a recipe includes positive and negative commands, alternative suggestions, motivations and other kinds of explanatory text. Which of these the proposed pipeline covers is not clear to me.

The abstract should be expanded to better situate the research and to give a more informative description of the paper’s content, in particular the pipeline structure. 
Also, the paper is entirely focused (motivations, development and simulation) on the robot scenario. If not the title, at least the keywords should include pointers to robotics or robotic applications.

The choice to evaluate the results via simulation in a virtual environment is suitable due to the domain of application. I would suggest to add also checks internal to the pipeline, e.g., a theoretical validation that all input receivable by a node in the pipeline is processable by that node; and I guess some check of consistency across annotations in the same scene might be useful: what happens if I have two objects dubbed fridge in the overall assumption that there is only one fridge in the kitchen?

My other concern is that the simulation case is limited with respect of the variety of possible commands. What is the outcome of the pipeline when the command itself presents some ambiguity? E.g., “Put the chair on the table” (where the chair is used by mistake while meaning the bowl previously put on it); “Take the cheese out with the bottle” (perhaps having said earlier “take the bottle out of the fridge”); “Put the bottle back” (meaning: back to the fridge).
It is unclear which of these cases are out of scope and how the others are managed by the system. For the first, information should be stated clearly from the start. For the latter, I believe more simulations are needed to verify if and up to which point the system works correctly.
In short, the research question asks how we can solve a general problem but the paper remains unclear about the restrictions assumed for the pipeline to solve the problem, and the evaluation section does not clarify this either.

I had to guess what is assumed at the different stages of the information flow. Overall the pipeline makes sense at first sight but what kind of input is assumed at each step with respect to a real scene, what kind of information is manipulated at each step and what is the result is only suggested (notwithstanding the Listing 1-4). I suggest a more systematic presentation of the information flowchart.

The authors could be clearer on the parts of the pipeline which are newly introduced (if any) and those that are preexisting it. This can be easily shown using thick boundaries (say for the new parts) and thin boundaries (for the old) to mark the nodes of Fig. 1.

Fig.1: in the legend for Data flow and Annotation put just the arrows (drop the slots). 
According to the figure, the DLU ontology is sending annotation to context (d) without any input regarding the Scene which is really weird, how is this actually working? 
In the caption of Listing 2 rephrase in natural language the content.

Similarly, in Listing 3 we have 
<urn:Cup> ref:type dlu:Cup
<urn:cup_0> ref:type dlu:Cup
<urn:Cup> ref:type soma:DesignedContainer
<urn:cup_0> ref:type soma:DesignedContainer
where one is a symbol and the other an object.
In which sense are these of the same type?

Why the uuid used in listing 2 is not one of the identifiers occurring in Fig.3? Is there a reason?

The authors should clarify further the use of the term context as introduced in Sect 3.3. Apparently it is the scene but practically it seems to be the scene and the set of sentences parsed so far, that is, the internal representation of the knowledge of the scene at this point in time. Yet, the text uses also ‘context description’ as if the two notions should be kept apart. 


pg. 4 suddenly restricts information to a special type of commands: “For a command to be executed, an environment with objects to manipulate is mandatory.”
Is this excluding commands of type “Go near the fridge”, “Stay away of the cooker” and “Check if the pot is on the burner” where no manipulation (except of itself) is needed nor expected?

It is unclear whether this claim “Each object in the kitchen scene is internally annotated with unique identifiers and semantic labels aligned with SOMA” is a prerequisite for the system to apply or an observation of how the input provided by component (f) is. In other words, can it happen that an object has no semantic label? What happens in this case?
(I must confess that I have this kind of reading problem across the paper especially when referring to the pipeline: statements are given as they were isolated which does not help to understand whether they are derived by the system or assumed to be true at that stage even though it might not be the case while running the system.)
I wonder whether the claim “a semantic map is assumed to be provided” trivializes the earlier discussion and the grounding problem itself. This should be clarified.

Sect. 3.4: Is the mapping btw objects and symbols bidirectional? All the other maps are described as being unidirectional.

I would be interesting to know where the heuristic of Table 1 fails. Could you add some examples. Are there checks for these cases?

In fig. 3, I have hard time making sense of links like this
id:8e065… (a trajectory) dul:classifies id:18b08.. (a cup)
Please, add more information on how they should be read.

","gpt-4o-2024-11-20, Jon Wasky",slightly negative
2866,"The manuscript was submitted as 'Data Description'. It describes a linked dataset of SPARQL query logs (LSQ), version 2.0. LSQ 2.0 features logs from 27 SPARQL endpoints and describes 43.9 million SPARQL query executions on these endpoints. The paper is an extension to a paper ""LSQ: The Linked SPARQL Queries Dataset"" published on ISWC 2015. The submited extended version has the same structure as the previous paper. Both papers consider the same set of use cases for the LSQ dataset and both introduce the same data model for representing query logs. The reviewed paper describes significant extensions compared to the first version from 2015. This includes a significant extension with more datasets, queries and query executions, and more detailed statistics. Also a broad usage of the dataset is presented in a structured form which transparently shows various recent research works which used LSQ for the evaluation. The paper shows only the usage of LSQ 1.0. However, this clearly proves the usefulness of the dataset. The new version as a significant extension to the first version will undoubtedly find a broad usage as a pool of queries for various benchmarks.

In summary, even though the paper does not bring anything ground breaking compared to the paper from 2015, it shall be published in my opinion. It is necessary to inform the research community about the new version of the dataset. The paper contains all necessary information the readers need to start using the dataset.

I have two minor issues related to the information provided about the dataset in the paper.

I1) Metrics and statistics on external and internal connectivity of the dataset: The SPARQL endpoints are described in a LSQ specific way. However, existing descriptions of SPARQL endpoints expressed as VOID resources could be linked instead.
I2) Use of established vocabularies: This is related to the previous issue. On page 4, Query instance, the authors describe that the originating endpoint of a query is associated with the dataset using a LSQ specific property lsqv:endpoint. The reason is that void:endpoint or sd:endpoint could not be used because they have different domains. It is a strange reason. For example, a LSQ query can be associated with a void:Dataset using a LSQ specific property. Then it would be possible to use void:endpoint (https://www.w3.org/TR/void/#sparql). I understand that it is not possible now to change the vocabulary. However, the rationale behind this design decision should be justified better.

The paper demonstrates the necessary level of the maturity of the dataset, including its quality, stability, and usefulness proved by the usage of the dataset by many researchers as a benchmark for evaluating their results. The description of the dataset is at the necessary level of detail. I have two minor issues related to the clarity of the description.

I3) Page 4, query instance: The property lsqv:endpoint is described in the text with lsqv:Query as a domain. However, Fig. 1 shows that the domain of lsqv:endpoint is lsqv:RemoteExec. The textual description of the LSQV vocabulary is inconsistent with the figure.

I4) Page 4, static features: It is described that static features are defined for a query, independently of the dataset over which the query is evaluated. However, the paragraph above describes that queries are considered only in the relation to an endpoint. In other words, an instance of lsqv:Query is always a pair of SPARQL expression and endpoint. LSQV does not allow to describe features of queries independently of the endpoints. Therefore, the paragraphs ""Query instance"" and ""Static features"" on page 4 are in a contradiction.

Regarding the LSQ data file

(A) It is well organized and available for download as well as via a SPARQL endpoint through an access page http://lsq.aksw.org/. The access page contains necessary information about the access to the dataset.

(B) The provided resources are complete for replication of experiments. The access page provides also tools for the experiments and their documentation.  

(C) LSQ is not available through a public repository such as GitHub, Figshare or Zenodo. The authors use their own web server which seems appropriate for long-term discoverability. LSQ does not have a DOI.

(D) The provided data artifacts are complete. The LSQ dump is structured to files for individual SPARQL endpoints. The content for each endpoint seems complete.","gpt-4o-2024-11-20, Jon Wasky",slightly positive
2873,"The paper describes an ontology-based framework for security analysis of industrial systems, which combines several technologies and existing systems and platforms into a complex analysis framework that partially automates the analysis process. The paper is well written language and layout-wise, and easy to understand at a general level, but one of the main shortcomings of the paper is the lack of both definitions and detailed descriptions of the technologies and components, as well as the lack of detail of the associated method, which actually does not in the end allow the reader to fully understand how the approach actually works, nor to make it reproducible or reusable. The linked GitHub repository mainly provides the code and a very brief Readme-file, but no additional documentation or technical details as far as I can tell. I am sure the code is fine, but without additional documentation, datasets etc. it is not so useful. A strength of the paper however, is the separation of the framework itself from the prototype implementation, which is in general a good thing. 

Nevertheless, in addition, the evaluation section has more the character of a discussion rather than a scientific evaluation of the proposed framework and associated method. The evaluation also claims to validate the requirements, but since the requirements are described in a very generic fashion, and are sometimes ambiguous themselves it is not clear that the evaluation is rigorous enough to even establish that the framework supports these requirements. Regarding the requirements in section 3, questions arise like: What is a “high degree” of automation? What is a “flexible” model extension? What does “secure” mean in the context of information extraction, and is it really information extraction as in extraction from natural language text that is referred to in point 2? What is “exchangeable model extension logics” in point 9, does it mean exchangeable rules or exchangeable semantics? What does “analogous” mean in point 10? Etc. Further, several of the figures are very unclear to me. For instance, I have no idea what Figure 13 is trying to express. Is it some kind of evaluation workflow? Relations between datasets and processes of the framework? Or something else? The caption says it is a knowledge base excerpt, but if this is an instantiated ontology, the notation really needs a thorough explanation, to clarify its semantics. 

Taken together, these two aspects (lack of technical detail, and lack of detailed formal evaluation) leaves the paper with very little scientific contribution. The framework is not described in enough technical detail to determine if there is really a novel use of ontologies and/or other semantic technologies there, and the evaluation does not really provide any insight into the contribution of the semantic technologies, nor how this approach compares to related ones. Based on this I unfortunately do not think that this paper, in its current form, fits as a full paper within the SWJ. It could fit better as an “application report”, however, then it would have to be considerably shortened and condensed. Or it might be submitted to and “in-use track” of one of the semantic web conferences, if the framework has been applied in a more realistic industry-scenario (than the lab setup that is discussed in the paper). 

Another problem of the paper is that it is not really targeted at the SWJ audience. It focuses more on the security analysis problem, and also assumes a lot of knowledge regarding that domain from the reader, but it glosses over the problems that would be more interesting to the SWJ audience, such as the development and resulting complexity of the ontologies, how the reasoning is performed using them, how OWL is combined with all the other technologies and formats mentioned, such as SWRL, property graphs (neo4j) etc. One thing that would greatly improve the understandability of the overall problem and the complex framework, for non security-researchers, would be a running example that has the character of a “toy example” so that the reader can follow what happens in each step of the workflow, what the model extensions and reasoning steps produce etc. This would give SWJ readers a more concrete understanding of what this framework actually does. The paper does introduce a working example in section 2, but at a very abstract level, and without considering that many of the tasks are completely unfamiliar to the main part of the SWJ audience. It would be more useful then to show a small excerpt of the actual ontologies representing this, followed by the data that can be generated, the workflows etc., and use that as the basis of a running example. ","gpt-4o-2024-11-20, Jon Wasky",strongly negative
2873,"# Overall assessment 

(1) Originality

The paper describes a framework that automates the generation and execution of security analysis workflows for industrial systems. The approach uses ontologies for the representation of component and system models as well as for the storage of policy knowledge used to generate the workflows. The use of workflow modeling and orchestration techniques in combination with ontologies to generate tailored workflows for ontology-based analytic tasks is interesting and novel (and not necessarily limited to security analyses). 


(2) Significance of the results

The paper mainly focuses on the high-level software framework, its architecture, and workflow management aspects and presents a high-level methodological framework for security analysis. This results in an extensive set of requirements, a generic model processing workflow, and a detailed description of the prototype implementation.

The ontologies used in the process, their conceptualization and their role in the overall process, by contrast, are not discussed in sufficient detail and it is somewhat unclear what exactly they contribute to the overall concept.

It is not clear from the examples given how exactly the process works and how the analyses are driven by the ontologies. Overall, the workflow generation and security analyses are interesting, but the contribution of semantics and ontologies in the analysis is somewhat opaque. Consequently, the significance of the results for the SWJ audience is somewhat unclear.

Also, the evaluation should be covered in more detail to support the significance of the results (see more detailed comments below).


(3) Quality of writing. 

The paper is clearly structured and for the most part easy to follow. The various figures and tables illustrate the approach well and support the flow of the paper. The quality of writing should, however, be improved substantially throughout the paper (some sections appear more mature than others, but proofreading by a native speaker would benefit all sections) - for illustrative examples of language issues on the first page, cf. the minor comments below. 



# Comments

*) The title seems to stress that the ontologies facilitate collaboration, but the paper does not cover these aspects beyond stating that it is a requirement in Section 3. The only other reference to collaboration I could find is that the framework ""supports arbitrary security experts and security knowledge bases with the concept of analysis definitions"" - it is not clear to me how exactly the ontologies facilitate the collaboration. Do they bridge semantic gaps that may exist between the stakeholders involved? Do they define their roles and responsibilities in the workflow? If this is one of the key contributions of the paper, it should be covered in more depth and be better illustrated. In that case, I would also expect a definition of the stakeholders and roles involved in the analytic workflows.

*) The role of Semantic Web methods and technologies is not explained in detail in the paper. Given the target audience of this journal, it would be important to know how, for instance, the ontologies actually support the dynamic generation and execution of security analytic workflows.

*) It may be illustrative to compare a security-analytic process with and without the support of the framework. A qualitative evaluation of the  usefulness in practical settings, e.g., with domain experts would also help to strengthen the evaluation. If this is not feasible, at least the approach and results of the lab evaluation should be explained in sufficient detail.


*) A working example like the one introduced in Section 2 is useful and important and should be helpful to understand how the overall process works. It is currently difficult, however, to ascertain what exactly happens in the workflow steps outlined in Section 5, which makes illustrative references to the working example, but does not really explain the overall process - e.g., by means of an end-to-end example. The evaluation section also refers back to the working example, but only gives a very high-level perspective at a very coarse level of granularity. It is not clear from Fig. 13, for instance, how the compliance checks and configuration analysis are actually performed. A concrete end-to-end example with actual that shows how the specific queries ""that analyze the policies"" are generated and/or how are they linked to policies (cf. Fig. 7) would be useful.

*) No concrete instance data (i.e., component and system models) on the working example seems to be available - given that no concrete information on these models is given in the paper either, their concrete role in the overall process is fairly unclear..

*) The second evaluation on ""another more complex lab environment"" is only mentioned in passing and no results are presented. 

*) Overall, it seems that the conclusions drawn in Section 8 do not directly follow from the evaluation results (e.g., Where is the ""ability to exploit synergies.. for the analysis of different types"" actually shown?)




# Minor comments


Examples for language issues on p 1:
 *) Abstract: ""Security analysis, like configuration, compliance ..."""" → I suppose ""such as"" conveys the intended meaning (like could imply that these are not instances of security analysis activities)
 *) Abstract: "".. is highly complex for (industrial) systems"" → Why parentheses (statement ""is highly complex for systems"" does not make much sense)
 *) p. 1: ""At the latest since"" → ""At least since""?
 *) p. 1: ""a general progressive adaptation of information technology""  → ""adoption""?
 *) p. 1: ""in comparison to the impacts"" → ""impact""


Fig. 13 Legend: no explanation of the abbreviations used given






# Assessment of Resources

The authors provide a link to a GitHub page with various projects that contain the source code of various components and ontologies related to the publication. 

(A) whether the data file is well organized and in particular contains a README file which makes it easy for you to assess the data

I could not find concrete instructions on how to set up these various components from multiple projects in order to replicate the setup described in the paper at this point.

(B) whether the provided resources appear to be complete for replication of experiments, and if not, why, 

Due to this lack of instructions, no in-depth evaluation of the provided resources and replication of results was possible. 
The SyMP Camunda Platform project readme mentions that a Dockerized implementation with all components (which would presumably make replication much easier) is under development.

(C) Whether the chosen repository, if it is not GitHub, Figshare or Zenodo, is appropriate for long-term repository discoverability

Publishing on GitHub should ensure long-term discoverability.


(D) whether the provided data artifacts are complete. 


The artifacts published on the GitHub page of the author (referenced as stable URI) include the source code of the Camunda extensions, a tool for OWL conversion, and various ontologies - they include both code and ontologies, but are dispersed across projects, not all of which are related to the contents of the paper. Therefore, it is not easily possible to assess whether the artifacts are complete.

Note: the provided artifacts are in any case insufficient to replicate the experimental setup from the paper, as the data from the evaluation section is not provided.

","gpt-4o-2024-11-20, Jon Wasky",neutral
2873,"The paper considers an application of an ontology-based solution for the automated support of a system’s security life cycle and multiple analysis types. The topic is highly relevant nowadays. The authors developed the novel framework implementing the proposed solution. It is open-source and available on GitHub.
The paper is well-written and well-structured. 
The paper provides the comprehensive review of the related research and clearly describes the place of the proposed solution.
The provided results are original and significant, and contribute to the system's security life cycle analysis.

Some remarks:
1.	It would be nice to highlight contribution and novelty in the introduction.
2.	Section 2 – Working example – Connection with industrial systems is not clear.
3.	It is interesting how exactly system model is constructed: as I understood the general ontology is generated manually. How does it instantiated? Based on the events analysis? Or is it out of scope of the paper?
4.	In section 3 the requirements do not include requirements to the big data processing or operation time. If ontology is instantiated in dynamics based on the events processing it seems essential. 
5.	It is not clear if the set of requirements is complete.
6.	Table 2 is not filled for the proposed framework (for qualitative comparison). It would be nice to do it in Section 7. For the proposed framework all ellipses will be filled, will they? 
7.	Section 5 – Please clear, should policies for the analysis be manually processed and represented as ontology first?
8.	Fig-s 11 and 15 are hard to read in the printed form
9.	Fig. 12 – please explain the results: the assets provided on the right do not satisfy to the policies represented on the left, do they?
10.	Section 7 – what are the evaluation tests?
11.	In section 7 the second tested environment is mentioned. In Fig. 16 only its hardware part is represented. As soon as the solution is for the industrial systems, the architecture of the second tested environment and its description is very interesting.

Some typos:
1.	P. 2, row 20: Allocation of analysis types to LCS phases. -> Allocation of analysis types to SLC phases.
2.	P.10, rows 44-45: Workers listen to the topics of the tasks they can fulfil -> Workers listen to the topics of the tasks they can fulfill
3.	P. 16, rows 21-23: Thereby the framework focusses on the automation of highly flexible model generation, extension and analysis -> Thereby the framework focuses on the automation of highly flexible model generation, extension and analysis.

The data file provided by the authors under “Long-term stable URL for resources” is available on the GitHub, well organized and contains a README file. The provided resources appear to be complete for replication of experiments. ","gpt-4o-2024-11-20, Jon Wasky",extremely positive
2885,"""(1) Quality, importance, and impact of the described tool or system (convincing evidence must be provided). ""
------------------
This paper presents TermItUp, a generic architecture integrating multiple state of the art tools with the purpose of providing a one-stop-shop for all terminology extraction needs. 
The tool has been developed following FAIR and open science principles, using standard LLOD and LOD formats, guided by a set of requirements based on observations in the state of the art, 
but also discussions with terminology experts. 
The tool will be an extremely useful to the community as the systematic integration of its different components for each single project would be incredibly time-consuming and ad-hoc.
There are high impact uses of the tool in H2020 and other collaborative projects, showcasing the potential of the tool. 


""(2) Clarity, illustration, and readability of the describing paper, which shall convey to the reader both the capabilities and the limitations of the tool. Please also assess the data file provided by the authors under “Long-term stable URL for resources”.""
------------------
The paper is well written and organized, and clearly tell the story of TermItUp in terms of its capabilities and shortcomings. 
There are small language corrections to be made for the camera ready version, highlighted at the end of this review. 

The review of literature is complete, although I would have preferred to see at least a mention of SOTA efforts for terminology-extraction around TermEval2020/ACTER and an explanations as to why such systems although theoretically very accurate would be very difficult to integrate in a production system. 
There's a very interesting multilingual extraction system in the 2021 finding of the ACL https://aclanthology.org/2021.findings-acl.316.pdf
I mention this because it's interesting, but the positioning of the paper doesn't necessarily require to go into this particular literature. 
Perhaps the mention in the paper that there are ongoing efforts for multilingual terminology extraction actually refers to this. 

Regarding the perspectives, I would love to see the future integration of a knowledge-graph aware association rule mining approach in addition to the extraction of hierarchical relations. 
I am not asking to mention this in the paper, just an interesting thought. 


""In particular, assess (A) whether the data file is well organized and in particular contains a README file which makes it easy for you to assess the data,
(B) whether the provided resources appear to be complete for replication of experiments, and if not, why,
(C) whether the chosen repository, if it is not GitHub, Figshare or Zenodo, is appropriate for long-term repository discoverability, and 
(4) whether the provided data artifacts are complete. Please refer to the <a href=""/reviewers"" target=""_blank"">reviewer instructions</a> and the <a href=""/faq"" target=""_blank"">FAQ</a> for further information.""
------------------

The github repository is easily accessible and significant information is given in the README or on the main website of the tool. 
The overall explanations are clear and the documentation of the API provided through swagger is very helpful. 
The main improvement directions would be 1/adding a more technical documentation that explains how to deploy the service 2/adding some docstring documentation for developers 
3/potentially refactor the code base if there is a subsequent increase in complexity, I would personally favour a generic class structure with polymorphic genericity rather than reflexively loading code modules that all include the same functions, even though the latter tends to create less overhead. 
As the tool is still a prototype under active development, those are not significant issues with regard to the publication of the paper. 



**I recommend acceptance of the paper**, the corrections are mostly cosmetic for the camera ready. 



***Detailed corrections***

Page 2 line 12 left: the bit of the sentence about DBPedia is confusing, I don't understand what it means. 

P2 l19 left: is to find -> is finding

P2 l25: different backgrounds and expertise levels to face language and related needs [...]

P2 l30 right: discussions that have arisen

P2 l37 right: This section covers 

P2 l38 right: different processes mobilized in our system

P3 l42-43 right: Combining Wikipedia and other resources, BebelNet constitutes an multilingual [...]

P4 l9 left: domains, with half being closely related to [...]

P4 l10 left: Several scientific works are devoted

P4 l12 left: a SPARQL

P4 l28 left: can be of great help

P4 l6 right: corpus -> corpora 

P4 l36 right: These can correspond to  different [...]

P4 l43-45 right: The meaning of a unit is to be discovered in text and constructed  through relations to other terminological units. 

P5 l22 right: can significantly contribute to improving performance [...]

P5 l22 right: to developing

P5 l45 right: this translates to a necessity [...]

There are aditionnal small corrections like these to be made, can transmit feedback to authors later as time permits, preferable to delaying the review submission.","gpt-4o-2024-11-20, Jon Wasky",'slightly positive'
2885,"Overall, the updated version of the paper reinforces the high quality and relevance of the work being developed, as well as TermitUp’s potential impact among the linguistic data science community. This enhanced version of the article provides a more thorough description of the tool’s architecture and its underlying methodology, as well as of TermitUp’s contribution to EU-funded projects (e.g. Prêt-a-LLOD, Lynx, and, more recently, SmarTerp) and to other initiatives, such as the collaboration with the DFKI and the work on the Emofinder database. Moreover, the authors managed to further highlight TermitUp’s complementary role when compared with other existing tools currently supporting terminological work. The topic concerning when and how human intervention may occur across the system has also been clarified. In addition, information has been provided by the authors on recent work in the proposal of an Ontolex Terminology module, which can certainly contribute to TermitUp’s evolution while, in turn, also benefitting from the tool’s development. 
The links to the various repositories (namely GitHub and Zenodo) have been added, and the provided data resources appear to be complete, thereby enabling replication or experiments. Finally, even though most of the spelling issues have been solved, there are still some minor revisions pending:
p.4|9L: the ‘being’ before ‘half’ has still not been eliminated
p. 8|33-34R: “*Prospective* experiments to this module *were* already published *at the* LREC 2020 conference”
p. 9|48R: “be more suitable than *the* other”
p. 10|29L: “are divided *into* two parts…”; 43L: “editing platform connected to *the* TermitUp triple store”; 39-43L: The beginning of the sentence “Much as Prêt-à-LLOD…” is odd; please consider rephrasing.
p. 11|41L: please eliminate one of the “presented”
p. 13|50R: “similar to *those of TermitUp*;”
p. 14|11-12L: “community can *improve, contribute to or adapt it* to their specific *use cases*”","gpt-4o-2024-11-20, Jon Wasky",slightly positive
2894,"The paper is clearly a very significant and major revision of the prior submission. The paper clearly has merits for publication, and there is even more significance and potential impact with the numerous revisions made to the system. As a result, I believe that this paper, with a minor revision, should be accepted, but not as a 'full paper' but as a ‘tools & systems paper’. That is, the type described here: http://www.semantic-web-journal.net/content/special-call-semantic-web-tools-and-systems
and the impact as per http://www.semantic-web-journal.net/faq#q20 is quite clear with the available system and its incredible performance comparing with state-of-the-art solutions.

The reason I do not recommend acceptance as 'full paper' is primarily due to the quality and style of writing, and in general lack of scientific rigor that is expected from a research paper published at a high-quality journal.

There are two issues that need to be fixed before the paper is published:
1) The paper claims reproducibility based on the available and very well-documented code base that allows reproducing the results reported in the paper. However, the results are only reproducible if the server is up and running. We all know that software systems cannot be maintained indefinitely and have a limited lifespan. The paper once published will be accessible for many years to come, but a large number of URLs cited (mostly as footnotes) will have a much shorter lifespan, potentially broken even by the time the paper is published. I suggest rewriting all the parts that refer to URLs to make them readable and useful even if the URL is not available. This means using citations of papers if possible, replacing citations of the entity search API URL with reference to the section that describes the entity search function, and moving implementation details to a separate section, under Evaluation.
2) The notations in the paper still need serious work and rewriting. This applies to most of the formulas and equations and definitions in the paper. Starting from the basic definition of a Knowledge Graph. Why do you need to define a KG as (E, T, P, Triples) ? You can define a KG as a collection of triples and then define what E, T, P are, but what you have is not even complete. For example, a T is not just a set of types, but it is derived from rdf:type predicate of the Triples. And then a “literal value” is not formally defined. Couldn’t you just re-use an existing definition of an RDF knowledge graph? The other really strange notation is when it comes to probabilities, i.e., equations 7-15. What is 	Pr(t|SpaCy_{c_j}) !? This could make sense if SpaCy_{c_j}) was a Boolean function with an input, but it is not. The definition you have for it in 3.4.4 is not a definition of a probability function. It seems to be some score function that you then normalize to view it as a probability function, but the normalization should be in equation 14 not just described in the text as you have it now. And you are not using the conditional probability notation properly: https://en.wikipedia.org/wiki/Conditional_probability when you say P(A | B), that means ""the conditional probability of A given B”, and that “the event of interest is A and the event B is known or assumed to have occurred”. t and  SpaCy_{c_j} are not events!… Also, the name of the function being the name of the software library is yet another indication that this paper is written like a code documentation. The fact that SpaCy is used is irrelevant for understanding the function. That is just an implementation function. You could replace SpaCy with any other NLP library that does the same task.

You also need to further proof-read the paper for English issues. Examples:
8.2: session -> Section
7.3: environmental -> environment
Page 2 col 2 line 24-28: “According to Wang et al., they only could” -> “Wang et al. state that only our system could generate the annotations”, also remove the .
In abstract, remove the mention of CEA, CTA, and CPA and say what they are. That is:
…to generate annotations with DBpedia in three annotation tasks: 1) Cell-Entity (CEA), 2) Column-Type (CTA), and 3) Column Pair-Property (CPA). 
->
…to generate annotations with DBpedia in three annotation tasks: 1) matching table cells to entities 2) matching columns to types, and 3) matching pairs of columns to properties. ","gpt-4o-2024-11-20, Jon Wasky",slightly negative
2898,"The changes made have significantly improved the paper. 
 
As an addition to my previous suggestions and those of my colleagues, as well to the improvements made by authors, I have only one minor issue, and that is:

The linked_ted-mdb files (@ Long-term Stable Link to Resources) do not open in Microsoft Excel (Mac version 16.52). However, I can open them in LibreOffice. It must be something small. Please give them a second look and improve them. 

Otherwise, the rest fits.
","gpt-4o-2024-11-20, Jon Wasky",'extremely positive'
2898,"The authors have exhaustively addressed all previous comments, and I was very happy of the improvements.
The paper is much clearer and easier to follow. 

The presentation of the two methods used to align discourse segments and relations across the parallel corpora is clear and sound - description of Method II has improved a lot.

Editorial comments:
- move Table 2 to be closer to where it is referred to in the body of the document
- move Table 3 to be closer to where it is referred to in the body of the document
- pag 8 line 14: incriminating --> increasing?
- pag 9 lines 15/18: check for consistency the use of italics for the connectives in the body of the text
- pg 10 line 20  and following: when presenting the sources of errors, I would use the \paragraph{} command in LaTeX rather than starting the paragraphs with a sentence in italics
- Table 9 and Table 10: I would anticipate and discuss Table 10 first -move it to where Table 9 now is and then present and discuss Table 9","gpt-4o-2024-11-20, Jon Wasky",'slightly positive'
2898,"Upon reading the revised draft, I am happy to see that the authors have addressed the points more or less well. A few more (minor) points identified in my second review are listed below:

Page 1, Line 39-41: The way the text reads, it seems that Connective-Lex presents linked data. If so, you should describe how the data in the lexicons under the database are linked.

Page 2, Line 22-24; 2-4: In my opinion, a single point is redundantly divided into two. You may want to merge them, making the single point more succinct. 

Page 5; example 7: I think the example (how much the argument differs across languages) should be described more clearly.

Page 6, Line 19-23: The point on the possibility of multiple relations within the same text span needs more elaboration, preferably with examples.

Language: I noticed some grammatical errors in the updated paper. Examples:

Page 4; Line 50: “This design criterion lead to” > leads to/led to

Page 5; Line 34: “… and their evaluation is provided” > are provided

Page 10, Line 26: “The linking performance… are measured” > is measured
","gpt-4o-2024-11-20, Jon Wasky",neutral
2904,"As suggested in my earlier review, the paper was now submitted as a Dataset Description. This type is better suited for the paper. The authors do need to draw very close attention to the call for papers for dataset descriptions, though. There are still characteristics of the dataset for its usage missing, e.g., spelling out the name, the URI, the version date and number, licensing, availability. Also, what is still missing is evidence of its use. Also, while datasets in a Government data repository are probably fairly save in their long term availability, a w3id identifier, DOI and availability on Figshare or Zenodo would be advisable.

Beyond the advertisement of the dataset and reporting its importance, a dataset description paper should also serve the purpose of lessons learned in the generation of the dataset. The paper still falls a bit short in this aspect. Two main issues:

- The transformation process is not described in detail and is not repeatable, i.e., someone with a similar problem cannot follow a methodology presented in this paper. There is some mentioning that the ontology was build manually, but then how was the mapping done. It looks like it was all done with custom scripts. Why not using mapping tools such as the ones listed here: https://github.com/semantalytics/awesome-semantic-web. Potential tools for the process could be any2rdf, triply, tarql, J2RM or any23.
Also, to be of real value to others, these tools should be configured (a wrapper built around them) to automate the process for the widely used data portal used for the Nova Scotia Open Data portal, i.e., Socrata.

- The level of human intervention is also not clear. It is stated in the conclusion that there was a tool developed to retrieve open datasets, but the identification of disease datasets was carried out manually. This is unclear. And the methodology section needs to clearly state which parts are manual and which parts can be automated. For example, the ontology development was obviously manual, but the mapping to the ontology can be automated using the aforementioned tools.

Overall, the paper can be shortened in several sections to save space to clearly describe the dataset, the methodology and the ontologies developed. For example, the description of the data portal itself is too long. Table 1 is not needed, as these seem to be the datasets in its original format, at least it appears to me when I click on them. Figure 3 shows an example observation. It would be better to first present the ontology in detail and then show an example instance. Section 4.4 is redundant. It seems as if these are just some proof of concept SWRL rules. Are they deployed and used? If not, they should not be included in the paper. Section 4.6 about the queries looks again like a proof-of-concept. Are these predefined SPARQL queries using SPIN or SHACL advanced features and can be used through the portal? If not, again, it should not be in the paper. Wikidata has a SPARQL interface with predefined queries. This is one way of helping users to access the data.

There should also be a stronger focus on lessons learned for jurisdiction to deal with Linked data. There are several Government Linked Data Working groups globally (and the W3C) that publish (have published) guidelines and recommendations on best practices. If they have been followed, what aspects of those had to be changed/customised for the local context and which one's were applicable.

There are a few language issues in the paper that need to be addressed, e.g.,

""a multi-dimensional structure should be defined consists of measures, and dimensions describing the measures"" misses a verb
""As a proof of concept, we designed a SWRL rule to infer the transitive relationship of diseases in a dataset using Protege rule engine""
""downing the road""

There are also some formatting errors such as ??? for Figures and references.
The reference list is also ill-formatted and not consistent. See FAQ10","gpt-4o-2024-11-20, Jon Wasky",slightly negative
2904,"
Overall impression
-------------------------
The article presents a dataset description of aggregated statistical data on diseases related to a Canadian locality. Although the topic is interesting and could have potential, the dataset presented is only a fragment of the main dataset, that has been transformed to RDF, which does not necessarily show a real contribution to state of art. The article neither shows the impact of published data in RDF,  nor demonstrates its use with utility by third parties. The relationship between the authors and the generators or maintainers of the data is not described either. Finally, in my opinion there are technical deficiencies in the specification of the dataset.


Positive aspects
---------------------
As example, use cases based on SPARQL query logic, based on pattern matching are presented. Also, the basic aspects to be specified in a description dataset are presented, such as name, URL, version date and number, licensing, availability,

Negative aspects
-----------------------
Quality and stability of the dataset
==========================
No evidence is presented on how the authors are involved in the generation or maintenance of the dataset, or with the consent of these people.

Usefulness of the dataset
====================
There is a missing contribution of weight that justifies the necessary impact to be published in the journal.
The dataset does not present growth potential, in fact the data present is up to 2017, nor is there a SPARQL endpoint where to execute the queries.
No evidence of use of the data by third parties is shown.

Clarity and completeness of the descriptions
==================================
There are no explanatory diagrams of the data model used, nor the way in which the referenced vocabularies are related. 
The vocabularies and ontologies are vaguely described.
Interlinking mechanisms are not presented to make 5 stars linked open data.
The URI scheme used and the other type of specifications such as Shape Expressions (ShEx or SHACL) is not described.
Although minor, There are typographical and formatting errors, also unfinished paragraphs (example the first paragraph of the conclusions)
","gpt-4o-2024-11-20, Jon Wasky",strongly negative
2904,"The paper presents the work to properly publish data under the principles of linked (open) data). To do so, the authors have promoted to the linked data initiative a dataset coming from Nova Scotia. 

The work is interesting and applicable to many domains in which sometimes data is published for the shake of publishing. However, there are some things to improve:

-In the abstract, it should be necessary to be more informative and quantify some adjectives like “most of datasets” to see some context and magnitude of the problem/context.

-Include also more technical details about the results: ontologies/semantics rules and provide some quality assessment if possible.

-In the introduction, some existing issues must be justified: ""The datasets act as isolated pools of information that cannot be queried or linked."" This implies to categorize and identify the current dimensions of the problem to be addressed:
--Problem of reusing data, Which are the principles? For instance, alignment to Open Science/Data principles, etc.

Then, establish the specific technical issues that are preventing a proper data federated environment: data modelling: linked data patterns? data integration, data quality, data querying mechanisms, lack of APIs, use of standards and existing vocabularies, etc. to finally assess if there are interoperability issues: interoperability issues: communication protocols, syntax and semantics

-In the state of the art/background section, there are multiple works about linked data lifecycles that must be mentioned. 

-In the methodology section, it would be nice to see some description of the datasets (type of data, issues in data: consistency, naming, etc., need of logics, etc.) to finally end with the decision on the vocabularies and data model including a description of what is being reused (with more details of the process) and what is new. 

-Regarding the semantic rules, does it make sense to directly use SPARQL to produce new facts? In terms of consistency, it should be nice to see the reconciliation process between entities (if it was necessary).

-Authors also show some SPARQL queries. It should be necessary to link the potential of these queries to the initial needs. 

-The quality checking of the dataset seems a bit simple. It would be also nice to see how to consume that information (if it is publicly available).

Finally, authors properly comment the main conclusions and envision some future work.
","gpt-4o-2024-11-20, Jon Wasky",'slightly negative'
2911,"The main contribution of this paper is an ontology called the Intelligent Energy Systems Ontology (IESO).

The manuscript was submitted as 'full paper' and should be reviewed along the usual dimensions for research contributions which include (1) originality, (2) significance of the results, and (3) quality of writing. 

The main contribution is the ontology, I consider that this submission should be rejected, and the authors should be recommended to prepare and submit a 'Description of ontology' type of paper instead. See http://www.semantic-web-journal.net/authors#types


The Related Work section (Section 2) almost only introduces work from the authors (19 self references!). As the core contribution of the paper is the IESO ontology, I consider that the Section 2.1 introducing the MAS Society is totally out of scope. Section 2.2 is more in the scope as it lists the ontologies for the Power and Energy Systems domain.
- SAREF is now in version 3.1.1. The authors should update to this new version
- EMO is first mentionned, and then described again. The section should be restructured.
- The wrong reference is used for SOSA. The authors should update to the W3C rec, or the reference to the Semantic Web Journal.
For such a section I would expect an analysis of the different ontologies in terms of the domains they cover, the terms they introduce, the way they are published, etc. The goal would be to motivate their reuse or the creation of a new ontology.

Instead of self-citing unrelated work, the authors should consider checking out what has been published in the domain, as a lot of related work is missing. To name but a few:

- Zeiler, Wim, and Gert Boxem. ""Smart Grid-Building Energy Management System: An Ontology Multi-agent Approach to Optimize Comfort Demand and Energy Supply."" ASHRAE Transactions 119 (2013): H1.
- Wei, Song, et al. ""Multi-agent architecture of energy management system based on IEC 61970 CIM."" 2007 International Power Engineering Conference (IPEC 2007). IEEE, 2007.
- Hippolyte, Jean-Laurent, et al. ""Ontology-based demand-side flexibility management in smart grids using a multi-agent system."" 2016 IEEE International Smart Cities Conference (ISC2). IEEE, 2016.
- Haghgoo, Maliheh, et al. ""SARGON–Smart energy domain ontology."" IET Smart Cities 2.4 (2020): 191-198.
- Hammar, Karl, et al. ""The realestatecore ontology."" International Semantic Web Conference. Springer, Cham, 2019.
- Kabilan, Vandana, Paul Johannesson, and Dickson M. Rugaimukamu. ""Business contract obligation monitoring through use of multi tier contract ontology."" OTM Confederated International Conferences"" On the Move to Meaningful Internet Systems"". Springer, Berlin, Heidelberg, 2003.   
- Santodomingo, R., J. A. Rodríguez-Mondéjar, and M. A. Sanz-Bobi. ""Ontology matching approach to the harmonization of CIM and IEC 61850 standards."" 2010 First IEEE International Conference on Smart Grid Communications. IEEE, 2010.
- Neumann, Scott, et al. ""Use of the CIM Ontology."" Pacific Northwest National Laboratory (2006).
- Küçük, Dilek, et al. ""PQONT: A domain ontology for electrical power quality."" Advanced Engineering Informatics 24.1 (2010): 84-95.
- Gillani, Syed, Frederique Laforest, and Gauthier Picard. ""A Generic Ontology for Prosumer-Oriented Smart Grid."" EDBT/ICDT Workshops. Vol. 1133. 2014.

The conclusions of Section 2 that there is considerable heterogeneity among the models has not been demonstrated by the authors.

Section 3 describes the methodology for establishing and publishing the IESO ontology, then details each of the modules.
The choosen methodology is probably outdated. See SAMOD or Linked Open Terms. 101 stands for one-oh-one, not one-on-one. See https://en.wikipedia.org/wiki/101_(topic)

The IESO ontology consists of a core module that imports different modules. The authors claim the modules may be versioned, but do not describe how the versions are managed. 
- What is the semantics of the version number?
- What happens if a new version of a module is released?
- The namespacefor prefix ieso: includes the version number v1.0.0. Does this means that there will be a new prefix and a new namespace if the version changes ?

The authors claim that the ontology is published according to the best practices. However they never explain what these best practices are, or justify that they actually implement them. All the modules define concepts with the same namespace <https://www.gecad.isep.ipp.pt/ieso/v1.0.0/>, but the terms do not dereference: For example <https://www.gecad.isep.ipp.pt/ieso/v1.0.0/SystemOperator> returns a HTTP 404 error.
The server seems to implement some sort of content negotiation because Protégé manages to retrieve a machine readable version of the ontology, but I can't manage to find wich is the value I need to set to the Accept HTTP Header. text/turtle, application/x-turtle, application/rdf+xml, application/owl+xml, ... and many other just don't work.

The ieso ontology contains strange datatype properties: ieso:id ieso:name, ieso:number, ieso:type, etc:
- ieso:id - The 'id' datatype property relates an object or instance with its identifier. - why reinventing IRIs ?
- ieso:name - The 'id' datatype property relates an object or instance with its name. - why reinventing rdfs:label ?
- ieso:type - The 'type' datatype property relates an object or instance with its type. - why reinventing rdf:type ?

The authors claim that most modules contain a mapping to existing ontologies. There is no such mapping online.

Actors module
There are many sub-classes of Role. Some are mutually disjoint. They are not described in the article. The article should include some example snippets of how the module is used. For example taken from the companion dataset from reference [76]

Building module:
- why is the ieso:adjoins property inverse of the ieso:intersects property
- there are many sub-classes of ieso:Building and ieso:Space. How have they been chosen ? What is the procedure if one cannot find the appropriate sub-class in this list ?
- the authors claim that the classes are directly mapped to the respective BOT classes using owl:sameAs. This is not true in the Turtle document. Furthermode, owl:sameAs is for individual equality, not for classes equivalences. See Section 9.6.1 of OWL 2 Web Ontology Language Structural Specification and Functional-Style Syntax (Second Edition) 

Contract module:
- This module is not compared to existing ontologies. See for example https://spec.edmcouncil.org/fibo/ontology/FND/Agreements/Contracts/ 
- Some modules use terms (for example: module Demand-Response uses term ieso:hasRemuneration) but it's impossible for a machine to track where this term originates, or if this term is defined somewhere (ieso:hasRemuneration is defined in the Contract module)

Device module
- The Device module just copies part of the SAREF ontology in an old version, without refering to it at all. 
- the authors claim that the classes are directly mapped to the respective SAREF classes using owl:sameAs. This is not true in the Turtle document. 

Measure module
- It is not clear how this module should be used. The article should include some example snippets of how the module is used. 

Power Transmission and Distribution
- It is not clear how this module aligns to the CIM ontology, or to other ontologies in this domain.


Section 4 introduces a case study with companion dataset, queries, rules, etc., as reference [76] with permanent Zenodo DOI. The case study is in the Energy domain, and fails to illustrate exactly how the ontologies are used. The article should be self-sufficient. It should include Turtle or SPARQL snippets, with examples of the input and output.

There is no metrics about the ontologies. It is not clear what is the maintenance plan, how one can contribute, or how widely it isused.

The authors should validate the ontologies using some of the existing and well-documented approaches. See for example 
- Poveda-Villalón, María, Mari Carmen Suárez-Figueroa, and Asunción Gómez-Pérez. ""Validating ontologies with oops!."" International conference on knowledge engineering and knowledge management. Springer, Berlin, Heidelberg, 2012.
- Gangemi, Aldo, et al. ""Modelling ontology evaluation and validation."" European Semantic Web Conference. Springer, Berlin, Heidelberg, 2006.

See also the typical review criteria for ontology resources in Semantic Web conferences.

","gpt-4o-2024-11-20, Jon Wasky",'strongly negative'
2911,"This paper overviews relevant work regarding PES existing ontologies and presents a new ontology describing its purpose, requirements, development options. The application of the proposed ontology is demonstrated in an agent-based simulation of the local grid. In general, this paper is well written and the proposed ontology makes sense, I will recommend accepting it.","gpt-4o-2024-11-20, Jon Wasky",extremely positive
2911,"This manuscript presents the Intelligent Energy Systems Ontology (IESO), which provides semantic interoperability within a society of multi-agent systems (MAS) in the frame of power and energy systems.

The manuscript is very well written, thank you.

Major Comments
Lines 299 to 301. 
The manuscript highlights a limitation identified by reference [69]. “Plus, publicly available models may also become unavailable, making our model obsolete. Furthermore, importing ontologies from cross domains may cause inconsistencies due to heterogeneous definitions of the same concepts, different granularities, among others.” 
This is a significant departure from many developed approaches. In fact, reference [69] integrates existing ontologies in order to form a new ontology for a specified purpose. The approach taken in the manuscript under review is not substantiated by the work that follows in section 3. 
The key weakness in this manuscript results from the absence of sufficient detail when designing the new ontology. A cohesive and comprehensive description of the requirements that the ontology must fulfil is not provided, references 45 and 46 are listed but only a surface level description of requirements is provided. This manuscript currently describes some business processes and tools but does not describe the information required by these tools in terms of objects and properties, for example: the turtle file provided at the link in footnote 9 only contains 9 properties. The sum of these information requirements would underpin the ontology. 

Section 3.2: Why have you taken this approach and not just reused the BOT ontology? From a building geometry perspective the relationships are confusing particularly in terms of geometric topology. Zones can exist over multiple storeys. 

From a demand response perspective, is HVAC not important? Please consider the BRICK ontology.

Case study 
This section is missing the value proposition for the new ontology. Yes, the IESO ontology can be used in the ways described but how does this use case compare with business as usual? Does this IESO ontology work better in some way than the solutions that are currently available?

The case study would also significantly benefit from a process diagram that shows the full range of tools and data exchanges that occur.

Conclusion summarises the developments and demonstrator but is missing a key paragraph that describes what this new ontology does for the world. In other words how is this domain better off now that this ontology has been created and demonstrated?

Minor comments:
35: unnecessary comma between references 1 & 2.
80: wording is incorrect, “provides semantic reasoning” should change to “enables semantic reasoning”
","gpt-4o-2024-11-20, Jon Wasky",'slightly negative'
2911,"(1) Originality: 
The paper describes the Intelligent Energy Systems Ontology (IESO), which aims at supporting an interoperable co-simulation of the power system including market mechanisms. The authors proposed a so-called multi agend system (MAS) society build of different MAS and ontology-based communication. With the ontology the data models of these different MAS are defined and validation and unit conversion are supported. The approach of a ontology-based co-simulation in energy systems context is novel and promising. The used MAS and their ontologies, were already desribed in earlier papers of the authors and are integrated into the IESO. 

(2) Significance of the results: 
The authors give an overview over already available ontologies in the domains of interest and describe how they make use of them for their approach to support interoperability with other tools. The modules of the IESO are described detailled, mentioning the dependencies between the modules and to other ontologies. In the explanation the authors mention, that ""owl:sameAs"" is used for referencing other ontologies, but I did not find this in the online available version of IESO and the related ontologies. 

A case study shows the capabilites for the simulation of a distribution grid with some households and market mechanisms. The data of every step of this case study and the used SPARQL queries are provided at Zenodo, which makes it very transparent. Two of the used MAS are available as web services, but the other used tools seem to be not openly available. Thus, the results can be retraced, but as the tools are not available and there is no instruction how to execute the tools, it is not possible to reproduce the case study and some questions stay open. How are the provided queries and data between the different tools exchanged? Manually or by script/TOOCC? How complicated would it be to change components of the simulation? 

Additionally, an evaluation of the performance of the approach would be interesting. The case study is focusing on a quite small grid with the simulation of only one time step. Of cause, this is sufficient and suitable for demonstrating the process and ontology, but for realistic simulation of the power system large systems have to be considered. Thus, at least some considerations about the scalability of the approach for larger grids, more agents, or simulation of larger time intervals would be helpful to value the relevance of the approach. 

(3) Quality of writing: 
Overall the paper is written well, the approach is described in a well understandable way and figures are used to visualize it. Many figures have a quite low resolution and should be replaced by vector graphics or higher resolution source files. 
Some small remarks: 
- line 41-44: ""... such as: ..., to name a few"" doubled 
- line 19: Oxford comma missing after ""operation""? ""simulation"" instead of ""simulations""? 
- line 258: Is ""hardening"" the right word here? Maybe better ""hindering""? 
- figure 1: There are used two different nuances of orange and different fonts (e.g., boxes ""Decision Support"" and ""Historic Data""). If this is deliberately, it should be explained. Otherwise it should be consistent. 
- figure 1: Some parts of the figure are barely readable, even when zooming in. A vector graphic should be used or at least a high resolution source file. 

Open Science Data: 

(1) Organization of the file: 
In the provided data file a README file is missing. The description of the case study in the paper contains references to the according files in the data, which allows to follow well. 

(2) If applicable, whether the provided resources appear to be complete for replication of experiments and if not, why: 
The provided data seems to be complete to allow reproducing the case study. The Power Flow Service and Electricity Market Service are also available as Web Serives, but the additional used tools (TOOCC, MASGriP, AiD-EM, SSC) seems to be not available and there is no instruction how to use the different tools to execute the case study. Thus, from my point of view, it is not possible to reproduce the case study. 

(3) Assess the appropriateness of the chosen repository if it is not GitHub, Figshare, or Zenodo: 
The IESO and all of it's modules are hosted on a homepage and have integrated the version number in the URL. Due to not using a repository like GitHub, FigShare or Zenodo, it seems that version management and repository-discoverability is not supported and no DOI is used. 

(4) If applicable, whether created data artifacts appear to be fully included in the file: 
According to the description of the case study in the paper, the data artifacts seems to be complete for all the described steps of the process. ","gpt-4o-2024-11-20, Jon Wasky",neutral
2913,"The present paper introduces a new ontology that aims to serve as an agnostic representation of heterogeneous data mapping languages which generate Knowledge Graphs. For this purpouse the authors carried out a deep study of the existing data mapping languages to then produce a kind of meta-language which promise to be able to represent the functionalities of these languages in a language-agnostic way. The objective of this paper is a very valid and strong one, as through the last years more and more data mapping languages have appeared, but unfortunately there are no further convergences, letting alone users to explore this overwhelming field. However, I found somewhat the arguments and motivation used in the paper not strong enough.

It is claimed that the ontology is able to represent all of the languages expresivity, however I would argue that this is a very difficult goal to achieve as the variablity and difference in functionality and expressivity among the languages could be huge. For example, looking to SPARQL-Generate which offer a big functionality and flexibility, it is possible to indicate an iterator (e.g, from 1 to n) to harvest all the data in an API. Looking to the ontology I don't see a way to represent this without loosing the semantics. Additionally, as mentioned in Section 3, xR2RML offers the possibility to push down a value further in a JSON tree-structure in order to cover the inability of JSONPath to get parent nodes. Again, I do not see how this could be represented following the proposed ontology. Therefore, I think that the real scope should be clarified alongside the problem of losing expressivity and how this would enable backwards conversion without losing the original semantics.

Another interesting point is the argument that this ontology would serve as an interexchange language but also as a meta-language that would prevent users to tie to a specific language or engine. It is true that if widely supported this ontology would indeed cover the translation among languages, but also it would need to cover very specific cases and be very flexible which in the end would translate into adding more characteristics to the original ontology. Therefore, this would end up in a even more verbose language which, as being a meta-language, it is already more verbose than existing ones. So, I do not see how the usability would be fostered if users need to spend more time producing the mapping rules. I am also very interested in the rationale behind creating a new language (although its main objective is to be a meta-language), instead of trying to improve an existing language incorporating features from other languages.

Then, in Section 3, I really liked the great effort surveying the existing languages and I think that the tables in Appendix A are of great value. But, I miss two things here, firstly I would add Facade-X (aka SPARQL-Anything) to the comparison as it is a very interesting and new approach. Then, I think that in some parts the text only remarks what is possible and not in the language, while a brief discussion of the pros and cons of these decisions would really benefit the text but also a newcomer to the field. For example, in Section 3.3 Linking Rules, you describe how different languages can make links. However, it is intuitive that if any language is able to generate dynamic subjects then the link will be automatically build if the resulting subject and object URIs are the same. So, it is interesting to further discuss the benefits of using other constructions.

Some other remarks:

ShExML is included as an RDF-based language which is not. ShExML is based in Shape Expressions but unlike ShEx it does not have an RDF syntax. Also, in Listing 3, the ShExML mapping rules would not work as the ""$."" included in FIELDs is not required. 

In the introduction you mention ""Deciding which language and technique should be used in each scenario [...] that reduce reproducibility, maintainability and reusability."" I guess that the real problem is not having the right tool to do the mappings and that the analysis of time-benefit for using and learning these tools is higher than going for ad-hoc solutions. I think that finding one tool that suits all the needs is quite utopic and then different tools should be used (if available). Then, thanks to the RDF compositional property the results can be merged seamlessly which is a big benefit when compared to data integration in other formats.

In Section 2.2 you cite [21] as an initial analysis of 5 mapping languages. I think the correct one is:
De Meester, B., Heyvaert, P., Verborgh, R., & Dimou, A. (2019). Mapping languages analysis of comparative characteristics. In 1st International Workshop on Knowledge Graph Building, co-located with the 16th Extended Semantic Web Conference (ESWC 2019) (Vol. 2489).

In Section 5 you mention the benchmarks as one possible use case in order to not having to produce mapping rules for each language being tested. However, in a benchmark you would need to use specific features in order to really asses the performance of this language/engine, if you rely in translation then you are introducing a bias which is the translation itself that can be better or worse depending in the target language.

Also in Section 5 you say: ""This situation is directly related to the adoption of these technologies"". This is a strong claim, just support it with some references or drop it.

In Section 6 you conclude by saying ""where mapping rules are first-class citizens"". Although this is a well-known term in programming languages theory it is not clear what you mean here.

You should review carefully the format of the references. Some mistakes: [4] no venue/journal, [10] no authors, [13] iot (capitalise) 

Just out of curiosity, how would the ontology support hierarchical data, namely accessing nested levels in the hierarchy? Would it be using join conditions or has it some direct support?

A very picky one: You mention in Section 2 that YARRRML is a serialization format for RML, in that sense I would expect to be able to convert back and forth from and to RML but it is not the case in its conception. Therefore, I would define YARRRML more like a compact-syntax for RML rather than only a serialization format.

Typos:

Abstract:

capabilities of current mapping languages -> capabilities of the current mapping languages

Introduction:

This analysis studies how languages describe the access to data sources, how triples are created and their distinctive features: and is presented as a comparative framework. -> This analysis, presented as a comparative framework, studies how languages describe the access to data sources, how triples are created and their distinctive features.

Related work

existing mapping languages, summarized in Table 1 -> existing mapping languages, listed in Table 1

Comparison framework

the main features included in mapping languages -> the main features included in the mapping languages

General features for Graph Construction:
Generate feature that apply yo statements -> Generate feature that apply to statements
","gpt-4o-2024-11-20, Jon Wasky",slightly negative
2913,"There are some minor grammar errors (in general, I would suggest reviewing the use of the prepositions, they are not always correct) or unfortunate phrasings,
but overall, the paper is clearly written. However, I think the reproducibility of the method and validation of the result is not fully discussed,
and I'm having a hard time trying to review the following recommended indicators:
- the design principles (throughout my comments below, you can see that I'm a bit confused about what the scope of the Conceptual Mapping is supposed to be, and thus question some design decisions)
- comparison with other ontologies on the same topic (other ontologies are extensively discussed, but a clear alignment between the Conceptual Mapping and these other ontologies is missing)
- and pointers to existing applications or use-case experiments (this seems to be future work)

More specifically:
- The comparison framework: it is currently unclear what this entails, however, I have the feeling you were rather ""collecting stamps"" than ""physics"" (I don't want to sound negative, both have merits): analyzing existing languages and extracting their features, and not trying to come up with a complete set and map that to languages.
Please be very clear about what you are doing, clarify, and provide argumentation to make your work more reproducible (e.g., how do you detect certain features: based on the specification, a set of test cases, verified expert opinion, ...?).
- The mapping language does not seem validated: please provide argumentation as to why certain modeling decisions were made, and provide some proof that the mapping language is validated. As this is an ontology paper, I currently have trouble finding the proof for ""Quality and relevance of the described ontology (convincing evidence must be provided)"".
- Some of the arguments of the discussion section come out of the blue for me, e.g. the provenance, applicable shapes, extensible character: I fail to understand how these have been extracted from existing mapping language features, and if not extracted, why they are added.

That said, the work is valuable, relevant, and timely. I believe the hardest work has been done, and many of the comments I have are requests for clarifications.
However, without a validation of the actual result (i.e., a comparison/alignment with the other languages, to showcase you indeed cover all bases),
I would not be inclined to accept it, hence I suggest a major revision.

Introduction

- I really like the conclusion of p2 left column line 26
- Please clarify whether you made an ontology to unify definitions across mapping languages, or a mapping language that is a superset of existing mapping languages. This is currently not clear. Given this was submitted as an Ontology Description I assume a ""short paper describing ontology modeling and creation efforts"", however, I have the feeling this is not the case here. If this instead is supposed to be a full paper, I would assume to review ""originality, significance of the results, and quality of writing [...] and more specifically the evaluation sections in a style and level of detail that enables the replication of their results"".
- I commend for clarifying the usage rights, persistent identifier, and clear documentation of the ontology. However, I found the following errors:
  - I'm quite surprised a mixture of rdfs:comment and skos:definition is used for the ontology, that doesn't feel right.
  - http://vocab.linkeddata.es/def/conceptual-mapping/protocols_list.ttl returns a 404

Related work

- For 2.1, I would appreciate some discussion on which criteria you used to select this set of mapping languages to compare with, and not, e.g., SPARQL-Anything, XRM, or SMS2.
- In 2.3, for me, it is not clear why in Mapeathor the spreadsheet is language-independent. Mapeathor imposes a specific structure within the spreadsheet (is that not 'a language'?), very similar as to how, e.g., YARRRML imposes a specific structure within a YAML document. Neither change the underlying serialization (and for completeness, YARRRML also supports translation into R2RML). That said, this kind of discussion is quite interesting: what kind of distinction is there between e.g. Mapeathor and YARRRML vs RML and SPARQL-Generate? What changes if someones builds, e.g., a tool that directly works on YARRRML mappings instead of translated RML? In general, I'm not sure this distinction is relevant for this paper.
- For 2.3, I miss a concluding remark, for now, I don't understand why this section is included (translation is a future work, AFAICT)

Comparison framework

- You state which languages you include, but there's no argumentation why those (similar comment than my first one of the related work section). Can you provide a more rigorous argumentation as to why you include specifically those?
- The example is a quite limited RDF structure -> no rdf:Lists or similar constructs, no graphs. I'm currently not convinced this is a complete example that touches your complete ontology/language.
- It is for me not clear whether this language/ontology is meant to be abstract (so tries to attempt completeness), or rather a superset of existing languages (so tries to cover everything that existing languages cover). There is a distinction between these two, so clarifying that scope is important.
- For example: Data retrieval: the fact that there are 3 retrieval 'modes': is this complete, or is this extracted based on the features of existing mapping languages? [1] describes factors that influence an RDF graph generation algorithm, and makes the distinction between 'real-time' and 'on-demand' trigger, where I can see that your distinction 'Streams' maps to 'real-time', and 'Asynchronous' and 'Synchronous' are two types of 'on-demand' triggers (event-based could be a third type of on-demand trigger). I'm not saying one categorization is better than the other, but some argumentation would be good. If the point above is well tackled, this point should become a non-issue.
- For the data source description, I would expect a discussion on the extensibility of the language vs support of tools implementing the language, e.g. RML does support Streaming data sources (https://github.com/RMLio/RMLStreamer#processing-a-stream), but this is indeed not specified in the original paper or specification, since RML provides an extension point concerning data source descriptions, and is only implemented in the RMLStreamer. How could you compare such features on a language level?
- p8 right line 43: please clarify provenance, it is not clear. Given that you state ""No language considers the specification of its provenance"", I have the feeling you attempt to create a complete set of features. I would strongly suggest not going down this path and instead trying to create a superset that (only) contains the features that are currently supported by your set of mapping languages. Otherwise, I would need some argumentation (and preferably, theoretical grounding) why some features are taken into account, and why some aren't.

Mapping Language Ontology

- For reproducibility, I would expect that the requirements specification is (publicly) available
- p11 left line 9: so the set of functions a mapping may use is predefined, not extensible. How is Conceptual Mapping then an abstraction of existing features? (e.g. FunUL supports function extensions)
- I think the mapping validation is important, however, I could not find the results described or linked to in the paper. Based on what I read up till now, I would expect some validation document that states 'feature X is supported by construction Y (or a combination of constructions)', and as such, you can prove that you cover all features. Or, if impossible, argue why this is not provided. Otherwise, I cannot review ""whether the provided resources appear to be complete for replication of experiments, and if not, why"".
- For example, it is unclear to me which feature the CombinedFrame construct solves.
- How do you specify that an expression is either an XPath or a JSONpath, or 'among others'? Why is this not a SKOS ConceptScheme?
- By linking the datatype to the statement, don't you get in trouble when you want to create mixed-type rdf:Lists?
- From the features listed in Section 3, I don't understand why the Ontology or shapes constructs were required.

Discussion

- p15 left line 35: the problem of 'lack of information on valid combinations' has not been clearly explained up till now.
- p15 left line 45: the purpose of this language has, up till now, not been validated: there's no overview of how existing mapping language features are supported by conceptual mapping language.
- p15 right line 3: please exemplify and better argue why provenance definition and shape applicability are relevant in this paper. For now, it is unclear. Same with extensibility: currently it's a bit unclear this is possible (I assume you mean you can add metadata triples to the existing mapping graph? is that an extracted feature?)
- I kind of disagree that mapping governance has not been developed so far, e.g. http://events.linkeddata.org/ldow2016/papers/LDOW2016_paper_04.pdf (last page) showcases how author metadata could be added, using PROV-O statements.
- I find the maintenance guarantee a bit underpromising (who is ""we"" in this case?), can this be made stronger, e.g., some public statement on Github, an endorsement by an organization instead of a group of people?

Conclusion and Future Work

- ""an ontology-based conceptual model that aims to gather the expressiveness of current mapping languages"" --> so why also include other features (such as provenance) that are mentioned in the comparison framework to not be considered by any language?
- ""Finally, we want to specify the correspondence of concepts between the considered mapping languages and the Conceptual Mapping"" --> I'm very much confused by this statement, then what is the Conceptual Mapping at this point? How can you be sure that it currently gathers the expressiveness of current mapping languages if you don't have this correspondence table?

-- spelling/grammar/details

I personally prefer using Oxford comma consistently, for both 'and' and 'or'.
I tagged some phrases that were unclear in the PDF at link https://www.dropbox.com/s/4hr7lobx1fm006v/swj2913_bdm.pdf?dl=0","gpt-4o-2024-11-20, Jon Wasky",slightly negative
2913,"The paper describes an ontology about mapping languages that captures the commonalities between different mapping languages and technologies like R2RML, XSPARQL, CSVW, ShExML, etc. 

Given that the paper has been submitted as an ontology description, I will use the criteria defined by the journal to assess these papers.

(1) Quality and relevance of the described ontology. 

The ontology has been defined to give support to the problem of categorizing different mapping technologies that have appeared in the last years. The exercise of defining this ontology can shed light on this topic and is relevant as it can improve existing approaches. 

The ontology has been developed following a well-established methodology and the ontology itself follows best practices. 

One possible drawback is that the ontology seems to have been defined by members of the same research group, without a proper consensus work involving external stakeholders which could help improve the acceptance of the ontology. 

In the discussion, the authors say that “we guarantee that the ontology will evolve incorporating new features of the mapping languages, ensuring transparency and the correct translation among them. For example, at the moment of writing, the RML mapping language is evolving towards an updated specification throughout the W3C Community Group on Knowledge Graph Construction.” Although that is indeed a very good ambition, there is no clear evidence about it. It would be more natural that the ontology could already handle the different extensions and proposals that are being discussed in the Community Group. In that sense, it would be great if it was possible to add some provenance information to the concepts included in the ontology to the discussions about those concepts in the Community group or in other venues, or at least some mechanism to track the relationship between them.

(2) Illustration, clarity and readability of the describing paper. 

Overall, the paper is very readable and the ontology has been defined following a sound methodological approach. The ontology has been published using tools like Widoco and Ontoology and has a dereferentiable URI at: http://vocab.linkeddata.es/def/conceptual-mapping#

The ontology is available under a github repository and contains a README, although the README contains only a picture of the ontolog. I would recommend the authors to improve the quality of the README adding more information about how to contribute to the ontology, what methodology has been used, some examples, etc.

There is not enough information about which ontology patterns have been used in the design of the ontology. 

The authors indicate that they have been using competency questions and statements that have been validated by the stakeholders, however, I couldn’t find information about which competency questions were used or which stakeholders have been participating in the process. 

They also say that the ontology was evaluated with OOPS! And that some minor issues were found and fixed. It would be nice if those issues or new issues appeared in the github repository and in general, if the ontology development process followed some practices commonly employed for open source projects using github issues. At the moment of this writing, I could only find one issue…I encourage the author to keep maintaining the ontology and creating more issues documenting that process.

The authors declare that a real use case is the development of the GTFS-Madrid-Bench where different mapping languages have been used. However, it is not clear if the authors have also used the ontology instances that define those mappings and if there could be some translation between those ontology instances and the real mappings. In my opinion, the github repo could improve if it contained some examples of those mappings defined as ontology instances and converted to the real mappings.

(4) whether the provided data artifacts are complete. 

In my opinion this ontology is a first version of an ontology proposal which has not yet been reviewed or discussed by the community and the stakeholders. Although the idea may be interesting, it would be important to define some tools that could be used to translate instances using the ontology to real mapping scripts, which could help assess the completeness of the concepts represented in the ontology.

The approach followed by the authors based on the creation of an ontology instead of a mapping language is interesting but it is not clear what are the advantages of this approach over other approaches in practice. I mean, although having a common ontology to define the different mapping concepts is beneficial to understand better the domain, it would also help if there were some tools that could convert existing mapping languages to instances of that ontology, and vice versa, from instances of that ontology to some existing mappings.

Some minor typos/comments:
Figure 3.b contains “La Coruña” while table 3.c contains “A Coruña”. I am not sure if the authors provide some tools to unify both names which would also be interesting, or it is just a typo.
In page 11, the authors say that a source frame corresponds to a data source and defines which is the data in the source that is retrieved and how it is fragmented with expression using XPath, JsonPath, etc…how are those different expression languages distinguished? Would the ontology need another property to declare the language employed in those expressions?
Page 13, the mapping use<s> the data sources…
Page 14, listing 9 uses the prefix cmf in, for example, cmf:concat, however, I think the prefix cmf is not declared in the ontology
In the discussion section, the authors say that conceptual mapping can ease the knowledge graph generation process. However, it is not clear why the instances of the conceptual mapping ontology like, for example, the running example in listing 9 and 10, could be more usable than the ShEXML example in listing 3. Maybe, the authors could add some discussion about how to improve the usability of those mapping languages.
The authors also declare that the GTFS-Madrid-Bench uses mappings in R2RML, RML, CSVW and xR2RML that were created manually and that having tools that allow translation between those languages would facilitate those tasks…however, the current proposal seems to be only an ontology. Are the authors considering creating such translators between the ontology instances to those mappings? If it is still work-in-progress, I am not sure if the authors consider a better approach to use an ontology or if they have considered defining an intermediate language instead of the ontology, which could be easier to manage than ontology instances in RDF. Maybe, some discussion about alternatives to solve this issue could improve the paper.
Reference 2 contains J. E. L. Gayo, when it should better be J. E. Labra-Gayo and  Reference 10 lack the authors.
","gpt-4o-2024-11-20, Jon Wasky",slightly negative
2914,"This manuscript showcases a Stream Reasoning application in the healthcare context. It is well written and appropriately organized. The goal of the research is clear and relevant. The originality is low. The same authors published a similar paper in [1] on September 2021. The significance of the results is also low. The paper presents a use case. It demonstrates that it is possible to solve the problem using Stream Reasoning, but it does not prove that it makes sense. I would expect to see a comparative study that shows advantages and disadvantages compared to a solution purely based on a stream processor engine (e.g., esper, ksqldb, flink, or Spark Structured Streaming). 

The data file provided by the authors under “Long-term stable URL for resources” is well organized and contains a README file. The provided resources are published in their institution GitLab and appear to be complete for replication of experiments. I did not import the project using an IDE, but the artifacts appear complete. 

As a side comment, I invite the authors to stop using c-sparql and use rsp4j [2,3] instead.


[1] Mathieu Bourgais, Franco Giustozzi, Laurent Vercouter: Detecting Situations with Stream Reasoning on Health Data Obtained with IoT. KES 2021: 507-516
[2] Riccardo Tommasini, Pieter Bonte, Femke Ongenae, Emanuele Della Valle: RSP4J: An API for RDF Stream Processing. ESWC 2021: 565-581
[3] https://github.com/streamreasoning/rsp4j","gpt-4o-2024-11-20, Jon Wasky",neutral
2914,"While deep learning technologies have been used in many out-of-hospital health monitoring scenarios, they are suffering from the pain of lack of training data. Collecting data from patients is time and resource consuming, and it may also have some ethical issues. This paper provides a novel stream reasoning based approach to overcome the lack of training data, and it is really appreciated that the authors are taking an empirical approach to the problem.
The paper is well-motivated, well written and well designed. Firstly, the paper clearly identifies the current research gaps on live health IoT data: lack of data interoperability, context consideration and training data. Then, this work proposes an ontology to represent the patient's medical information, activities, events and the sensors observing particular properties of the patient. The paper also illustrates constraints and situations concepts to interpret the values measured by sensors in regard to the patient's static context. The evaluation of the ontology is based on experimental analysis of real body temperature. I suggest the following improvements:
1. Section 3.4: Consider the subtitle ""ontology validation"" instead of ""ontology evaluation"". Ideally, ontology evaluation for real-world applications focuses more on completeness and expressiveness. OOPS! (OntOlogy Pitfall Scanner!) is a very good tool to detect the ontology pitfalls, but it is designed for validating the structure, architecture and design of the ontology.
2. Section 5.1: Figure 3 is a good example of the ontological representation of EMR. However, one specific example may not be enough to evidence the coverage of the ontology. It may need a quantitive analysis to make it more convincing. For example, ""We experiment with the ontology on XXX number of EMR, and it covers an average XX% of the concepts.""
3. Section 5.2: It could be better to clarify constraints, context and situation. Ideally, the situation refers to a high level of context and is inferred from the aggregation (fusion) of multiple pieces of context. For example, in Table 1 and Table 2, C1-C5 refer to the temperature context, while C6-C8 refer to the time context. In this case, the situation is a high-level abstraction from temperature and time context. Compared with constraint->situation, a more logical flow is constraint->context->situation.
4. Section 5.3: The expressiveness of the ontology needs a short paragraph. It could be a comparison of ontological stream reasoning with other techniques mentioned in Section 2.1. The results will be very engaging if stream reasoning shows (1) significant time reduction, (2) better performance on limited training data, and (3) successfully detect the personalisation and provide customised alert service.

Additional comments:

1. Section 3.3: The relationship (i.e., object property) of the ontology can be integrated into Figure 1, which could save some space for other sections.
2. Be careful about the use of words and punctuations. For example, on Page 4 Line 31, use ""his/her"" or ""their"" instead of ""her""; On Page 1 Line 46, use ``’’ instead of """" (LaTex specific).
3. Add one short paragraph to discuss the severity of the importance. From a practical perspective, a health monitoring system should be able to distinguish the severity of the importance, such as Critical, Important, Moderate and Low, etc.","gpt-4o-2024-11-20, Jon Wasky",'slightly positive'
2914,"The authors present a study about how to detect the emergency situations of patients based on data from medical devices using semantic stream processing techniques. Hence, the authors introduced an ontology for semantically annotating data from medical devices using the RDF. After that, they demonstrated how their system can detect the high temperature of a patient during night. 

The quality of research and presentation of this manuscript has not reached the standard of the semantic web journal yet.
Overall, the work has to be improved:

1. The authors claim that they have been using stream reasoning techniques in their work. However, what they demonstrated is only using a continuous query. They did not explain how new knowledge can be inferred or what reasoners they have been using. As their context is to monitor real-time data, the processing time of the reasoners should be concerned. 

2. The given example (in Section 5) is too trivial. In the situation given in the example, semantic annotation for the data does not give any benefit. 

Otherwise, the manuscript is essentially a copy of their previous publication[1] without significant improvement. The content (text and figures) of sections 1, 2, 4, 5, and 6 remains the same with a few rewritten sentences. They have a small extension of their ontology and ontology evaluation in Section 3.4. However, the ontology evaluation section does not make sense as the tool they used in the evaluation has not become a standard yet. 



1. Bourgais, Mathieu, Franco Giustozzi, and Laurent Vercouter. ""Detecting Situations with Stream Reasoning on Health Data Obtained with IoT."" Procedia Computer Science 192 (2021): 507-516.","gpt-4o-2024-11-20, Jon Wasky",'strongly negative'
2915,"The paper presents the output of the beArchaeo project, which includes the formalization of an ontological structure that extends the capabilities of cultural heritage models associated to CIDOC-CRM, especially CRMSci and CRMArchaeo, to encompass other aspects of the praxis of archaeoogical investigation such as archaometrics, and by extension their reach into other related disciplines such as chemistry. The model is enjoying adoption within the beArchaeo community and alignments with well-known controlled vocabularies like AAT.

There is a prior version of this paper that was reviewed earlier. Compared to that version, it appears that the authors have put a lot of effort into heeding the comments of the reviewers and I agree that the paper is in much better shape right now, therefore I will only highlight the last few changes required and elaborate on the new content.

The ontology description has been given adequate space and structure and now elaborates on the characteristics that a reader of this journal, even on a domain-specific special issue, will find themselves comfortable with. The structure of this section is, in fact, so detailed that it makes me wish other sections were similarly structured: particularly, Section 3 is very long and, since there is a perceived sequencing of the phases of the data curation model, perhaps numbered subsections reflecting these phases would be more inline with the ontology section structure.

The authors have provided the methodological underpinnings of their development process: the choice of the NeOn methodology makes perfect sense, however, in the interest of having the paper as self-describing as possible, it would be useful to spend a few words reminding the reader what the scenarios are about. It's enough to just quote the scenario title from the NeOn book (e.g. ""Scenario 2: Reusing and re-engineering non-ontological resources"") and let the reader look it up for more details.

I also observe that it is now much clearer in which ways the beArchaeo ontologies relate to other models like ArCo and thesauri like AAT. What I find myself less comfortable with is the choice of how to represent alignments. The authors have opted for bespoke terms for the vocabulary to refer to (e.g. hasGettyAATMaterial): this is one of the possible ways to do it and is used for e.g. authority control in Wikidata, but there are concerns about the flexibility of this model as it fuels the expectation that the ontology will need to be extended should another vocabulary, say GND, be incorporated. An alternative way would be to use a single hasMaterial property that may reference values in AAT, beArchaeo and other indistinctly, and attach provenance information to these alignments as needed (though that might be best achieved using more recent methods of annotating statements like RDF*).

I am also interested to know if the authors have looked into how basic knowledge patterns of what they modelled in beArchaeo, if any, have been represented in foundational ontologies (e.g. how measurements, regions and parameters are modelled in DOLCE/DUL) and if subsuming concepts from these ontologies could be considered.

What seems to be the most important addition to this new version is a preliminary evaluation section. This is largely based on a discourse that is part qualitative, part describing how beArchaeo is situated in its research network. Obviously, it would have been unrealistic to expect that a full-fledged user evaluation be scheduled and carried out between revisions of the paper, so on that basis the provided content may even suffice. I still wonder if some more context could be provided to substantiate important statements like ""The archaeologists have found the model accurate"": for example, with a few quantitative data about the size and nature of the group that gave such feedback, or if the process to reach such accuracy (did they also argue about completeness by the way?) was iterative and based on multiple feedback rounds or not. Within the boundaries of what is sensible to expect, the more concrete this section can be made the better.

For a stable resource link, the authors have supplied the URL of an OWL/XML document that imports the entire ontology structure. This is acceptable as the authors have also generated LODE documentation as requested and provided its URL in the paper: perhaps the authors could clarify if the entire parent directory http://www.di.unito.it/~vincenzo/ontologies/beArchaeo/ is a relevant reference for the paper content as well?

Several footnotes (those linking to CRMsci, CRMarchaeo, ArCo and others) have corresponding publications, journal/conference papers as well as white/technical papers: please turn as many of these as possible into bibliographical references.

More detailed things to look into (notation in page:line range)
* 3:14-15 - including representations of specific domains is not a job of the Semantic Web paradigm, please rephrase (one possible way is to replace ""pardigm"" with ""schema coverage"" but there might be a better formulation)
* 10:44-46 - ""the beArchaeo ontology comprises three modules: [...]""
* 10:48 - ""non-ontological sources""
* 10:17 - ""NeOn"" is capitalized like this
* 11:10 - rather than providing a human-readable version of the ontology, LODE automatically produces ontology documentation.
* 12:32 - I understand the reason for an isEqualTo propety separated from ontological equivalence, but perhaps the property could be renamed to reflect that the equivalence relation is that of belonging to the same stratum
* 14:32: capitalized as PROV-O
* 15:37 - shouldn't it be ""thermoluminescence"" with an 'h'?
* 17:29-30 - ""[...] that are being used for interpretation and will be the basis for the final exhibition.""
* 17:44 - ""Some interesting issues also rose"" (or ""were also raised"")
* 18:46 - ""The conceptual model _is_ the outcome"" (or ""was"")
* 18:32-35 - that this is the first born-Semantic archaeological approach is a rather bold statement: would one argue that prior projects like Pelagios do not apply? I would advise a scrutiny of the state of the art in e.g. the Digital Classicist wiki at https://wiki.digitalclassicist.org/Category:Linked_open_data (by the way, pleae make sure to be there).","gpt-4o-2024-11-20, Jon Wasky",'slightly positive'
2915,"The paper can be accepted. 
I have however some recommendations for future work on the subject. 
1) Obviously it is unlikely that the first version of an ontology is perfect. It will be necessary to refine what at present seems a very complex structure, with some possible redundancy in the class & property definitions. One example: why there is a new class FormationProcess, subclass of A4 Stratigraphic Genesis? Was the latter insufficient? Was this new class strictly necessary and a solution with   ""P2 has type"" not suitable? Thus I would consider the paper a first attempt, requiring further work to be implemented but still worth publishing. In the meantime, implement Occam's razor!
2) The repository part is still very preliminary. This is acceptable at the present level of development, where the focus is on the theoretical aspects of the ontology and not on its implementation, but it will be necessary to go beyond Google drive (as authors also state). I would not consider this a secondary aspect in the future, nor an easily solvable one.
3) Same comment for the use of Omeka-S. Is it the appropriate tool with all the required functionalities? I am not sure it is powerful enough. The choice of the package should go hand-in-hand with the repository solution.
However, such data management aspects (points 2 & 3) can be addressed in future work and it is useful to publish the present one as is, not least to allow a discussion on the matter and compare different solutions.","gpt-4o-2024-11-20, Jon Wasky",slightly positive
2915,"With the revised version, I feel that the authors have improved the quality of their paper significantly. More so, all my original points are addressed either completely or close enough, except for the lack of a proper evaluation. 

While I greatly appreciate that the authors have added a whole section on preliminary evaluation, I still find it worrisome that, for a research paper, no proper evaluation with metrics and significance testing has yet been performed. It is comforting to read that there have been discussions with some domain experts, and that these discussion have already led to new insights and possible improvements, it still tells us little about how well the archaeometric domain has been modelled by the authors or how effective the model is for use by its target users, let alone whether any of these results is actually significant. With the seemingly close ties between the authors and the archaeological community, one would assume that doing a workshop with 15 to 20 participants, who would then test and evaluate the model, would not be too difficult to set up in a short amount of time. That the authors decided not to pursue this direction remains a sore spot of the paper in my opinion. That the authors also do not mention the number of experts who partook in these preliminary evaluations, whether they are part of the BeArchaeo project, and what form these discussions took, only increases my scepticism.

As for the PURL link, despite the authors confidence that this link is working correctly, I'm still welcomed by the same 404 page as during my original review: https://www.di.unito.it/errors/404.html. Since PURL is clearly not at fault, I would suggest that the authors check with their own IT department why this might be the case. Also, I suggest that the authors move their ontology from their personal webpage to some more durable repository, like GitHub or Zenodo, to avoid further issues. Since the future of PURL has become a bit uncertain the last few years, the authors might also want to consider using one of the more recent alternatives, like w3id.org.

Finally, I want to stress that I think that the work done by the authors is very relevant to the archaeological domain, and that it deserves a publication, but that, as mentioned, it is the lack of proper evaluation that prevents me from giving it the green light. However, if not here, then I encourage the authors to publish it elsewhere after having done such an evaluation.","gpt-4o-2024-11-20, Jon Wasky",'slightly negative'
2918,"I thank the author for addressing all my previous comments. 
I tested the newly provided SPARQL endpoint at [1] from my command line and it works fine. Still, I would encourage the authors to provide a working on-line interface for the endpoint at [1] in order to increase the dataset ease of accessibility.


All in all,  I would like to accept the paper.

[1] https://linked-maps.isi.edu/sparql
","gpt-4o-2024-11-20, Jon Wasky",'slightly positive'
2918,"The paper describes a method to generate RDF datasets for historical maps, whose topological features change over time. The method was evaluated over a selection of US historical map data.

Having reviewed the previously submitted version of this paper, I may observe that this version contains the necessary improvements without introducing further issues. The authors have addressed my remarks both in the paper itself and in the author response, and have even addressed my ""curiosity"" comment in the concluding section. They have relicensed the data properly, made a release and improved its findability. In the future, they might consider separate releases for the data and for the code, although they will need to keep sharing the GitHub repository so as not to invalidate the stability of the supplied URL.

I have no further recommendation for this paper and believe it is ready for publishing. Good work.","gpt-4o-2024-11-20, Jon Wasky",extremely positive
2918,"* Summary: The article describes an approach to convert vector geographic features extracted from multiple historical maps into contextualized Spatio-temporal KGs. The resulting graphs can be easily queried (GeoSPARQL) to understand the changes in different regions over time.  The approach and its evaluation focus on linear and polygonal geographic features.


* Overall Evaluation (ranging from 0-100):
[Criterion 1]
	[Q]+ Quality: 95
	[R]+ Importance/Relevance: 85
	[I]+ Impact: 90
	[N]+ Novelty: 80
[Criterion 2]
	[W]+ Clarity, illustration, and readability: 95
[Criterion 3]
	[S]+ Stability: 100
	[U]+ Usefulness: 90
[Perspective]
	[P]+ Impression score: 90


* Dimensions for research contributions (ranging from 0-100):
(1) Originality (QRN): 87
(2) Significance of the results (ISU): 93
(3) Quality of writing (QWP): 93


* Overall Impression (1,2,3): 91
* Suggested Decision: [Accept]


* General comments:
The work is solid.  The paper is easy to follow and understand.
There is a novelty in the proposed approach.


* Major points for improvements:
{
# all have been addressed
}


* About the data files and related software artifacts: (“Long-term stable URL for resources”)
(1) ""data files are well organized and contain a README file which makes it easy to assess the data"": [YES. CORRECTED]
(2) ""the provided resources appear to be complete for replication of experiments"": [YES.  COMPLETE]
(3) ""the chosen repository is appropriate for long-term repository discoverability"": [YES. GitHub]
(4) ""the provided data artifacts are complete"": [YES]


* Minor corrections:
[A06] Indeed, this (unrotated) box is the required input to the reverse-geocoding service and poses a limitation for us. We have clarified it and described how the calculation is done (section 2.4, paragraph 5).
[R06] In the manuscript, you didn't mention that is a ""limitation"" of the reverse-geocoding service.  Perhaps, it would be better to mention this part in section 5, paragraph 5, where you described ""several limitations"".
","gpt-4o-2024-11-20, Jon Wasky",'extremely positive'
2920,"## Overall:

The paper presents a ""semantic meta-model"" for the precision agriculture and livestock farming domains. The main motivation the authors provide for this model is the need for data integration from multiple heterogeneous sources. They particularly focus on spatial, temporal and data access metadata. The paper takes for granted that such a meta-model is needed (i.e. is absent from the literature or unavailable) which might be true but given the plethora of existing vocabularies/standards/ontologies in the semantic web and the agrifood sectors needs to be justified a little more carefully. 

Fundamentally, the meta-model provides a way to annotate data sets with metadata which can then be queried to determine if the underlying data set has relevant data. As such the approach does not involve mapping or translation of the underlying data to a common format (which may or may not be a strategically sensible approach). It is a peculiarity of this paper that there is repeated reference to the formulation of SQL queries, and yet all the worked examples in Section 6 are using SPARQL.

Overall the paper presents an interesting and relevant approach to the use and integration of heterogeneous data sets. There are a number of conceptual inconsistencies or perhaps design decisions which would benefit from greater clarity of explanation and thus improve the paper overall and its utility to future readers and developers (see below some issues.
Originality: Somewhat original in the extension of the DCAT model and the deeper description of the form of underlying data sets. Lots of conceptual issues remaining including whether whether the approach could obtain wider adoption, and what kind of web architecture would support more widespread use of this approach. Review of existing literature somewhat limited as there is a lot of work building on DCAT which is not mentioned.
Significance: For the agrifood sector, this paper may have some impact. However, the narrow focus on time and location means there are many other aspects of data integration which remain unaddressed.
Quality of writing: Generally excellent, well structured, almost no grammatical mistakes. 
Resource: The data model is available on the OGC website and provides a stable URL. There is significant documentation here for ensuring subsequent further use. The model and its availability appears to follow the FAIR data principles.


## Detailed comments:

* Sect 1
	- It might be useful to explain what the authors intend by a ""semantic meta-model"" as opposed to a simple ontology or other classificatory structure.
* Sect 2
	- Sec 2.1 is unnecessary given the audience of this journal
	- Sec 2.2 and 2.3 are catalogues of relevant vocabularies but do not provide us with any critical insight. It is usual in the ""background and related work"" section of a paper yo make clear not just *what* another piece of work does but also what *weaknesses* there are.
	- Sec 2.3 - the selection of agrifood vocabularies/ontologies mentioned seems arbitrary, and given the large number of existing vocabularies perhaps some criteria for the selection of this set of item could be offered - why are these mentioned and not others?
* Sect 3
	- ""functional and non-functional requirements"" -- this does not make sense when speaking of an ontology or data model. Reading section 4.3 one can see what you mean by this but this is a very non-standard way of interpreting or defining ""functional and non-functional requirements"". See comment below.
* Sect 4
	- 4.3 ""requirements for the proposed semantic model in terms of functional requirements (i.e. activities that can be facilitated by the use of the model) and non-functional requirements (i.e. aspects/concepts that need to be covered by the model)"" -- here I would expect the use of the term ""competency questions"". Later use of the term ""competency aspect"" is not really the same thing.
* Sect 5
	- The main categories of metadata described seem to be largely concerned with providing metadata about a dataset, rather than providing a generally applicable data model i.e. to enable querying and interoperability (????)
	- ""However, the qb:dimension and qb:measure properties have the qb:Component Specification as domain the and cannot be used directly at the dcat:Dataset."" -- grammar!
	- AGROVOC is chosen as the vocabulary to define the dcat:theme of datasets. Why? No justification is provided for this choice and this is an important design choice. (Obviously other uses could make different choices while using the same meta-model). One reas
	- ""datasets could declare conformance to the proposed model"" -- this is not clear to me because the model basically allows one to describe a data set not to ensure the data model used in the actual data follows this or any other model. e.g. weather data could be in multiple formats/models and its metadata could be described with this model nonetheless. There is limited mention of mapping in this paper but it seems to mostly concerning time and location.
* Sect 6
	- ""In order to enable the generation and execution of SQL queries based on the metadata, the label (rdfs:label) of the dimension/measure should be the same as the corresponding field at the database table where the dataset is stored."" --- so if I have understood correctly all the metadata description of the data sets is there to enable an SQL query to be issued. This seems a very non-semantic approach so an explanation of this approach would be in order here.
	- ""In this dataset the temporal coverage (February 2016) is defined using the reference.data.gov.uk Time Interval vocabulary (the example uses the prefix uk-month)."" --- but in the previous example xsd:dateTime is used, so how does the meta-model actually unify time (mentioned as a key dimension for integrating data sets)? In the next paragraph it says ""Note that the time dimension URI is the same as the one used at listing 5 in order to facilitate the interoperability between datasets."" so this further confuses the reader. Please make clearer what is going on here.","gpt-4o-2024-11-20, Jon Wasky",slightly negative
2920,"This paper introduces a meta model to semantically enrich and integrate precision agriculture and livestock farming.  The model reused several widely adopted standards, including DCAT, QB, and PROV-O. The main contribution is the creative linking between DCAT and QB through a SHACL shape so that data measures are associated with datasets. Plus, the paper is well-structured in general and is relatively easy to follow. Below are my major comments:

(1). The meta model is proposed for the domain of precision agriculture and livestock farming. However, even though there are some discussions about the specifications of agricultural metadata requirement, I do not see how these ‘specifications’ are different from other types of domain-specific data and how they are implemented in the model. The discussion in section 4 seems very general. Consequently, I do not see much novelty of the proposed meta model for precision agriculture and livestock farming. 

(2). Following my first comment, I am suspicious on how the authors categorize agriculture and livestock farming data. First, I think the categorization generally works for all types of data, e.g., environmental data, urban data, and etc. So how different this proposed meta model would be for non-agriculture data? Second, won’t sensor data overlap with earth observations? Won’t maps and earth observations all include location data? So the proposed categorization is not mutually exclusive? Next, how does this categorization help the design of the meta model? Will it also be a class in the model (I do not see it in Figure 2 though)? How would such a categorization help end users to answer their competency questions? Finally, I think maps (if you mean vector-based polygons, polylines, or points) are structured data as they are stored as relational database in most GIS systems. Also, I am curious why maps and images cannot have a structure (see the statement of the second bullet point in Section 5.1)? For example, a geographic entity can have a spatial relation with another, which should be captured by a schema (data structure). 

(3).  In section 4.3, the authors summarized three functional requirements based on interviews and survey with stakeholders. I am wondering what kinds of questions have been asked in the interview or survey? How many stakeholders have been interviewed? Without these elaborations, the summarization seems arbitrary. Additionally, like my comment (1), I do not see how different the non-functional requirements (Table 1) would be for non-farm/non-agriculture domain. 

(4). In Section 6: Application of the Model, I suggest replacing the listings to figures (for these data and structures in RDF), which would be more readable and it can save a lot of space as well. More importantly, I do not see much significance from these demonstrating examples. For example, I believe using SOSA together with DCAT might have similar capability, if not even better as temporal and spatial info are already modeled in SOSA. So a comprehensive comparison between different ways of designing the model would be needed here to show the significance of the work. Alternatively, the authors should show the capability of this model to address rather complex competency questions, e.g., semantically integrating data from various sources. The current queries shown are trivial IMHO (i.e., can be done using other models).  

(5). It would also be worth for the authors to explain on whether it is a better idea to semantically annotate individual data records using RDF, rather than only on the meta level?  One advantage I think is that one does not need to know both SPARQL and SQL at the same time in order to query useful data. It might be beneficial to have either relational database or linked data in a project, but not both? All these questions are fundamental to this work and worth discussing. 

(6). The model is served on a long-term maintained URL. However, README in the provided Github page is missing. The replicability of the model/data might be difficult.   

(7). The paper must be proofread carefully. There are many typos. E.g., Page 2 paragraph 1, “… as well as the identification the best harvesting period” --> “identification of the best”. Page 3 right side paragraph 1, “… as the definition of the proposed mode in this is paper” --> delete 'is'. 

In summary, the topic discussed in this paper is trending and the proposed SHACL shape to address the linking between two ontologies is creative. But for a journal paper, this paper should be substantially improved in terms of its methodological originality and result significance. ","gpt-4o-2024-11-20, Jon Wasky",slightly negative
2920,"This paper proposes a meta model that can integrate various forms of heterogeneous data. The motivation of this model is the author’s claim that data aggregation can improve the predictive power of models learnt from that data.

The paper was well written and easy to read.

The motivation seems to have sound foundations as data aggregation in competitions such as the Netflix prize greatly increased the F-Measure of models. The literature review seems a little brief and covers the usual suspects. I would have expected that agricultural machinery would have formed part of the data of the sources that you would have wanted to integrate. Therefore I would have expected to have seen ADAPT in the literature review. Most of the resources mentioned are produced by non-profit organizations or NGOs, and therefore have excluded the big industry players, and therefore we can’t be sure that these resources are actually used by agricultural organizations that this product is targeting.

The main issue I have with this paper is the exclusion of industry or at least no mention of industry involvement. The lack of industry involvement in the development of standards or models will condemn the standard to obsolescence. In the development of the case studies I would expect more than just “domain experts” which infers the usual suspects from NGOs and academia. I would be grateful if you could provide a more detailed description of the people that you consulted. I appreciate that it was applied to an EU project, however the project does not contain any of the large industry actors such as Syngenta.  Therefore I feel that this a top down approach which imposes a structure on potential end users rather than a bottom-up approach which creates a model that the majority of industry will use.

The user requirements are a little light. I would have liked to have known how participants were selected, their profile and how conflicting requirements were dealt with. Without knowing the profile of the users, it is difficult to know if these use cases are relevant or just edge cases.

The rest of the paper is an in depth description of the  model, and I have no real complaints about it. My main concern is that once the project is over, that this model will simply not be used.  If you could give a more robust explanation about the user requirements, and who participated I think that this paper could be a good candidate for acceptance into the journal.
","gpt-4o-2024-11-20, Jon Wasky",slightly negative
2924,"The paper tackles the problem of knowledge graph (KG) construction. It proposes planning and execution techniques to speed up KG construction pipelines specified using mapping rules in [R2]RML. The proposed method relies on the partition of mapping rules so that the evaluation of the groups in the partition, reduces the duplicated generation of RDF triples and maximizes the parallel execution of the mapping rules. The proposed techniques are implemented in MorphKGC, an RML-compliant engine; the behavior of MorphKGC is assessed in three existing benchmarks. The reported results suggest that the proposed methods can accelerate the execution of mapping rules as the ones composing the studied benchmarks.

Overall, the paper is relatively well-written and presents an efficient solution to a relevant data management problem. The experimental results provide evidence of the benefits that planning the execution of the mapping rules brings to the process of KG construction. Moreover, the outcomes of the empirical evaluation put in perspective the improvements achieved by the proposed planning techniques. However, the current version of this work suffers from several issues that considerably reduce its value. First, the problem is not formally defined, the definitions are not well-formulated and several relevant concepts are idle defined. Second, the complexity and correctness of the proposed algorithms, and the conditions that prerequisite their performance and soundness are not even mentioned. Third, the experimental study is conducted over a limited set of test cases, preventing, thus, the reproducibility of the reported outcomes. Finally, the state of the art is superficially analyzed, and the proposed techniques are not positioned with respect to similar data management techniques. 

All these issues prevent from a positive evaluation of the current version of the work. The recommendation is for a major revision addressing all the following comments. 

Mathematical Pitfalls: 
•	Definition of equivalent triples maps. The conditions to be met to decide when two triples maps are equivalent are not formally stated. Note that two triples maps can produce equivalent results even if they are defined over two different logical sources. However, the informal definition presented on page 3, suggests that both triples maps should be defined over any given data source. Moreover, the property of “two equivalent term maps with different positions are not equal”. The differences between equivalent and equal must be differentiated. What is the complexity of the problem of deciding if two triple maps are equivalent?
•	“Because of RDF set semantics”.  An RDF document is formalized as a graph, please, clarify and reference to which RDF set semantics the authors refer to. 
•	It is never defined when an RDF triple is generated from the evaluation of a triples map over a logical data source. Please, formally state the results of a triple map evaluation.
•	Definitions of position(.), type(.), value(.), and literaltype(.) incorrectly assume in the domain a single element T. Contrary, the domain in all these cases must be a set of term maps.
•	Self-joins are used without any definition in the context of mapping rules. 
•	Definition 1.  The concepts [R2]RML document, equivalent normalized  [R2]RML document, and [R2]RML documents without self-joins are not used without any previous formal definition. Consequently, the description of canonical [R2]RML document is mathematically incorrect. 
•	Definition 2.  A partition of a set X is a set of subsets of X, and the relationship of the parts G_i of P is not defined. Also, please, note the time complexity of enumerating all the possible subsets of a set X. 
•	Definition 3. A template should be defined in terms of the different structures that it may take, and then a prefix can be defined. 
•	Definition 4. An invariant is wrongly defined. T is a term map, while an invariant I is a string, and making I equal to T is a type mismatch. Also, the “if” conditions only state sufficient prerequisites. Please, provide sufficient and necessary conditions of the values of an invariant I. 
•	Definition 5. Term maps are considered sets, even though a term map is never presented in this way
•	Definition 6. The definition of Disjoint mapping rules relies on the definition that a triple set is generated from a mapping rule which has never been defined. 
•	Definition 7. It presents a property instead of the definition of a Disjoint mapping Groups. Also, it relies on definition 6 (which is incorrect) and assumes that a triple map is a set.
•	Definition 8. It defines a Maximal Mapping Partition of an [R2]RML document as the one with the largest number of mapping groups. This is exactly the one where each group is a singleton set unless another missing condition needs to be satisfied. What is the complexity of finding a Maximal Mapping Partition?
Proposed Algorithms:
•	Algorithm 1 simply replaces an object reference for the template definition of the corresponding parent triples map. Algorithm 2 and 3 use functions which are not defined. 
•	Heuristics implemented by Algorithm 3 are not defined and because the problem solved by Algorithm 3 is not defined, it is impossible to demonstrate its correctness. 
•	The complexity of Algorithm 3 requires to be demonstrated. 
Empirical Evaluation
•	The empirical evaluation does not consider engines that also implement similar planning techniques, e.g., SDM-RDFizer 4.0 [1]. 
•	The meaning of Table 1 is not clear, and the reported results do not have any connection with the rest of the outcomes presented in this section. Please report the results if the experimental study was also conducted over all these datasets. Otherwise, eliminate this table. It is misleading for the reader.  
•	The absolute values of figures 4 and 5 should be reported. 
•	Detailed discuss of the conditions to be met by a data integration system to benefit from the proposed techniques. For example, what happens when the joins between triples maps are not self-joins, and the selectivity change? The authors claim that they have discussed the parameters that impact the execution of the KG construction process in previous work. However, only a few of them are considered in this study, reducing, thus, reproducibility of the results in more general testbeds. 

Related Work 
•	This section simply describes tools instead of analyzing the data management techniques proposed by each of the existing approaches. Please, provide a deeper analysis of the problems solved by the approaches reported in the literature, their pros and cons of the proposed techniques, and position your techniques with respect to them. 

Minor comments
•	The benchmark Genomics – TIB is mentioned but then, the COSMIC testbed is used. Are they both the same? Do the authors refer to the benchmark available at [2] or at any other benchmark? 

[1]  https://github.com/SDM-TIB/SDM-RDFizer
[2] https://figshare.com/articles/dataset/SDM-Genomic-Datasets/14838342/1
","gpt-4o-2024-11-20, Jon Wasky",'slightly negative'
2924,"The paper presents a method for materialization of triples that are used for knowledge graph construction. Two different algorithms are presented for mapping partitions, which group mapping rules that create triples. The presented algorithms are evaluated on three benchmark datasets and compared against four existing systems. The experimental results suggest that the presented sequential and parallel processing solutions perform better than the compared systems in terms of memory usage and execution time, respectively.

Originality: the paper addresses an important problem of reducing memory usage and execution time for knowledge graph construction. The novelty aspect of the paper lies in the presented two ways of using mapping partitions: parallel or sequential execution.

Significance of the results: the authors compared to existing systems on multiple benchmark datasets. The presented methods seems to perform better in both memory usage and elapsed time. Even though the shown results are better than those of compared systems, two different configuations (parallel or sequential execution) are highlighted for the elapsed time and memory usage, respectively. The authors have compared them side-by-side in Figure 3 where advantages/disadvantages of each system can be seen. What is the performance of each configuration Morph-KGC^p+ and Morph-KGC^p on GTFS, COSMIC, and NDP datasets? For instance, the Figure 4a on GTFS datasets shows only Morph-KGC^p+ (parallel) and Figure 4b shows Morph-KGC^p (sequential). How does each configuration (parallel or sequential) compare with other systems for both memory and execution time? The conclusion how one can combine each configuration to build more efficient materialization for KGC is missing. And what cost the execution time suffers for keeping low memory usage or vice versa when two of these configurations are merged? These points are not made clear in the paper.

Also, five different configurations of Morph-KGC are mentioned but only three are compared/evaluated. 

Quality of writing: The paper is written very well with clear structure and adequate language.

Overall: The results section and conclusion about the drawbacks of each highlighted configuration needs to be mentioned as well, where one can see them from the results sections. Besides this point, the paper has addressed an important challenge, presented significant contributions.
 ","gpt-4o-2024-11-20, Jon Wasky",'slightly positive'
2925,"Thanks to the authors for providing a color-coded paper and a thorough cover letter. I do really appreciate your effort! The paper got significantly improved and thus, hopefully will achieve a significant impact in the DH community. 

# Long-term stable URL for resources assess:
The paper presents a novel DNN model for QA over genealogical data. Neither the raw GEDCOM data, nor the KG, nor the Gen-SQuAD data nor the fine-tuned QA is publicly available - due to GDPR constraints as the authors explain in their cover letter. None of the criteria below can be assessed due to being protected under the European General Data Protection Regulation (GDPR) and Israeli Protection of Privacy Regulations. The model, the vocabulary, tokenizer config, special chars mapping, and other configurations are available to the reviewers. 
The new version of the paper allows replicability in a sense, that users would need to use their model, the presented model, and their dataset to calculate numbers. Still, the numbers from the paper cannot be reproduced but that is not a big issue in this case-
The new assessment of the long-term stable URL (which is currently not long-term stable) is: (A) A READMe, example file, and example code are available.. (B) it depends, see above, (C) yes, (D) given the paper. yes. Overall, the authors did a good job of enhancing the resource material.

# Review
The authors propose an end-to-end QA approach for the field of genealogy, a first in the field. The authors use existing semi-structured data (RDF+full-text) and convert it into a form that is suitable for machine-reading/comprehension algorithms.

## Introduction
The introduction reads well and allows laypersons to get familiar with the problem at hand. The motivation and the contributions are clear.
 
## Related Work
The related work section is convincing and covers all standard literature for DNNs as well as QA. It is also a good read for beginners, as it introduces all main concepts in detail. Figures one, four, and five help to understand the standard as well as the proposed QA pipeline.
 
## Method
The new version of the chapter reads really well! 
 
## Experimental design
The experimental design section is also well-written and easy to follow. 

## Results
The results section is quite easy to understand and up to par. An ablation study was performed on the input parameter (degree).","gpt-4o-2024-11-20, Jon Wasky",'slightly positive'
2926,"The Nagoya protocol on Access and Benefit Sharing defines the legal processes constraining both the collection and reuse of data in Biodiversity studies. To find information regarding the existing studies and the administrative processes to collect and reuse the data requires to retrieve information from various heterogeneous databases and documents. It becomes particularly complex if you consider studies in regions of the world for which sovereignty is unclear as in the case of the United Kingdom and their external territories. 
The authors are presenting in this paper a proof-of-principle ontology designed to improve the current situation by supporting the implementation of the FAIR principles. The NagO ontology is a cross domain ontology which aggregates ontologies for biodiversity, geography together with additional terms describing organizational and legal aspects of the Nagoya protocol and the different stakeholders involved. The ontology follows the OBO Foundry principles making it interoperable with the biodiversity relevant ontologies and some geographical ontologies. These various ontologies are aligned under the Basic Formal Ontology which provide a consistent semantic and logical framework. 
	
The paper is well written and provides a clear overview of the context and the problems the ontology aims to address as well as a seemingly exhaustive list of the relevant data sources. The general design process of the ontology is clearly described with few exceptions that I list as minor comments below. This ontology is definitely relevant for the domain. However, I have several major concerns regarding the article, the ontology and the related data. 

First, although the context is clear, the work is not supported by concrete use-cases from the domain which would illustrate the purpose and usage of the ontology. In section 5, usage scenarios seems to be more prospective scenarios than real tested ones. This definitely undermines the presentation of the work. I would suggest the authors to revise the paper to add concrete examples of use which would ground the work in practice and provide more clarity on practical challenges and added value for non-expert readers. These examples should provide both the related competency questions used for the design and the associated SPARQL queries to support the testing and exploration for new users. These queries could be shared on the github together with the ontology. Based on my understanding of the ontology and the context, I would suggest the authors to enhance the content of the ABS Clearing House (accessible through an API) to provide the missing link between the external territories and their sovereign country. This could be done by leveraging the Nagoya Look Up service mentioned in the article and developed by the authors. 

My second major concern is related to the ontology itself. A lot of effort has been put to import biodiversity related ontologies such as ENVO or even the NCBITaxon while the main addition of the ontology focuses on the legal and organizational aspects of the information as shown by the example patterns used to illustrate the ontology. The paper does not actually show the connection between this organizational aspect of the ontology and the biodiversity, geographical aspects. This raises the question of the relevance to import these ontologies. Furthermore, as the author acknowledge in the paper, the ontology is actually really narrow while its scope is quite large. I would suggest the authors to extend it to at least other European countries such as France or the Netherlands which also have external territories and to consider at least one conflicted area. This would definitely enhance the usefulness of the ontology and also extend the ontology for governmental and sovereignty concepts.
 
My second major concerns about the ontology is related to the shortcuts used to build this first version of the ontology. First these shortcuts are not clearly explained in the paper. To understand the explanations, one needs to be an expert on the BFO and RO ontologies. I would suggest to clarify the text. In addition, one of these shortcuts implies that signatory roles are actual subclasses of “Homo Sapiens”. First this is semantically and logically wrong and second I am wondering about the need to describe persons based on their species. Why not using existing terms to describe persons? This issue actually would have an impact on the usage in applications. 

My final set of major concerns is related to the data. The author are sharing the ontology on github. However, it is hard to understand which ontology file to use. The readme provides little documentation on the various files (e.g nago-base.owl, nago-full.owl, nago.owl). A first analysis of the ontology using Protégé revealed that none of these file includes both the additional NagO classes and their associated instances. After some digging I found the “complete” ontology in the src folder named nago-edit.owl. This is really cumbersome for interested users. It seems this particular github structure is tightly linked with the use of the ontology starter kit. I would recommend the authors to revise their README file to provide more information about its content. In addition, the ontology has little descriptive metadata (missing authors, contributor, description of the scope, …) and the link http://obofoundry.org/ontology/nago is broken.  

Although this work is of high interest to support the enforcement of the Nagoya protocol and the implementation of the FAIR principles within this context, as presented here the ontology is clearly incomplete and would benefit from additional work. 

Minor concerns
-	Section 2 – Background
o	Page 2 – column 2 line 31: “NagO aims to connect the vocabulary of different topics”. Should be “vocabularies” as many are existing and considered
o	Page 3 – column 1 line 3-7: “The challenge is transforming (…) the observed data discrepancies”: this sentence is unclear, please rewrite
-	Section 3 – Approach
o	Page 3 – column 1 line 13 – “NagO was created (…)”: it follows the OBO principle and leverage the logical framework of the Basic Formal Ontology as Top level ontology. You should also mention that the ontology has 5 branches here. 
o	Page 3 – column 1 line 23-24 – “A general overview was developed and then extended with document terminology as well as Nagoya protocol processes and involved instance”: A general overview of what? What do you mean by document terminology? 
o	Page 3 – column 1 line 33 – “Very few of the necessary vocabulary (…)”: replace vocabulary by terms 
o	Page 3 – column 2 line 1-3 – “The consensus between all researched (…) as an overlap of definitions”: this sentence is not clear. Could you please rewrite? 
o	Page 3 – column 2 line 24-25 – “In order to put these terms into perspective they were looked at in a broader context”: Are referring to the people, document and process? This sentence is not clear to me. Please clarify.
o	Page 3 – column 2 line 44-48 – “However, the ABS Clearing House itself provides openly accessible information on the role of genetic material (…) protocol text.”: What do you mean by “the role of genetic material”? 
-	Section 4 – Key features
o	Page 4 – column 1 line 3 – “ There are five branches which NagO consolidates: (…)”: As the authors used already the term of branches for the describing the different part of the ontology, I would suggest finding a new term. This is confusing to the reader. 
o	Page 5 – column 1 line 30 – column 2 line 4 – “In summary, NagO combines (…) for business stakeholders”: this sentence does not make sense. Please rewrite.
o	Figure 3 – shortcut relation is missing.
o	Page 6 – column 2 line 11 – “(…) have been chosen very consciously”: replace by cautiously? 
o	Page 6 – column 2 line 12-15 – “For example, while (…) as these axioms”: this sentence is unclear. Please reformulate.
o	Page 6 – column 2 line 16 – “Rather than creating BFO roles for terms”: what do you mean? 
o	Page 7 – table 1 – The table is not mentioned in the article. It can be removed. 
-	Section 5 – Usage scenarios 
-	Section 9 – Annex: the list of concept seems to be incomplete.
","gpt-4o-2024-11-20, Jon Wasky",neutral
2926,"
The goal of this article is the development of an ontology NagO. NagO is a domain ontology focused on terms regarding the United Kingdom and its external territories and their link to the Nagoya Protocol policy framework, processes and people involved.

The abstract clearly and concisely presents the purpose of the article. The main idea for developing the Nagoya Ontology (NagO) is to semantically model the complex policy framework around the Nagoya Protocol and to unveil the legal relationships between sovereign states and their external territories, illustrating the United Kingdom as a study case.
  I don’t have any comments here.

The introduction very clearly presents the reasons why the authors have decided to develop the Nagoya Ontology. Obtaining heterogeneous information about the sovereignty of different external territories of countries and not free access to reliable information on the status of these territories under the Nagoya Protocol gives the authors reason to emphasize the need to develop Nagoya Ontology. 
I don’t have any suggestions here.

Quality and relevance of the described ontology.

I think the ontology is qualitative and relevant. 
     1.	The information in NagO is carefully selected and it follows the Basic Formal Ontologyconcept  and the guidelines under the OBO Foundry. For the sovereignty branch of NagO, the Island Rights Initiative provides the majority of information by displaying the constitutional links and governmental relationships of every external territory of the United Kingdom. The approach to collecting and presenting the information contained in NagO is described systematically. 
    2.The ontologies, which provide terminology for NagO, include ENVO[16], IAO[17], BFO[8] and RO[18], and they were automatically imported using the ontology development kit [19]. The Dublin Core Metadata Initiative terms [20] and NCBITaxon [21] have been added with a manual import in Protegé.
    3.Specifically looking at the term definitions gave the authors an insight of how these terms are related to each other. The basis for this step was reading about administrational and political processes and unveiling the distinct people involved, the roles they have and what input is needed for certain processes. The ABS Clearing House offers information on country profiles and documents in relation to the Nagoya protocol.
   4.Where public data information was not found, personal inquiries were made to the government departments. The OBO Foundry delivers definitions and relations for the biodiversity and territory branches of NagO (ENVO, SDGIO, GO, NCBITaxon and GEO, GAZ respectively). 
   5.In Figure 1, the authors represent an overview of the NagO domain scope intersections with an associated topic and possible connections to existing ontologies. In the “Key Futures” Section they present and describe the connection of NagO with other existing ontologies in different domains connected with the Nagoya Protocol and the identify niches where semantic development is needed.
  6.The authors define some semantic patterns in the ontology. An example can be seen in Figure 3 - geographic territories are related to people and processes. There are three more semantic patterns defined in the ontology: Territory instances and the execution of a certain government type (e.g. self-governance); UK external territories and their connection to the United Kingdom; Ministerial departments and their connection to the head of state. All these semantic patterns can be used in other domains. 
  7.With the example for use of the ontology, the authors demonstrate that NagO can be used to query constitutional links of geographic regions. They also claim that it can be used to find role capacities of the people involved and relevant processes and instances of the Nagoya Protocol.
  8.In the last section of the article, different opportunities are discussed to extend NagO in different domains. 
  9.NagO is free and openly accessible in English on “https://github.com/hseifert/NagO”.

Illustration, clarity and readability of the describing paper, which shall convey to the reader the key aspects of the described ontology.

I think the paper is well written and all ideas are adequately presented with figures and tables. The developed ontology is properly described and the content of the article is clear and readable. All Figures and tables are explicit and comprehensible.
All files – ReadMe, nago.owl, nago-full.owl and the others are freely available at https://github.com/hseifert/nago/tree/develop 

Review comments:

1.My major comment is about the “Background” section.  In this section, the authors again present the reason to create the NagO ontology and its connection with other domains. I think it will be interesting to the readers if there are some similar ontologies about other protocols (conventions) to be referred here. 
2.The methodology for creating the ontology and gathering knowledge is well described. Various domains related to the Nagoya protocol are presented. These domains are included in the NagO-related ontology. There is a need to describe the connection to these related ontologies. Which classes are really re-used and how do the different ontological commitments relate to each other?
3.“Both figures 2 and 3 include shortcut relations used for this illustration” – I didn’t see the shortcut relation in Figure 3.
4.I think the authors can extend the usage scenario, for example with finding role capacities of people involved. This will more fully present the potential of the NagO ontology.


","gpt-4o-2024-11-20, Jon Wasky",'slightly positive'
2932,"Overall Comments

This paper proposes a complex framework for solving the contextual commonsense inference problem. Specifically, it contains three major components: (1) hinting; (2) Joint inference from multiple knowledge graphs; (3) adversarial training. In general, I think this paper still needs significant revision to meet the standard of this journal. Please see my detailed comments below.







Originality

The novelty of this paper is limited. Even though this paper proposes three modules to solve the commonsense inference problem, Most of them are borrowed from other tasks. The effects of these modules are also not very significant. For example, even though this paper spends several pages introducing the adversarial language model, it seems not very helpful on this task.


significance of the results

Some techniques like hinting have made significant contributions to performance improvement, while others are not. However, as this paper does not introduce too many details about the hinting, I cannot evaluate/verify if it can help the community. Moreover, many detailed experiences are missing. For example, where does the hinting come from, and what if you chose to use a different hinting strategy?

Quality of writing

The writing of this paper needs further polishing. Some arguments are too strong or not very clear. Please see my following comments:

Page 1, line 13: missing a “,”
Page 2, line 13: This argument is too strong (or the question this paper is trying to solve is too general). I do not understand why the proposed methods can solve the mentioned problems.
Page 2, line 17 (and many others): better to have a space before “[]”
Page 4, line 17: Personally, I like the idea of hinting, but this introduction omits too many details, and I cannot follow. For example, what does tuple refer to in your context?
Page 4, line 42: This argument is debatable. COMET also needs to be trained with a human-crafted commonsense KG. The success of COMET can also be that it utilizes the “semantic” information from language models to generalize the knowledge in those KG.
Page 5, line 1: Naming “merging several knowledge graphs” as joint inference might be misleading.
Page 5, line 8: This kind of conceptualization is not always correct. For example, one cannot walk a fish. But it fits all the patterns.
Page 6, line 46: Maybe I am missing. What vectorization tool is used?
Page 9, line 43: This argument needs more support. I do not think a 9.4% overlap can cause the failure of the joint reasoning. Have you tried to train the model with the data to remove the overlap part to see if that is the true reason your joint inference model is not working?
Page 14, line 20: Something seems to be wrong with the reference.



whether the data file is well organized and in particular contains a README file which makes it easy for you to assess the data

I do not see a readme in the provided repo.

whether the provided resources appear to be complete for replication of experiments, and if not, why

Yes

whether the chosen repository, if it is not GitHub, Figshare or Zenodo, is appropriate for long-term repository discoverability

Seems so


4. whether the provided data artifacts are complete

It was not analyzed.
","gpt-4o-2024-11-20, Jon Wasky",slightly negative
2932,"Overall, I find the paper clearly written. This paper studies an important problem: contextualized commonsense reasoning, and it proposed 3 key contributions. 1. a hinting method that better guides the commonsense inference. 2. An embedding-based method for grounding commonsense assertions to stories. 3. A GAN architecture for inferring assertions from stories. 
However, there are some weaknesses in the proposed methods:
1. The hinting method is not fully explained and only a paper reference is given
2. The human approval rate of the assertion grounding method is somewhat low (~65%)
3. The GAN model’s performance is slightly worse than the baseline generator. 
4. Some more questions as detailed below can be better explained. 
Questions to the authors:
1. Section 2.2, line 17, why do you report scores in such a way? Typically, we use the dev set to pick the best model and report scores on the test set. 
2. In table 1, is the no hint setup referring to the original setup proposed by the GLUCOSE paper? 
3. In section 3.6, the automated metrics seem to suggest that hinting is good. With hinting, the joint model is able to achieve comparable results as individual models, and overall results are much better than the no-hinting setting. But for human evaluation, the joint model with no hinting is getting the best results? which seem to contradict with the automatic evaluation?  
4. Overall, I think it would be better to compare with previous systems, e.g. BART/GPT2 trained on ATOMIC2020. 
I think the paper can be improved by addressing the aforementioned weaknesses and questions. ","gpt-4o-2024-11-20, Jon Wasky",'slightly negative'
2949,"The paper suggests an ontology called MOntCS (Modular Ontology for Common Sense) to represent knowledge with a goal to resolve some issues in other knowledge bases. In particular they target two issues they identify in ConceptNet's ontology, relation ambiguity and lack of structure.

The paper has a careful discussion of many of the aspects that influence the choice of relations and entity representation, and lands on a set of relations trying to strike a good balance. There is also good comparisons to various other approaches (although AMR could also have been discussed?).

As a test case, they rewrite the WorldTree knowledge base in this ontology, using a mixture of automatic (for some tables) and manual methods. They take the largest connected component as the KB (that could presumably be relaxed if need be), available for download in a github repository, and show that this is more dense and clustered than other KBs like ConceptNet and TupleKB. Finally they evaluate the KB by incorporating it into a QA-GNN model, measuring answer accuracy on the WorldTree question set.

On the positive side, the descriptions, discussions and comparisons are clear, and a concrete KB has been constructed and shared.

However, the utility of this ontology is not so convincing with the current evidence:

 - Evaluating on WorldTree QA accuracy is very indirect (and ""unfair"" as the WorldTree KB focuses only on knowledge for the correct answers, not the distractor options, giving strong bias towards the correct answer). One motivation in the paper is to provide better explanations - it would be great to see at least qualitative explorations of this, if not some quantitative measures (which can be tricky).

 - Applying the suggested ontology only to WorldTree is a bit narrow, it would be good to also discuss to what extent other knowledge sources, like ConceptNet, could be coerced into this ontology. How much would be covered, how much would be lost, etc? Also, calling WorldTree just ""common sense"" is a bit of a stretch, since it specifically tries to target elementary scientific principles as well.

 - Although some types of ambiguity is resolved with this ontology, there are others that remain. E.g., word sense ambiguity could be an issue (two meanings of ""bat"" exist in the WT graph, e.g.), and the granularity of the connections might lead to incorrect paths. E.g., the ""eat"" node has many different agents (like ""carnivore"", ""scavenger"", ""producer"") and patients (like ""acorn"", ""animal"", ""plant""), and only some of these are actually compatible (though these are indirectly specified in other links, like [""carnivore eat animal"", structural-patient, ""animal""]). 

 - Some more specific limitations are also mentioned in the paper, such as a lack of ability to easily express negation, comparison - it is good that this is explicitly discussed (with some ""workarounds""), but these do seem to limit the universality of the proposal.


In summary, while the motivation of the paper is good, the current proposal does not seem all that convincing. Still, there are worthwhile ideas and suggestions throughout, and it could be more convincing with more concrete evidence from applying to other KBs or showing promise in generating useful explanations.
","gpt-4o-2024-11-20, Jon Wasky",slightly negative
2949,"
The paper describes the authors' effort to define a modular ontology for common sense analysis. Their ultimate goal is to increase the density of the final knowledge graph by structuring and lowering the complexity. The have utilized the relation in WorldTree and created structured relations over those. Their ontology contains relations such as: structured relations, Verbal relations, Taxonomic relations, Affective relations, and grammatical exceptional relations. The paper carefully defines the relation type included in each category. Afterward they have described the annotation procedure to be combination of manual and automatic procedures and provided comparison of statistics of their KG with other common-sense knowledge-bases. Finally for comparing the quality of the ontology and collected KG, the paper describes the experiments over the task of QA and an ablation study to show the quality of every category of relations.

In terms of Quality and relevance of the described ontology, the paper provides a very structured ontology which helped in improving the results of the QA based task, yet there can be tested on more sub-tasks.

The paper is clear and readable but as a suggestion an outline can be added to the content in the introduction sections. 

The authors have shared a GitHub repository, containing their final knowledge-base, yet it is a little confusing and hard to navigate. it might be better to add README file. In addition the GitHub link does not include their scripts used for annotation. It might be good if the authors make those also accessible for other researchers to use.

The strengths and weaknesses of the article are as follows:

Strengths:
+ The authors gave a better structure to the existing ontologies in order to create a dense knowledge graph which ultimately will result in better inference. 
+ Using the described ontology the authors were able to use a combination of manual and automatic annotation scheme which reduces the manual effort needed to create the knowledge-base. 
+ The evalution presented in the paper shows a proof that utilizing more structured graphs (even with less nodes) can result in improvement of the model's performance in the QA task. 

Weaknesses:
- There are not many qualitative samples evolutions provided by the paper. 
- For the evolution, the comparison is drawn over the task of QA using QA-GNN, but it might be better to have the evolution over couple of other tasks to establish the performance of the new ontology and the KG created via that. 




Questions for the Authors:
* In the section, 3.3.2, it is mentioned that some concepts (e.g. distance  and far) are kinda similar, is there a threshold for determining similarity?
* In the sentence : ""bakers give bread to customers."" The authors mentioned that the given triple will be (‘bakers give bread’, beneficiary, ‘customer’), but does it also provide other artifact relations such as ('bakers', 'own', 'bread')?
* In section 4.3, the authors performed post processing in order to find and prune the errors. In what are the percentages of occurrences of those errors.  
* For section 4.3, are there any other type of the errors that remained within the annotated samples that are not caught, it might be interesting to see more qualitative samples of the final datapoints. 
* In section 4.3, page 14, line 6, there is a mention of ""In others this was genuine (‘open container’ is both a possible verb and noun phrase)."" but it is not clear what is the authors' approach for these cases. 
* Table 10 shows that the results of KG analysis without taxonomic relations is even higher than the model with those. If possible can the authors provide the samples that are marked correctly without taxonomic relation and not with them?
* The authors have started with the WorldTree relations, Is this annotation approach applicable to conceptNet?




Minor Comments:
* It might make the paper more readable if the authors can add the outline of the paper of what to be expected in each section and the overflow of the paper. 
* In page 7, line 18: base classes are combined in  --> to be combined
* In page 8 line 31 there are 2 ""cause"" words.
","gpt-4o-2024-11-20, Jon Wasky",'slightly positive'
2949,"The authors propose MOntCS, a new ontology for structuring commonsense knowledge bases (KBs. MOntCS aims to address several shortcomings of popular KBs like ConceptNet, including the inconsistent granularity of the events and redundancy of relations. The design guidelines include restricting the nodes to verb and noun phrases (to control specificity) and introducing structural relations to connect compound nodes to their constituents. In addition, the proposed ontology contains four classes of relations, including verbal (semantic), taxonomic, and affective relations, to cover a broad category of commonsense relations. The authors instantiate MOntCS on the Worldtree factors corpus ([1]) using semi-automated annotation. Experiments on Worldtree QA show that the KB obtained using MOntCS marginally improves over ConceptNet.
 
Commonsense reasoning is an increasingly popular area of research, and resources like new KBs and Ontologies can be valuable to make progress. The proposed work highlights some important shortcomings of the existing KBs, like the inconsistency in granularity and redundant relations. The new contribution, MOntCS, takes promising steps towards achieving these goals. However, grounding some of the claims in experiments and analysis, and clarifying parts of the paper will make it stronger. 

## Areas of improvement 
 
1. Lack of focus: the exact problems that the paper can be better motivated. It would have been better if the said shortcomings (multiple relations and inconsistent granularities) were grounded in empirical analysis, but the current version offers little to no evidence for this. There are works, for example [2-3], which are strengthened by multiple relations between entities in conceptnet. Thus, it might be useful to explain the said shortcomings better. Further, L19 states: ""The design of ontologies for more general, open-ended common sense reasoning has been so far under-explored, and this is where our current paper's focus lies."" However, MOntCS is inherently dependent on the available knowledge and, as such, does not address the problem of the presence of more general resources for common sense reasoning. 


2. Experiments
 
I think the current experiments do not offer sufficient evidence for the utility of the proposed Ontology. In detail:
 
2.1) Table 9, Column 1 shows that MOntCS has only marginal gains over Conceptnet, despite having a 1:1 alignment with the task, as the authors note. Further, the fact that the task overlaps with the graph makes WorldTree QA an uninteresting (and perhaps unfair) candidate for evaluation. To establish that the proposed scheme is general, I suggest that authors try MOntCS on at least one more task. For example, WIQA~[4] might align well with MOntCS.


2.2) Table 9 should have another row involving no graph for a fair comparison. I suspect that neither of the graphs is helping (and some might be adding noise). Adding a no-graph baseline will clarify this.

2.3) Adding significance tests and repeating the experiments for different seeds might also help establish the difference between the graphs in columns 1 and 2.

2.4) Table 10 essentially shows that taxonomic relations are not helpful. Do they still need to be included? Further, column 1 of Table 10 indicates that the performance is essentially the same without any graph? This relates to the point made earlier about a no-graph baseline.


3. MOntCS as a tool for explanation
As authors mention in Section 6, ""Models are increasingly evaluated not just on performance, but on their ability to provide explanations for the choices they make. We design MOntCS to be a suitable medium for expressing explanations in the common sense question answering domain."" However, the experiments section does not show any evidence that MONtCS can provide valuable explanations. Adding experiments on this front will significantly strengthen the paper.



## Grammar/typos, style, and presentation :
 
1. Page 2, L32: ""Relation this scenario…""

2. Page 3, L43: ""in the graph to take a..""

3. It might be better to either add a citation for statements like ""ConceptNet [10] is perhaps the most frequently used knowledge graph for common sense reasoning applications."" This will allow dropping speculative phrases like ""perhaps,"" which might not work well in this setting. Alternatively, you can rephrase them to something like ""ConceptNet [10] is one of the most frequently used knowledge graph for common sense reasoning applications."". A similar statement is present on page 5, L 11: ""ConceptNet is most commonly used as the base knowledge graph, a subset of which is chosen for computational reasons.""

4. ""As a graph grows denser, it becomes easier to select relevant data that may otherwise require many hops to reach from the starting nodes."" Similar to the above, this statement sounds general but will depend on the specifics of the underlying graph. The relevant information may or may not appear closer as the graph grows denser. Thus, it might be helpful to qualify this statement and explain why this is expected. Another such statement appears on Page 15, L37: ""a path length of 2 as used in prior work is insufficient."" 

5. Page 8, L3: ""additioanl""

6. Page 12, L34: ""missing structural links where they were missing""

7. Page 15, L32: ""However, because QA-GNN also includes this embedding within the GNN (figure 1, label '2'), the *langauge model can still be trained."" 
 
8. Section 3.4 is currently very nicely written and is one of the most interesting aspects of the paper. It is interesting because it clearly lays out the problems and presents possible solutions and design choices with motivating examples. I believe that some other parts of the paper (example, Section 5) can be improved with Section 3.4 as a reference.

9. Given the central role that Worldtree plays in this work, it is worth adding a sample Table either in the appendix or the main paper. 

10. Section 5 can use a re-write for clarity. Several statements are either mentioned without appropriate citation (a path length of 2 as used in prior work is insufficient) or are unclear, like ""Ensuring fairness in this scenario is difficult.”



## Questions:
 
Q1. Why is Causes (Table 5) not placed in Affective relations? 
 
Q2: Will ""semantic relations"" be a better term for verbal relations?
 
Q3: Is redundancy necessarily a bad thing? The paper lists redundancy as one of the main shortcomings of conceptnet. However, it is unclear why the responsibility of disambiguating the proper relation should not be with the downstream application. Further, the redundancy can sometimes be advantageous by providing the multifaceted nature of the relation between two nodes.
 
Q4: The second shortcoming of the level of specificity of the nodes is related to the underlying data source and not as much as a problem with the nodes. Since the authors use WorldTree, whether or not the derived KB is at the right level of granularity is to a great extent a function of the granularity at which the facts in Worldtree are expressed?
 
Q5: Page 15, L27: ""The design of QA-GNN does not ensure that the graph…difficult to know the extent to which they drive performance in this model."" Doesn't this directly undermine the motivation for having a knowledge graph at all? Please also see my note on providing evidence for using MOntCS as an explanation.


[1] Jansen, Peter, Elizabeth Wainwright, Steven Marmorstein, and Clayton Morrison. “WorldTree: A Corpus of Explanation Graphs for Elementary Science Questions Supporting Multi-Hop Inference.” In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018). Miyazaki, Japan: European Language Resources Association (ELRA), 2018. https://aclanthology.org/L18-1433.
 
[2] Xu, Yichong, Chenguang Zhu, Ruochen Xu, Yang Liu, Michael Zeng, and Xuedong Huang. “Fusing Context Into Knowledge Graph for Commonsense Question Answering.” In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, 1201–7. Online: Association for Computational Linguistics, 2021. https://doi.org/10.18653/v1/2021.findings-acl.102.
 
[3] Wang, Han, Yang Liu, Chenguang Zhu, Linjun Shou, Ming Gong, Yichong Xu, and Michael Zeng. “Retrieval Enhanced Model for Commonsense Generation.” ArXiv:2105.11174 [Cs], May 24, 2021. http://arxiv.org/abs/2105.11174.
 
[4] Tandon, Niket, Bhavana Dalvi, Keisuke Sakaguchi, Peter Clark, and Antoine Bosselut. “WIQA: A Dataset for ‘What If...’ Reasoning over Procedural Text.” In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), 6076–85. Hong Kong, China: Association for Computational Linguistics, 2019. https://doi.org/10.18653/v1/D19-1629.







","gpt-4o-2024-11-20, Jon Wasky",'slightly positive'
2957,"If I recall, I accepted this paper with minor revisions when it was submitted as a tools paper. However, it is difficult to accept this as rising to the level of research for a full paper. In particular, while the level of usability research is sufficient to demonstrate the tool, it does not compare that tool to other approaches for inputing structured data.

On the other hand, if the focus were either the EPOS-DCAT-AP or the publication pipeline, the attention to the SHAPEness interface distracts from the evaluation of a pipeline that consists of a standard ontology for harmonization (EPOS-DCAT-AP), a structured editor (SHAPEness), and some governance for admitting edits.

I appreciate that this journal's tools papers are ""short papers describing mature Semantic Web related tools and systems"" and that this paper feels like it surpasses that metric, but I feel that the topics covered in depth (interface description and six-person usability evaluation) best fit that category. Perhaps this could be split into two, a tools paper for SHAPEness and a research paper describing the pipeline and comparing it to other distributed editing/harmonziation approaches. From my experiments, I believe the tool has no dependency on EPOS-DCAT-AP and could be described simply as a shapes-based strctured data input tool.


Questions posed in the <http://www.semantic-web-journal.net/reviewers> template are prefaced with '> ':
-------------------- <review template> --------------------

> (1) originality,
> (2) significance of the results, and
> (3) quality of writing.

It is difficult to follow the template as I'm unsure whether to evaluate it on the ontology, the tool, or the pipeline. 


> Please also assess the data file provided by the authors under “Long-term stable URL for resources”.
> In particular, assess
>   (A) whether the data file is well organized and in particular contains a README file which makes it easy for you to assess the data,

The homepage <https://epos-eu.github.io/SHAPEness-Metadata-Editor/> is included in a footnote at the very end of the paper (a real test of the diligent reader). Placing it prominently near the top would allow readers to experiment with SHAPEness while reading the paper. I created a PR to help folks starting from the README. https://github.com/epos-eu/SHAPEness-Metadata-Editor/pull/8

I did not mention in that PR that the sample epos-dcat-ap_shapes schema downloaded with SHAPEness includes the OWL, annotations and the schema. This makes it less clear what the SHAPEness tool needs. From my experiments, you can trim all that out up to the start of the SHACL. The SHACL uses an idiom of `SHAPE sh:targetNode CLASS` and `P sh:hasClass CLASS` instead of `P sh:node SHAPE`. I wonder if that's for efficiency, fault-tollerance, or to allow one to work with partial data.


>   (B) whether the provided resources appear to be complete for replication of experiments, and if not, why,

Yes for EPOS-DCAT-AP and SHAPEness


>   (C) whether the chosen repository, if it is not GitHub, Figshare or Zenodo, is appropriate for long-term repository discoverability, and


DOI: https://doi.org/10.5281/zenodo.5702869
repo: https://github.com/epos-eu/SHAPEness-Metadata-Editor
release: https://github.com/epos-eu/SHAPEness-Metadata-Editor/releases/tag/1.2.1
last activity: 2021-11-19 (R1.2.1)

The DOI points at a GitHub with releases for the version evaluated in this review.


> (4) whether the provided data artifacts are complete. Please refer to the <a href=""/reviewers"" target=""_blank"">reviewer instructions</a> and the <a href=""/faq"" target=""_blank"">FAQ</a> for further information.

The data artifacts allowed me to run SHAPEness with at least a slice of the schema described in the paper.

-------------------- </review template> -------------------


Expressivity: It's unclear what constraints are enforced in the SHAPEness tool. The example schema doesn't include node constraints beyond sh:datatype. I added a `pattern` constraint t ContactPointShape's schema:telephone: ```sh:pattern ""^tel:"" ;```. (I know, that should be an IRI, not a string, but it serves as a test.) The input form accepted a new schema:telephone of ""asdf"" so I don't believe that SHAPEness currently validates field input per the schema. I don't believe the paper or documentation mentions this limitation.



3.5. Validating RDF graphs

""SHAPEness implements on-the-fly quality and
consistency check of the graph, validating its content
according to the SHACL constraints. Users receive
notifications about constraint violations of invalid
portions of the graph.""

This brings up the spectre of readable diffs. It's probably worth menting if e.g. Jena emits a graph that's pretty consistent, and whether adding a blank node (with no co-refs) results in extra noisy diffs.
","gpt-4o-2024-11-20, Jon Wasky",neutral
2957,"* Summary: the article describes SHAPEness, an open-source Java-based desktop software application that is a SHACL-driven visual editor for creating and validating RDF graphs.
It combines in a single user interface, 3 different types of views (graph-based, form-based, and tree-based).  The paper describes the application's features, architecture, and usability in the context of the European Plate Observing System.


* Overall Evaluation (ranging from 0-100):
[Criterion 1]
	[Q]+ Quality: 75
	[R]+ Importance/Relevance: 80
	[I]+ Impact: 80
	[N]+ Novelty: 85
[Criterion 2]
	[W]+ Clarity, illustration, and readability: 88
[Criterion 3]
	[S]+ Stability: 100
	[U]+ Usefulness: 95
[Perspective]
	[P]+ Impression score: 88


* Dimensions for research contributions (ranging from 0-100):
(1) Originality (QRN): 80
(2) Significance of the results (ISU): 92
(3) Quality of writing (QWP): 84


* Overall Impression (1,2,3): 85
* Suggested Decision: [Major Revision]


* General Comments:
- SHAPEness is a Java-based desktop application.
- It's a useful SHACL-driven visual editor.
- It has proper user documentation and material.
- The tool's architecture should be improved (see points below).
- Also, the source code could have a better design on the parametrization of constant values (configuration/settings), such as namespaces (prefixes and IRIs).


* Major points for improvements:
{
(a) From (pag. 09 / algo. #2), it seems that not all SHACL definitions/constraints are handled by the application.  Rather, only basic SHACL constraints.
What about more complex SHACL constructs such as ""alternative paths"", ""zero-or-more"", ""SPARQL-based constraints"", etc.?
The authors should clarify precisely what kind of SHACL constraints are supported by the desktop application.
In the conclusions section (pag. 15), the authors claim that ""... SHAPEness ... meet SHACL compliance requirements"".  If all SHACL constraints are not supported this is not true.

(b) pag. 09: Why the JavaBean class was named ""Person"" in Figure 8 rather than ""ex_PersonShape""? The naming scheme should be explained in detail.

(c) pag. 10: It would help a lot to include a proper representation of the Engine's components' design complexity, such as a UML class diagram.

(d) pag. 10: Where is the SHAPEness data model? There is only one mention about the ""superclass Node"". Is that all? Again, this ""data model"" should be clearly defined and expressed properly, for example, as a UML class diagram that could capture the main structure.

(e) pag. 10 / sec. 4.2.3: ""Each node has its own... behaviors (actions...)"", can you explain more about these actions? The shown example (fig. 8) only presents setters/getters actions.  What other actions are there?

(f) pag. 15 / sec. 7: The authors claim that ""... SHAPEness ... meet SHACL compliance requirements of RDF data faster and more accurately"".  Faster and more accurately compared to what other tools? Was this tested/analysed? If not, this claim is not supported by any evidence.  The authors should rephrase or remove this claim.

(g) secs. 3 and 7: the paper claims that the application can take ""as input a set of SHACL constraints"".  If this is true, how the application combines multiple SHACLs into one single ""schema""? This would be a very important feature.  An algorithm explaining how the application performs this combination of multiple SHACLs should be presented/explained.
}


* About the data files and related software artifacts: (“Long-term stable URL for resources”)
(A) ""[code] files are well organized and contain a README file which makes it easy to assess the [code]"": [YES].
(B) ""the provided resources appear to be complete for replication of experiments"": [YES.  COMPLETE]
(C) ""the chosen repository is appropriate for long-term repository discoverability"": [YES. GitHub + Zenodo]
(D) ""the provided [code] artifacts are complete"": [YES]


* Minor corrections:
pag. 01: ""The validation of the structure of RDF graphs"" ==> ""The validation of the RDF graphs structure""
pag. 02 / [Schímatos]: ""The software is available to download as a React.js application"".
pag. 04: ""compact IRI expressed as prefix:suffix"" ==> ""compact IRI expressed as prefix:localPart"", following the W3C XML NS spec (https://www.w3.org/TR/xml-names/).
pag. 09 / alg. 02 / Line 11: it's better ""get existing JavaBean class""; ""sh:maxcount"" ==> ""sh:maxCount""; ""sh:mincount"" ==> ""sh:minCount"";
pag. 12: ""e.g. use office, excel..."" ==> what is *office*?, should it be ""Microsoft Office Suite""? (which includes ""excel"").
pag. 13 / sec. 6.3.2: ""listed in Table 1: a)...; b)..."" ==> it should be ""Table 1... and Table 2...""
pag. 14 / tab. 1 / [T4, ""User's comments""]: ""not clear why and ID is needed"" ==> ""not clear why an ID is needed""
pag. 17 / [26]: ""S.R. Méndez"" ==> ""S. Rodríguez Méndez""; ""O.P."" ==> ""P. Omran""; ""Schimatos"" ==> ""Schímatos""
replacements: ""web"" ==> ""Web""; ""Schimatos"" ==> ""Schímatos"";


* Others:
{
+ The Google Form Questionnaire presented in section 6.2 should be included in a specific location (folder) on the GitHub repo.

+ pag. 13-15 / sec. 6.3: The analysis presented is based only on six different users.  For sure, more users would be suitable for a proper user testing analysis.
Based on reference [20], it seems that the software has been used at least since 2019.  It is recommended to broader the usability analysis by considering more users, especially for the other 2 missing user groups. Although this is explained/covered in section 7, the authors should've considered broader the usability analysis before submitting the paper.
}
","gpt-4o-2024-11-20, Jon Wasky",neutral
2964,"In this paper the authors describe the ExaMode ontology and some of its applications. ExaMode includes a few hundred terms for disease, diagnosis, patients, tests, test results, procedures, etc. focused on histopathology, and specifically on four diseases: colon cancer, lung cancer, uterine cervix cancer, and celiac disease. The system has been used to automatically extract annotations from case reports in Italian and Dutch, and the results can be visualized as a graph.

This work is valuable and difficult. The authors are to be commended for building a working system from many disparate parts, and for making good use of existing ontologies to improve interoperability of their systems and data. They provide a good ontology description in this paper, detailed ontology documentation at <http://examode.dei.unipd.it/ontology/index.html> and open code <https://github.com/ExaNLP/sket/>. I encourage the authors to continue this work.

# Specific review requirements:

(1) Quality and relevance of the described ontology: I believe ExaMode is relevant, and suitable for purpose, although I have concerns about quality discussed below.

(2) Clarity of the paper: Good overall, with specific points below.

The attached Zenodo archive includes `examode.owl` and conversion to several other formats. (A) iI includes a description but not a README per se. (B) The files are complete ontology files. (C) Zenodo is appropriate for archiving. (D) No other data artifacts are provided.

# General concerns

I do, however, have a number of concerns about how the ontology is built, which I hope will lead to clarifications and improvements of the ontology itself. These criticisms may also help with revisions of the paper.

This project seems to be squarely aimed at annotation, and I am persuaded that it is suitable for that purpose. When I look at the details of the OWL file, I see several problems. While it is good to reuse existing terms, ExaMode includes quite a mixture of terms from different source ontologies with different modelling strategies. For example, UBERON logically defines 'endometrium' as equivalent to ""mucosa and part of some uterus"", but ExaMode asserts that 'endometrium' is `owl:partOf` NCIt's 'mucosa' term. The OWL specification does not include `owl:partOf`, but more importantly this should be a subclass relation, and I do not understand why NCIt's 'mucosa' term was used instead of UBERON's 'mucosa' term. Term reuse aids with interoperability, but inconsistent term reuse undermines that goal.

It was not clear to me from the paper, but looking at the OWL file in Protege I was quite surprised that UBERON and MONDO classes had been ""demoted"" to owl:Individuals. This was not always done consistently, so MONDO:0002271 'Colon Adenocarcinoma' is an owl:Class, but MONDO:0002032 'colon carcinoma' is an owl:Individual. It suggests that there were two 'Resection' procedures, but I suspect there was only one. For the outcome on the left we have 'Mild Colon Dysplasia' as the `rdf:type`, but on the right the same term is used as the object of a `exa:hasDysplasia` predicate, and 'Moderate Colon Dysplasia' is also specified.

More generally and more subjectively, ExaMode's four ""semantic areas"" seem to each have quite different modelling approaches, which become apparent when looking at the OWL file as a whole. For example, an 'Onset' is not a subclass of 'Annotation', although a record may be annotated with information about an onset.

Although the authors do make good reuse of many existing ontology terms, I believe that there are good candidate terms in OBO and elsewhere for many of the terms that they do create in ExaMode. I would encourage the authors to expand the scope of their collaboration, and grow ExaMode toward closer integration with the larger open ontology community.

# Specific points about the manuscript

- Abstract and elsewhere: ""four largely diffused and studied histopathology diseases""; ""diffused"" seems a strange word choice to me (as a native English speaker)
- Page 2 line 3: ""complexity increment"" is another strange word choice; ""increase in complexity""?
- Page 2 line 17: ""is subjected to"" -> ""is subject to""
- Page 4 line 27: I do not understand how the the NCIt is ""More than an ontology"", when the rest of the sentence spells out what a good ontology should be
- Page 5 line 20: EBI's Ontology Lookup Service is not limited to OBO (flatfile) format, it supports OWL format
- Page 10 figure 1:
  - `doid:Disease` should be `doid:4`
  - it seems strange to use one term 'patient' from IDOMAL when there are candidates in ontologies you are already using, such as OAE
  - there are existing ontologies covering gender (from multiple perspectives); the `examode.owl` file actually uses NCIt 'gender'
- Page 11 line 44: `exa:NegativeOutcome` is elsewhere referred to as `exa:NegativeResult`
- Page 12 line 1: `oae: 0001850` should not have a space
- Page 12 figure 3: HP defines 'Onset' as ""The age group in which disease manifestations appear."" I don't see how this can be a *subclass* of `exa:Annotation`, or a sibling to `exa:SemanticArea`.
- Page 13 line 25: HPV is referred to earlier, but this is the first time that abbreviation is spelled out.
- Page 14 figure 4: The OWL specification does not define `owl:partOf`. UBERON logically defines 'endometrium' as equivalent to ""mucosa and part of some uterus"", so I believe that 'endometrium' cannot be part of 'mucosa'.
- Page 15 line 4: `uberon:0001052` has label 'rectum', but ExaMode seems to be relabelling it ""rectum, Not Otherwise Specified (NOS)"". If this is the case, I would prefer that the authors emphasize the changes they are making. This applies to several other ""NOS"" terms and any other label changes.
- Page 18 table 2: There is no comparison to a ""gold standard"" annotation of the reports, that I can see. Figure 7 below shows that not all relevant terms are in ExaMode.
- Page 18 line 43: The fact that clinical reports were translated to English is only mentioned briefly here, but I think it requires more discussion. How effective was this translation?
- Page 19 figure 7: Not all the highlighted words are present in ExaMode -- I would like to see a discussion of this.
- Page 19 figure 8: The text of the WebVOWL graph is illegible.
- Page 20 figure 9: I am quite confused by this figure. Were there really two 'Resection' procedures? I suspect there was only one. I do not understand how 'Mild Colon Dysplasia' is the type of one outcome while being a property of the other, or how one outcome has both mild and moderate colon dysplasia.

# Specific points about the examode.owl file

- I do not undertand why the various anatomical classes from UBERON and NCIt have been changed to owl:Individuals, and likewise for MONDO disease classes.
- I do not understand why MONDO:0002271 'Colon Adenocarcinoma' is an owl:Class, but MONDO:0002032 'colon carcinoma' is an owl:Individual.
- `https://hpo.jax.org/app/browse/term/HP:0003584` is not the correct identifier for ann HPO term. It should be `http://purl.obolibrary.org/obo/HP_0003584`
- The paper refers to `exa:Gender` but the OWL file uses `NCIT:C17357`.
","gpt-4o-2024-11-20, Jon Wasky",extremely positive
2984,"The authors present the Helio framework for building and
publishing KGs as Linked Data. The framework sets its pillars on top of several requirements that establish the life-cycle of the KGs, meeting these requirements and also allowing practitioners
to publish KGs following the Linked Data principles.
Furthermore, the framework counts with a plugin system that prevents the generation of ad-hoc code that is
not reusable to address novel challenges identified in
new scenarios.

--This manuscript was submitted as 'Tools and Systems Report' and should be reviewed along the following dimensions: (1) Quality, importance, and impact of the described tool or system (convincing evidence must be provided). (2) Clarity, illustration, and readability of the describing paper, which shall convey to the reader both the capabilities and the limitations of the tool. 

As discussed in the Discussion section of the paper, the tool seems to have had impact already and has been adopted. The tool is clearly of importance, and the work is of high quality. On criteria (2) I think the paper could use some proofreading. In particular, too much passive voice has been used, which can make it hard to follow the writing sometimes. Also, there is the occasional typo or grammatical error e.g., 'cicle' rather than 'cycle' in the conclusion, or phrases like 'conforming the life cycle' in the abstract. Therefore, I encourage the authors to correct these problems and proofread the article as a minor revision. 

Finally, although the authors explain the lack of experimentation, I would like to see (perhaps in a future work section) how this could potentially be addressed. 

--Please also assess the data file provided by the authors under “Long-term stable URL for resources”. In particular, assess (A) whether the data file is well organized and in particular contains a README file which makes it easy for you to assess the data, (B) whether the provided resources appear to be complete for replication of experiments, and if not, why, (C) whether the chosen repository, if it is not GitHub, Figshare or Zenodo, is appropriate for long-term repository discoverability, and (4) whether the provided data artifacts are complete. Please refer to the <a href=""/reviewers"" target=""_blank"">reviewer instructions</a> and the <a href=""/faq"" target=""_blank"">FAQ</a> for further information.

The long-term stable link to resources is adequate from what I have been able to see. 

","gpt-4o-2024-11-20, Jon Wasky",'slightly positive'
2984,"
TL;DR:

- Very important idea and promising implementation with some serious limitations
- Relatively well documented.
- Good adoption, but the tool should be demonstrated with one of the use cases in more detail.
- The capabilities and limitations are not adequately described. Reformulating and substantiating the requirements may help.



# (1) Quality, importance, and impact of the described tool or system (convincing evidence must be provided). 

The system aims to address an important gap for the lifecycle of knowledge graphs. A configurable framework for supporting the entire lifecycle of knowledge graphs would be a great contribution to the literature. So the importance of the system (at least the intention of the system) is quite high. However, I have some rather major points I'd like to make about the presented tool:

- In the abstract, the authors make a claim that Helio reduces the effort required to perform knowledge graph lifecycle tasks. This is not supported by any qualitative or quantitative evaluation. By this, I do not necessarily mean a table with numbers about the performance of the tool. Its demonstration with a ""real-world"" use case would be very effective (see also my next point).

- The primary argument for tools impact is that the tool satisfies a list of requirements and is adopted by various projects and academic works. The second part of the argument is substantiated to some extent with a large list of projects making use of the tool. All appear to be research projects in which authors (or their research groups) are involved. If there are use cases outside of this circle, it should be prominently stated as it turns out to be an important criterion. I think it would be beneficial to describe one use case in detail to demonstrate the impact of the tool for implementing the entire lifecycle. DELTA use case looks like a good candidate for this.

- A rather subjective point: The first paragraph of the introduction gives the impression the KGs can be only in the RDF data model, which rules out the significant work on property graphs.


# (2) Clarity, illustration, and readability of the describing paper, which shall convey to the reader both the capabilities and the limitations of the tool. 

Overall the paper is very well written and easy to follow. 

There are some issues with the description of the requirements, the first part of the impact argument mentioned above. These requirements are not properly substantiated in terms of their source. Are they coming from the use cases provided in the paper? Moreover, the wording of the requirements is quite strong and I was not completely convinced that the tool covers all of them as claimed. For example, R06 gave me the impression that the tool allows plugging different mapping engines in, but in fact, it just channels externally created RDF data into a triplestore. Similarly, R11 says that the system must support at least one mechanism for various knowledge curation tasks, but it does not actually provide any mechanism intrinsically but provides a SPARQL endpoint that allows interaction with other tools that support SPARQL. The formulation of the requirements also has an effect on the literature review as it leaves out curation tools completely because they are not part of the system. Moreover, many triple stores have SHACL supports which would make them satisfy R11 to some extent.

Overall, I think the capabilities and especially limitations of the tool are not adequately defined at this stage: 

* The potentially interesting parts like linking with different knowledge graphs are very briefly mentioned. Does it also allow linking with knowledge graphs that are outside of the influence of Helio? How does it work? These can be explained a bit more. The focus is too much on creation, which I think misses the intention of the tool.

* How the curation tools are integrated into the lifecycle not described (how is it different than running them on a triplestore with a SPARQL endpoint? Can I run them periodically or after an RDF import?).

* No mention of what to do with conflicting property values during generation (in case an instance comes with a URI that already exists in the KG)

* No mention of quality assessment (which in my opinion is an important part of the lifecycle)  

Some other minor issues:

* Page 3 lines 45-47: How is embedded RDF data in an HTML page targeting humans and not machines? Search engines extract annotations from web pages all the time.

* Linking is presented first as part of the curation task, then as part of the creation module.
* page 6 line 36: Why ""despite""?
* page 6 line 29 Stardog is also a triplestore.


# (3) Assessment of the data file provided by the authors under “Long-term stable URL for resources”. 

## (A) whether the data file is well organized and in particular contains a README file which makes it easy for you to assess the data,

The documentation of the tool and toy examples for getting started is very well documented. 

## (B) whether the provided resources appear to be complete for replication of experiments, and if not, why, 

There is an important part missing though: use cases. Use case descriptions are listed as ""to do"" which I think is the most important point to verify the arguments made by the authors.

## (C) whether the chosen repository, if it is not GitHub, Figshare, or Zenodo, is appropriate for long-term repository discoverability,  

GitHub is used.","gpt-4o-2024-11-20, Jon Wasky",slightly negative
2984,"--------
Overview
--------

The paper proposes the Helio framework as a standard pipeline that generalizes the building and publishing of KGs around the following main tasks: KG Creation, Hosting, Curation, and Deployment. The paper elicits a set of core requirements for the Helio framework around the KG lifecycle which are also shown to be incorporated. The usage of Helio in various endeavors including research projects, scholarly articles, and bachelor thesis shows its impact and usefulness in the community. Such a framework is also the first of its kind.

---------
Strengths
---------

*It explicitly elicits the stages of knowledge graph creation and concretizes it as software that implements the pipeline. This work is somehow the first-of-its-kind.

*The pipeline handles the following the five stages: Knowledge graph creation; Knowledge graph hosting; Knowledge graph curation; and Knowledge graph deployment. Each step carefully addresses and satisfies the set of requirements elicited in prior work [1] from their surveyistic observations of the KG life cycle.

*The software is quite user-friendly even though it handles a set of tasks that have generally been implemented via adhoc implementations in the community. This goes to say that the authors have well surveyed the field to engineer and design the essential characteristics for this software.  

------------------------
Questions to the authors
------------------------

*Could it be explicitly specified somewhere the input file formats that will hold the specifications for the various modules? It would be beneficial to see a few samples (maybe in Appendix) of use-cases when one module is already satisfied, i.e. perhaps no data conversion is needed, and just one other module needs to be implemented, e.g. data publishing, etc.

*Are there some modules that are indispensable to each other where one could work without the other but not the other way around? E.g., data hosting and data publishing, where data hosting would work without data publishing, but not vice versa.

*Are there some modules in Helio that are mandatory to specify?

*Are there are some test units in place to check that the specifications provided conform to those programmed in Helio? If so, a new section discussing the Helio Test Suite would also be very informative to the reader in my view.

-----------------
Typos and writing
-----------------

The paper would need to be proof-read for the final version. I noted a few typos in the text. Some of them are listed below.

Line 38, column 1, page 1: “growth” -> “grown”
Line 6, column 1, page 2: “pipe lines” -> “pipelines”
Line 41, column 1, page 2: “relayed” -> “relied”
Line 25, column 2, page 2: the semicolon seems misplaced
Line 1, column 1, page 3: “during” -> “During”
Line 8, column 1, page 3: “practitioners” -> “Practitioners”
Note Line 3, column 1, page 7 reads “Hosting Module,” but Figure 2 reads “Host Module.” Similarly, Line 12, column 1, page 7 reads “Curation Module,” but Figure 2 reads “Curator Module.” My suggestion would be for the corresponding labels to match.
Line 39, column 2, page 8: “uniquely” -> “unique”
I stop here, but again my recommendation to the authors would be to have their paper proof-read thoroughly for minor writing grammar errors and typos. 


----------
References
----------
1.	Fensel, Dieter, et al. Knowledge graphs. Springer International Publishing, 2020.","gpt-4o-2024-11-20, Jon Wasky",'slightly positive'
2990,"
This manuscript discusses the ontology engineering aspects of an ontology named ROH. It captures the entities and relations in the academic and research domains. ROH reuses concepts and relations from existing vocabularies/ontologies. Competency questions and SHACL rules are used to validate ROH.

I appreciate the authors for putting in efforts to build a high quality ontology.

The following are the strengths of this submission.

1) Several terms from existing vocabularies/ontologies have been reused in ROH.
2) A good number of competency questions (CQs) have been used to validate the ontology.
3) A continuous development and integration step has been included in the ontology engineering process where the competency questions are rerun, and the ontology documentation is regenerated when there is a change to the ontology.
4) Good documentation has been provided for the ontology.
5) Permanent URLs have been used to identify the ontology resource.
6) Code is open-sourced.
7) The labels and descriptions of classes and properties are available in English and Spanish.

I have the following questions/suggestions for the authors.

1) A sample/synthetic dataset is generated and used to validate ROH. Why isn't real-world data from a university used instead? Please replace the synthetic dataset with a real-world dataset to validate the ontology.
2) The use of ontology design patterns (ODPs; http://ontologydesignpatterns.org/wiki/Main_Page) would make the ontology more modular. I would encourage the authors to explore the ODP repository and pick a few relevant ODPs to use in ROH. AgentRole ODP (http://ontologydesignpatterns.org/wiki/Submissions:AgentRole) and ActivitySpecification ODP (http://ontologydesignpatterns.org/wiki/Submissions:AgentRole) are two possibilities.

Other comments/questions.

1) As mentioned in Section 5.3, the authors consider Funding as an action. Should there be a class for an action or would a property be more suitable? To me, the latter (property) seems more appropriate, especially after looking at Fig. 7, where several classes are connected to funding through the same relation (funds). We can use roh:funds to connect these classes with the classes that roh:Funding connects to (perhaps, for example, Project?). Properties such as hasFundingID can also be added to capture the other details. Also, on page 10, line 49 (and Eq. 1), funds to some Funding seems wrong.
2) From Section 5.3, having FundingAmount as a class is a little confusing. What are the potential instances of this class? Why can't it be a property?
3) In Table 2, is ""Researcher Role"" a single class? Is it meant to represent a Researcher? If so, a Research Fellowship is a type of fellowship and does not seem like a researcher or a role played by a researcher?
4) In Table 2, what is the difference between the Subject and Degree entities?
5) In Table 2, the subclasses of Internship are Predoc and PostDoc. I don't think this hierarchy is appropriate because interns are temporary whereas the other classes are full-time positions.
6) On page 10, it is mentioned that entities have categories instead of a hierarchy based on some criteria. What are these categories, and where are the criteria defined/discussed?
7) How are the rules (Eq. 1, 2, 3, ...) implemented?
8) On page 13, the range of inScheme can be either KnowledgeArea or ProjectClassification or HRClassification or FundingProgramClassification. Is it justified to use this property in four different contexts?
9) In page 17, why is hasPublicationVenue connected to a Collection? What information is captured as part of the publication venue?
10) Does Eq. (9) mean that there can only be two publication metrics for a Journal? The case when z is equal to t is not handled.
11) On page 18, hasMetric connects to Journal as well as a  JournalArticle. What is the relation between a Journal and a JournalArticle? Unless one is a subclass of another, this doesn't seem right?
12) On page 20, generally, properties follow the lower camel case naming convention. For some properties, such as roh:ImpactFactorName, this is not followed.
13) Include a brief explanation of Listings 6, 7 and 8.
14) Section 6.4, why should Pellet be compiled when there are changes to ROH? Why can't the executable of Pellet be used directly?
15) In Section 7, it was mentioned that ""machine learning techniques will be applied to continuously enhance the existing contents of universities’ knowledge graphs"". This seems very vague. Either add a few more details or drop this line.
16) Perhaps the short Section 3 can be merged with the ontology description section.
17) Figure 1, how were the requirements gathered?
18) Table 1, how were the scenarios identified?
19) Page 9, line 44, vivo:Relationship seems to be a very general relation that can be used anywhere.
20) Please comment on the OWL 2 profile/description logic to which ROH belongs.

Typos/grammar issues

1) Page 1, Introduction, line 1, add ""the"" between presents and Hercules Network of Ontologies.
2) Page 1, line 42, it should be funding rather than founding.
3) Page 2, line 15, please rephrase the usage of the word ""describing"" here.
4) Page 2, line 20, ""MA"" should be ""The main"".
5) Page 4, line 9, the word ""on"" can be dropped.
6) Page 4, lines 32 and 33 should be rephrased.
7) Page 5, line 43, this line should be rephrased. ""At this analysis"" => ""After this analysis""?
8) Page 6, line 27, it should be eg., European.
9) Page 6, line 49, ""which they have participated"" => with whom they have collaborated.
10) Page 6, line 50, ""which are"" can be removed.
11) Page 7, line 33, extend => extent
12) Page 7, line 46, at => in
13) Page 9, line 32, it should be roh:PeerReviewedArticle and not PeeReviewedArticle.
14) Page 10, where and how is the Web Annotation Data Model used? Anotation is spelt wrong in Table 4.
15) Page 11, line 7, the phrase ""sparsely but just in succint remarks"" should be rephrased.
16) Page 13, line 21, at => in
17) Page 20, line 13, cite => citation
18) Page 21, line 7, ""Last"" can be removed
19) Fig. 13, competence => competency
20) Page 26, line 27, the phrase ""particularities of"" can be removed.

In general, please run a grammar checker on the entire document.
","gpt-4o-2024-11-20, Jon Wasky",'slightly positive'
2990,"The goal of this article is presenting the development of the Hercules Network of Ontologies (ROH). ROH is a set of ontologies that models the research and academic domain (e.g. projects, researchers, academic articles, universities, courses, organizations, research results and so on). 

The abstract is well-written, since it clearly and concisely presents the purpose of the article. The main goal of the paper is to describe the methodology followed for the development of ROH as well as all the steps for its implementation and continuous validation.

The introduction very clearly presents the reasons why the authors have decided to develop ROH. It is the result of a larger project (Hercules - https://www.um.es/web/hercules/ontologias) which aims to create a new information management system for Spanish universities based on Semantic Web and Knowledge Graphs technologies. In this context, ROH represents the ontology infrastructure with the aim of describing with fidelity and fine granularity the research domain.

(1) Quality and relevance of the described ontology: I believe ROH is relevant and suitable for purpose, although I have some concerns about quality that I am going to discuss below.
The implementation of the ROH network of ontologies was carried out following an iterative and incremental methodology. Even though any ontology engineering methodology is referenced, the implementation process is described systematically as well as the design principles taken into account. The modular approach applied during the design and development of ROH allows third users to reuse and extend the ontology in an easy way (e.g. adjusting it to different contexts). The three mechanisms (competency questions, the modeling of the CV,  and the SHACL validation) used to validate the ontology are adequate and well described. 

(2) Clarity of the paper: Good overall. 
I think the paper is well written and all ideas are adequately presented with the support of  explicit and comprehensible figures and tables. The developed ontology is properly described and the content of the paper is clear and readable. 
The GitHub repository includes (A) a well organized README file that contains useful information to understand and assess the data (B) the ontology modules files in .ttl format, but any .owl file is available. It includes also other resources that appear to be complete for replication of experiments (e.g., validation data and validation questions). (C) GitHub is appropriate as a repository. (D) No other data artifacts are provided.

Review comments: 
1. It was not clear to me from the paper, but looking at the ontology in Protégé I was quite surprised to see that entities reused in ROH have not kept their original IRI. For example, the reused class foaf:Agent has this IRI: http://w3id.org/roh/mirror/foaf#Agent rather than http://xmlns.com/foaf/spec/#term_Agent. In this regard, I think that the reuse of entities is not described in detail in Section 4.2. There are no insights about the external import used, e.g. whether the process is manually or automatically performed. 
2. Information about the ontology language and the expressivity of the ontology should be introduced in the paper. 
3. I expect that the .owl file will be added in the repository. 
The link https://herculescrue.github.io/ROH/0%20-%20OntologyTutorial.pdf is broken. 
4. I think the authors should extend the usage scenarios outside the Hercules project. It would be interesting presenting further potential uses of the ROH by giving concrete examples. ","gpt-4o-2024-11-20, Jon Wasky",'slightly positive'
2990,"This paper presents the ROH modular ontology dedicated to model the academic and research domains (universities, organizations, projects, researchers, papers, etc.), especially in Spain. It is designed to be reusable and extendable to suit any other country specificities. The paper is well written and the key aspects of the ontology are clearly described.

The paper reports an important ontology engineering work, following a precisely described methodology, and resulting in an ontology accessible on GitHub, validated with a base of SPARQL queries implementing competency questions and that can be continuously refined following a CI/CD software development practice.
A description of an existing application or use-case experiments are missing. Relatedly, an evaluation of the ontology is missing: a discussion on the competency questions and their implementation, a user evaluation showing the easy adoption of the ontology and an application relying on it. 

ROH is publicly available on GitHub. Additionally, I would expect to find it published on the Web, referenced in LOV, browsable using a dereferencing mechanism and/or queryable through a public SPARQL endpoint.

Regarding the design principles and methodology used to develop ROH and described in the paper, their description shows good practices when developing. However a positioning with respect to state of the art ontology development methodologies is missing. Also it may be interesting to develop and discuss the presentation of CICD workflow implemented in GitHub.

On OWL and SHACL:
- “ontological restrictions which validate the correct instantiation of classes and properties”: OWL class definitions using property restriction are not meant to perform any validation, but inferences.
- I would consider SHACL constraints as domain knowledge, part of the model, and not as a mean to validate the ontology.

On the mapping of FECYT’s CVN to ROH: again, I do not see it as a mean to validate ROH but rather as part of knowledge extraction process to build the ontology.

On the vertical module “knowledge area”: I am wondering what are the relationships between the scientific domains, the subject areas, the UNESCO codes and the FECYT referential. I guess some alignment work could be done.

Other local remarks and typos:
page 3, end of section 2, the positioning could be more precise
page 3 class.So
page 5 first sentence useless ad reference to Section 4 wrong since we still are in Section 4.
page 6 Table 1, first column, choose between a noun or a verb for each entry; line 7 last column reformulate without “return” like in the other entries
page 8-10 The choice of having a categorization in addition to a hierarchy is not obvious and should be explained
page 12 I suppose there is a confusion between “subclass of skos:Concept from the custom ontology” and SKOS concepts (instances of class skos:Concept) from the custom thesaurus.
page 13 and the following: I suggest to avoid the terms “entity” and “term” and rather use precise terms concept, class or property because entity or term can have a special meaning in a thesaurus. 
page 15: represent -> s
page 17 has been -> have
page 19 focus in -> on
page 20: competency query -> questions
page 23: Listing 4 is useless
page 25: “This task …” These 2 paragraphs should be developed and better explained.
","gpt-4o-2024-11-20, Jon Wasky",neutral
2997,"Thank authors for considering and addressing most of my previous comments in the prevision review round. The current version includes additional Background section, the search method, the significance in the introduction section, and limitation of the study. Review results table is better presented now and made accessible for other researchers. I think the current manuscript looks suitable to publish and will contribute to the field. However, there are some changes or improvement need to be made:

The quality (resolution) of Fig 1. PRISMA flow diagram is too low to read.
In the result section, the reviewed papers were presented in detail but I miss the connections. Can they be compared or contrasted with each other (differences or similarities?)
The General Recommendations will be very helpful for the researchers in the field. In my opinion, it can be more explicit and elaborate on more concrete actions. For example, according to “confidentiality, privacy and provenance mechanisms should coexist in favor of an in-depth privacy compliance solution for health data handling”, what is recommended to do then?
It would be better if “The limitation of the study” goes to the Discussion section. What are the potential impacts of these limitations? 
In general, the current manuscript is much more complete and structured after revision. I think it can be published after minor changes. Thanks.","gpt-4o-2024-11-20, Jon Wasky",slightly positive
3022,"The paper presents a a study of similarity in Wikidata and the impact that retrofitting (subsequent training of embeddings to fit with external information, in this case similarity of entity pairs in Wikidata) can have on both KG and text-based embeddings similarity.

This is a very relevant topic with several interesting applications in multiple domains.

I believe the paper would be much improved by addressing the following issues:

1. Related work

There is avery relevant paper in this area that is not covered in the related work:
Lastra-Díaz, Juan J., Josu Goikoetxea, Mohamed Ali Hadj Taieb, Ana García-Serrano, Mohamed Ben Aouicha, and Eneko Agirre. ""A reproducible survey on word embeddings and ontology-based methods for word similarity: linear combinations outperform the state of the art."" Engineering Applications of Artificial Intelligence 85 (2019): 645-665.

This is a fully reproducible paper, and producing results in the same conditions for the proposals presented in the manuscript would increase its value substantially.


Moreover, this work combines language models and KG embeddings, but does not cover the related work that covers this overlap.
Wang, Zhen, Jianwen Zhang, Jianlin Feng, and Zheng Chen. ""Knowledge graph and text jointly embedding."" In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pp. 1591-1601. 2014.
Xie, Ruobing, Zhiyuan Liu, Jia Jia, Huanbo Luan, and Maosong Sun. ""Representation learning of knowledge graphs with entity descriptions."" In Proceedings of the AAAI Conference on Artificial Intelligence, vol. 30, no. 1. 2016.
Peters, Matthew E., Mark Neumann, Robert L. Logan IV, Roy Schwartz, Vidur Joshi, Sameer Singh, and Noah A. Smith. ""Knowledge enhanced contextual word representations."" arXiv preprint arXiv:1909.04164 (2019).

2. Clear definitions
The concept of retrofitting is only presented very late in the text. This is not something most readers will be familiar with and it does represent an important aspect of the work. It should be defined in the introduction to help understand the goals. A definition of KG embeddings and text embeddings is also lacking. Although these are increasingly commonplace, and soft introduction to these terms would improve the readability. 

3. More focus on novel contributions
Retrofitting appears to be the more original aspect of the work. However, this is not described or analysed in depth. Results are only shown for cosine similarity of DistilRoberta embeddings. 
How the pairs are built is not very well described. The word edge I believe is used to mean a pair of concepts. The selected pairs are in all likelihood quite similar (parents and siblings),  and the distribution of similarity for these pairs is not studied.  

""We focus our experiments on cosine similarity as a
weighting function, because we observed empirically that it consistently performs better or comparable to the other
two weighting functions."" This is a pity. This is exactly what I was hoping to find in the paper. In the end, I am unsure if there is any real advantage of using wikidata to measure conceptual similarity, or if we are simply better off just using language models.

The analysis of the quartiles is potentially quite interesting, but results are not easy to read (no table), and now the performance metric is F-measure, which is not at all clear how it is computed.

I strongly advise the authors to apply their retrofitting method in the same datasets and conditions of Lastra-Diaz et al.

4. Justifications and clarification of methodological aspects

How TopSim is computed is not clear at all. Is this the measure proposed in 10.1109/ICDE.2012.109? 

Why does Composite-6 not include  labels and desc?

Why is DistilRoberta used for abstract, labels, label+description and BERT-base for lexicalization?

5. The large size of wikidata is referred to multiple times, but the application was at most to a few hundred entity pairs, what are the true implications of the large size of Wikidata for similarity estimation?).

Minor: it would be better to employ the terminology defined by Lastra-Diaz et al when categorizing the different similarity metrics.


In summary, there is an interesting idea in applying retrofitting to concept similarity with KGs. However, the paper does not consider related work appropriately, which limits the value of its contributions (see Lastra-Diaz et al). It also does not afford sufficient detail in the description of the methods and choices, and could be much richer in terms of tested configuration, presented results, and discussion.","gpt-4o-2024-11-20, Jon Wasky",'slightly negative'
3022,"In this work, the authors propose a framework for concept similarity in Knowledge Graphs with a focus on Wikidata. The authors focus on studying the impact of different similarity methods, language models, and knowledge graph embeddings for concept similarity. Retrofitting technique is deployed to the embeddings which iteratively updates the node embeddings to bring their connections closer using an external dataset. In this paper, the authors adapt retrofitting to tune embeddings from KGs and Kms to Wikidata and ProBase.
The framework has been evaluated on three different benchmarks: WD-WordSim353, WD-RG65, and WD-MC30.
Pros:

1. The paper is well written and easy to follow, and different combinations of framework variants have been used in the evaluation against the three benchmarks. 
2. The approach has been well explained using the diagram. 
3. The code and data are made available by the authors.
However, I have the following concerns about the paper:

1.  The paper has a brief description of the related work, however, some of the works on concept similarity have not been mentioned [1,2]. Also, the gap in the current research and the proposed model has not been clearly pointed out. 
    1. Alkhamees, M.A., Alnuem, M.A., Al-Saleem, S.M. and Al-Ssulami, A.M., 2021. A semantic metric for concepts similarity in knowledge graphs. Journal of Information Science, p.01655515211020580.
    2. Zhu, G. and Iglesias, C.A., 2016. Computing semantic similarity of concepts in knowledge graphs. IEEE Transactions on Knowledge and Data Engineering, 29(1), pp.72-85.
2. The results of the framework have been presented using different variants which are its features and different embedding models for the benchmarks. But the framework has not been compared with any of the existing baseline models. Therefore, it becomes difficult to analyse the effectiveness of the model and its utility. I would strongly recommend the authors compare their model with the state-of-the-art models in concept similarity. Otherwise, the impact of the obtained results could not be understood.
3. The framework comprises node embedding models such as DeepWalk as well as KG embedding models such as TransE. It is not mentioned how the node embedding models are trained on a KG. Is the relation information ‘r’ in a triple <h,r,t> ignored? If yes, why? It would be nice to have an explanation of how these models are trained. Furthermore, statistics of the KG trained are missing, i.e., what is the size of the KG in which these Node embedding and KG embedding models are trained, in terms of no. of triples, no. of entities, no. of relations, etc.? Also, the readability will be improved if a few real examples of the triples used in the KGs are provided in the paper while providing the explanation.
4. The abstract, labels and label + descriptions have been used in the framework. I assume that “abstract” and “description” are the same thing. However, it creates confusion. If it is the same please use uniform annotation, otherwise, I would recommend pointing out the difference in the paper. 
5. There are no detailed details about DBpedia-RG 65, and DBpedia-MC 30. In what ways are they different from WD-RG65, and WD-MC30? 
6. From where are the abstract/descriptions are extracted for the Wikidata dataset in Table 3? Are they extracted from Wikipedia? If yes, then why the results are not the same as DBpedia in Table 2 because it consists of the same abstracts from Wikipedia?
7. I would recommend the authors list down the major contributions by the end of the introduction section, to improve the readability. Also to provide the related work at the beginning, so that the research gaps are understood before delving into the contribution made by this paper. 
8. Minor: The link to google drive is available only after making a request. It would be nice to have it freely accessible without access request. ","gpt-4o-2024-11-20, Jon Wasky",neutral
3024,"(1) originality, 

*** 
The SOHO ontology has been first presented in a previous publication by the same authors (see, [13] A. Umbrico, A. Orlandini and A. Cesta, An Ontology for Human-Robot Collaboration, Procedia CIRP 93 (2020), 1097–1102). As far as I can see, the proposed extension of the SOHO ontology mostly (if not entirely) concerns the introduction of axioms defining the different identified collaborative modalities, that is, the ""independent"", the ""simultaneous"", the ""supportive"" and the ""synchronous"" modality (see, pp. 7-8). Hereafter I report my questions/comments about the presentation and the specification of the ontology, while taking into account the results presented in [13]: 

- In [13], the authors already mention that the four introduced human-robot interaction modalities have been defined. Considering that, I struggle in clearly characterising the original content presented here for what concerns the ontology modules.
- I noticed that part of the axioms originally introduced in [13] have been modified in the version of the ontology as in the present contribution. Examples are the axioms defining the concepts of ""Function"", ""ComplexTask"" and ""SimpleTask"". I would suggest the authors to comment on these changes, possibly explaining why they have been performed: what was wrong or not satisfactory enough with the previous version of them. Moreover, there are few details concerning the newly introduced axioms which puzzle me.
- In the new definition of ""SimpleTask"" (""ComplexTask""), for instance, the role ""isConformTo"" is used but, I wasn't able to find in the text a sentence explaining the reason why it has been introduced, especially if one considers that it was not present in the definition of ""SimpleTask"" (""ComplexTask"") in [13].
- The filler (using a DL-based terminology) of role ""isConformTo"" is in (as far as I can see in the paper) the union of ""OperativeConstraint"" and ""InteractionModalitiy"" but, the authors do not further argue about the details behind this kind of ""norms"", as they call them. In particular, I wonder whether an ""IndependentTask"" (and the same holds for the others ""collaboration modalities"" on pp. 7-8) has to be understood as a specialisation of the concept ""InteractionModality"".
- Which are the differences, if any, between ""ProductionNorm"", as introduced here, and ""ExecutionNorm"", as introduced in [13]? I noticed that in the definition of the concept ""Function"" the occurrence of ""ExecutionNorm"" has been substituted by an occurrence of ""ProductionNorm"". Should we consider them as two syntactic variants for the very same meaning or there is a more fundamental distinction between them?
Of course, I opened and analysed the ontology files and I found the correct answer by myself. However, I think that the paper should be self-contained with respect to this argument and should clarify justify and explain the suggested ontology internal structuring without asking the readers to look into the ontology modules specification by themselves.
- The discussion of the foundational counterpart of SOHO is basically the same as in [13] or, at least, I did not notice any major difference or improvement. This is not well justified to me. It is a fact that the authors decided to not reproduce in the current paper a substantial part of the original SOHO ontology as it is presented in [13] and, in my opinion, this negatively affects the overall understanding of the latest version of the ontology. Nonetheless, they also decided to entirely report, with no substantial modification, the arguments about foundational ontologies. Honestly speaking, I don't see the rationale behind the decisions: my suggestion is to remove the sections about the foundational ontologies and use the space gained to provide a comprehensive introduction to the SOHO ontology. 
- The bibliographic references for CORA and SSN can be found at the beginning of Section 2.2 but the two ontologies are firstly mentioned at the end of the previous Section 2.1.
- The very same content of Section 2.2 was already present in [13], Section 2.2 (by chance, the two sections share the same numbering in the two different papers but, not surprisingly, they have the same title ""2.2. What is Missing for HRC?"").
- I understood, from the definition of the algorithm on p. 10, that the role ""hasConstituent"" is not transitive in the introduced ontology. Is this correct? If yes, I would suggest to (also) make this claim explicit in the sections describing the ontology axioms (which come earlier in the paper).
- My last comment about the ontology axioms  refers to the usage of the term ""synchronous"", which associate to 'simultaneous' and 'contemporary'. Then I discover that , actually, this is not the case since a synchronous task in the CAPITAL-GOODS scenario requires human and robotic functions to follow ""a strict temporal ordering"" (synchronised on the very same target object). In the very same axiom, at p. 7, the synchronous task is defined as something being conformant to one and only one ""SequentialExec"", which reminds me the sequential order of, first, placing a bolt and, second, screwing it.
A I mention before, the fact that the role ""isConformTo"" and its possible fillers, i.e., the concepts ""Independent"", ""Simultaneous"", ""SequentialExec"", ""ParallelExec"" , ""Supportive"" and ""Synchronous"", are not further specified makes the clear understanding of the axioms quite difficult, in my opinion.
It is probably because of this lack of understanding that I find the definition of an 'independent task' <as something being conformant with one and only one 'independent' thing> quite tautological from a pure conceptual point of view (obviously, the same comment applies to the other types of tasks on pp. 7-8). If I had to clarify further my doubts I would say, for instance, that my intuition (that could easily be fully wrong) tells me that what the concept of ""SimultaneousTask"" means strictly depends on the meaning of the concept ""Simultaneous"", but this last is not introduced nor discussed in the paper unfortunately.

The introduction of an algorithm for the ontology-based automatic extraction of the relevant knowledge and planning constraints from the manufacturing scenarios at hands was not present in [13] and represents a completely original result introduced in the present paper. According to my understanding, and following the approach suggested by the authors, the algorithm plays a fundamental role towards the synthesis of the final planning model. I have several concerns about this and the sections 4 and 5 that I resume in what follows:

- I found the description of the algorithm quite difficult to follow and I think that showing an instantiation of its execution would be of great help. I take the decomposition graphs for each of the introduced scenarios as part of the information generated by the algorithm. What I completely miss is how these knowledge bases/graphs are then used in combination with the planning constraints, to create the admissible planning models.
On p. 15, the authors say ""this type of collaborative behavior is translated into a well-defined structure of temporal constraints into the task planning model"" but the way this is done is left to imagination and I think that this does not help the reader in properly evaluating the added value provided by the ontology-based approach introduced by the authors. The decomposition graphs are interesting but they represent the starting point only of the entire planning model synthesis, where the extracted knowledge is then fully exploited. My suggestion to the authors is to dedicate much more space to go into the details of this synthesis process, this way stressing the relevance of, and the added value provided by, the knowledge that is automatically extracted and formally represented according to SOHO.
- As a minor comment, and still connected to my previous point, I think that it would be helpful for the reader to have an explicit explanation on the semantics of the green bubbles in fig. 4 and of the green and the purple bubbles in fig. 6. Naïvely enough, and following the natural language explanation the text, one could think that the green bubbles ""AND_#"" assume different meanings in correspondence to the level of tree they appear (which is, by the way, counterintuitive!).
- I also have a question about the convergent arrows in the figure: Do the convergence of these arrows to a single human/robot function mean that the tasks they originate from (obtained by decomposition of the complex task above) are achievable by means of the very same function? (E.g., ""ProductionL1_h10"" and ""ProductionL1_h11"" converge to the leave ""Worker_tightening_nuts_reardoor""). Clearly, I cannot answer this question by myself 'cause I've no idea of what ""ProductionL1_h10"" and ""ProductionL1_h11"" are.
- Despite the fact the the first introduced scenario looks quite easy, I don't quite get the message of the last sentence on p. 12, line 24: what does it mean to 'execute a predicate'? Why the temporal order between the execution of the different highlighted function has not been made explicit here? Is there any implicit meaning in the used labels that refers to their (timeline-based) arrangement?
- Each single graph in Fig. 13 is said to represent the hierarchical relationships between the state variables as they have been generated for the corresponding scenario. However, I don't see how the rectangles ""Worker"" and ""Cobot"" can be understood as ""state variables"", according to the definition on p. 8, line 41-46, and I also suspect that the semantics of the arrow connecting ""Goal"" to ""ProductionL0"" is different from the one of the arrow connecting ""ProductionL1"" to ""Worker"". Could the authors please clarify this point.
- Fig. 6 includes two rectangles under the so-called ""ProductionL1"" state variable which do not have any incoming connection. Could the authors explain why this is consistent with the idea they introduce that the independent tasks which compose that state variable come from the decomposition of a complex task. Which is the complex task whose decomposition gives rise to the above independent tasks?
- My last comment here is about my almost complete inability to relate the arguments about the collaborative modalities and the way they are formally represented, as in the first part of the paper, and the arguments about the knowledge extraction algorithm and the planning model synthesis. Basically, I don't understand how to trace back the information provided for each application scenario (the decomposition graph and the associated text) to the axioms about the collaborative modalities. In the explanation the authors make claims about that but, formally, how is this working?


(2) significance of the results

***
I am not an expert in robotics but from the knowledge representation perspective, which is where my main competences are, I consider the arguments discussed in the paper of relevance for a wide audience of scholars. In addition to that, the more the automation technology is able to put on the market robotic systems that show a high level of autonomy, that can sense the environment and autonomously elaborate the collected observations, and that are meant to safely work with humans at their side, the more I think we need formal knowledge representation frameworks which provides declarative representations of the (human/robot) behaviours, competences, skills, functions, abilities in place. For Human-Robot Collaboration scenarios and related systems, being able to automatically derive dynamic and reconfigurable planning models for the involved agents, and theoretically sound proofs of their future actions, which is fundamental for safety reasons, is crucial nowadays in my humble opinion. 

Considering the attention that has been dedicated both from Robotics engineering and Computer Science to the human-robot collaborative scenarios discussed by the authors and the number of knowledge-based proposals that have been published, I miss a proper state-of-the-art section in the paper comparing the SOHO ontology with other similar artefacts. Related to that, here comes a list of potentially interest pointers from a survey I recently stumbled on, whose focus is indeed on ""robotic knowledge base systems"" (S. Manzoor et al., Ontology-Based Knowledge Representation in Robotic Systems: A Survey Oriented toward Applications. Appl. Sci. 2021, 11, 4324): 
- KnowROB: Know rob 2.0: a 2nd-generation knowledge processing framework for cognition-enabled robotic agents
- OROSU: Knowledge representation applied to robotic orthopedic surgery
- CARESSES: The CARESSES EU-Japan project: making assistive robots culturally competent
- PMK PMK: A knowledge processing framework for autonomous robotics perception and manipulation
- SARbot: High-level smart decision making of a robot based on an ontology in a search and rescue scenario
- IEQ: A Humanoid social robot-based approach for indoor environment quality monitoring and well-being improvement
- Smart Rules: An integrated semantic framework for designing context-aware Internet of Robotic Things systems
- ARBI: Ontology-based knowledge model for human-robot interactive services
- Worker-cobot: An ontology-based approach to enable knowledge representation and reasoning in worker-cobot agile manufacturing
- APRS: Implementation of an ontology-based approach to enable agility in kit building applications

Having in mind an additional state-of-the-art section, it would be also interesting in my opinion to look for ontology-based knowledge representation proposals in fields of application which are not directly related with industry, but where the human-robot collaborative modalities of working is central. A first promising application area that comes to my mind is the one of smart health and, even more specifically, the design and deployment of the so-called robotic surgical assistants (the OROSU proposal, here above, is just one among others).

Moreover, SOHO is declared to be built on top pf CORA and SSN but no further details are provided to clarify which are the axioms in SOHO that strictly extend both these ontologies. As far as I have been able to verify, in [13] a paragraph is dedicated to introduce the scope and limits of both CORA and SSN but, again, no forther details are provided to make explicit how SOHO formally builds on top of their integration and extends it with new concepts and roles.

Another point which is not completely clear to me is the one about the ALFUS framework, which is first mentioned in the paper on p. 3. 
I learn from the authors that ALFUS is ""a framework to characterize the autonomy levels of a robot"" and that an extension of it to include human operators was already introduced in [13] in order to represent their ability of working in autonomy: ""We take into account the framework ALFUS [16] to represent the levels of autonomy of robots and extend this model to human operators"". The problem I see comes from the fact that the extension of ALFUS is not really documented in the present paper or, at least, I did not find it. Even more misleading, in the paper the authors say that ""it would be interesting to extend the ALFUS model to human workers"" but, as mentioned here above, this extension is declared to be already worked out in [13]. Could the authors clarify on all that?


(3) quality of writing. 

*** 
My assessment on this respect is that the paper is in general well-written. In fact, most of the comments I listed above are of a conceptual nature and do not refer to the way the authors' arguments are exposed. Nonetheless, I report here below a few comments/questions concerning specific writing of paragraphs, readability of the figures, and typos that I have been able to catch.

- At the beginning of Section 4, first two paragraphs, a sentence looks duplicated up to some slight rephrasing. I suggest to remove one of them. On the other side, at the end of the introductory text, my interpretation is that the first occurrence of ""complies"" should be ""compiles"" instead (i.e, ""[...] a general procedure that <compiles> production knowledge into"").
- In the definition of the ""state variable"", Section 4.1, I don't understand the notion of ""feature"" and, in particular, I don't get what it means that a feature ""can assume or perform"" something. What is meant to be a feature in this context?
- Following the same block of definitions, it is not clear to me what is a value v_x. Assuming that, by definition, SV_i is always a tuple, in my understanding also v_x, which is a value the variable SV_i can assume, represents in fact a tuple of values. Is this correct? If yes, wouldn't be better to specify that v_x is representing a tuple of values?
- Last sentence on p. 10 contains a typo (see, ""to assess the proposed the reasoning"").
- P.11, line 15: should this be ""working-area""? Still, on p. 11, I don't think that is necessary to say that the ""Join"" function type is a specialisation of ""Join"".
- Figures 4-6-8-10 and 12 are definitely not readable in the printed version of the paper. Moreover, I ignore whether this is a (minor) problem I experience but, I've not been able to print out pages 12, 17 and 19: what happens to me is that the text under the figures on these pages simply disappear in the printed copy of the paper (I see the figure at the beginning of each page but not the text that is meant to stay below).
- P. 15, line 22: ""requiring;"" -> ""requiring:"".
- P. 15, line 23: ""associated to function"" -> ""associated to the function"" (I guess).
- P. 18, footnote: ""with an rule-based"" -> ""with a rule-based"".
- P. 18, footnote: If the authors want to keep this note, I would add a few additional words explicitly referring to the API methods of Jena, where ""OWL-DL-MEM"" and ""OWL-MEM-MICRO-RULE-INF"" are introduced. (Notice also that the URL is in a font size which is different from the one of the footnote itself). I wonder whether the authors could give some clarification about the differences between ""OWL-MEM-MICRO-RULE-INF"", whose associated OWL profile is OWL Full, and ""OWL_DL_MEM_RULE_INF"", whose associated OWL profile is OWL DL, according to the Jena documentation. I ask this especially because in the paper the authors claim that their ontological model falls within the OWL DL profile (not the ""full"" one).

 
Please also assess the data file provided by the authors under “Long-term stable URL for resources”. In particular, assess 
(A) whether the data file is well organized and in particular contains a README file which makes it easy for you to assess the data,

***
The additional materials have been stored in Dropbox. There, the reader could find the ontology modules that are introduced in this contribution and in [13] also. No README file is provided but the labelling of each file refer to one of the scenarios that are discussed in the paper (Metal, Railways, etc.) and this makes quite evident to recognise which module is for what.
It is not clear to me why in the repository there are different versions of the 'metal' module of the SOHO ontology, since there is no mention about that in the evaluation section of the paper. Honestly, I did not perform a comparison between them to identify which are the differences between the two files.

(B) whether the provided resources appear to be complete for replication of experiments, and if not, why, 
	
*** 
The repository contains the source files of the ontology modules developed by the authors to deal with the different application scenarios introduced in the paper.
This is somehow the core of what the authors wanted to present in the paper, together with the algorithm to automatically extract instantiations of this modules (see, ABoxes) and planning constraints, whose proper implementation is left to the interested reader (the article reports for its pseudo-code, as usual).
I guess that a more detailed specification of the application scenarios would also be required in order to properly replicate the experiments presented in Sec. 5.2 (""Generation of Planning Specification from Knowledge"") but, this is more a feeling I have than a certain claim by me. Anyway, I think that it would be useful to see the concrete models generated by the algorithm described in Sec. 4.2 to properly evaluate whether all the information needed is actually included in the ontology modules.

(C) whether the chosen repository, if it is not GitHub, Figshare or Zenodo, is appropriate for long-term repository discoverability, and 

*** 
Honestly, I don't think that Dropbox is the best repository to chose for long-term discoverability. In particular, anything is discoverable in Dropbox except what a user has been granted to access in advance. If this is not the case, I don't see how a potential interested user could discover and get access to material that has been provided there. Beside that, a number of web portals dedicated to exposing and sharing ontologies and ontology modules exist and I strongly recommend the authors to move (at a certain point) their artefacts in one of them. This will definitely help the discoverability and the re-use of their results. 

(4) whether the provided data artifacts are complete.

***
The ontology modules look complete to me. The only ""data"" in this case are in the form of individuals contained in the modules themselves. No other kind of data has been provided.","gpt-4o-2024-11-20, Jon Wasky",'slightly negative'
3024,"This manuscript was submitted as 'full paper' and should be reviewed along the usual dimensions for research contributions which include (1) originality, (2) significance of the results, and (3) quality of writing. Please also assess the data file provided by the authors under “Long-term stable URL for resources”. In particular, assess (A) whether the data file is well organized and in particular contains a README file which makes it easy for you to assess the data, (B) whether the provided resources appear to be complete for replication of experiments, and if not, why, (C) whether the chosen repository, if it is not GitHub, Figshare or Zenodo, is appropriate for long-term repository discoverability, and (4) whether the provided data artifacts are complete. Please refer to the <a href=""/reviewers"" target=""_blank"">reviewer instructions</a> and the <a href=""/faq"" target=""_blank"">FAQ</a> for further information.

Originality: good
Significance of the results: weak
Quality of writing: fair

The paper ""Deploying Ontology-based Reasoning in Collaborative Manufacturing"" focuess on the knowledge representation and reasoning (KRR) in the manufacturing domain of collaboration of human and robot behaviors. It extends an ontology SOHO (Sharework Ontology for Human Robot Collaboration) and introduces a ""knowledge extraction procedure"" to automate the synthesis of Artificial Intelligence plan-based controllers. The topic is very interesting and has certain novelty, but there exist multiple confusions that diminish the value of the paper. Thus, I recommend ""Major Revision"".

Motivation
The motivation of semantic model and reasoning for human-robot-collaboration is clear from the big picture, but it is not specifically clarified in this paper.
The paper claims ""Classical control processes that usually rely on static models of robot capabilities and production dynamics are  today obsolete since they do not provide robotic systems ... to support dynamic production enviornments."" 
It may be too bold to say classic control processes relying on static models are already obsolete, as I (with current industrial connection) observe the good old systems are still the main stream in the manufacturing industry, especially considering that the term ""static"" and ""dynamic"" are insufficiently explained in the paper.
The paper continues with the narrative that ""uncertainty"" is present and thus robots with ""cognitive capabilities"" are requiered, and the paper introduces the KRR technologies and concludes that ""Semantic technologies are crucial"". I totally agree with these points, but in this work the connection between semantic technologies and the ""uncertain"" and ""dynamic"" manufacturing systems are not particularly clear stated. More discussion is need on: how would the new ""perceive-reason-act"" paradigm benefit from semantic technologies, what excact changes that semantic technologies are enforcing (new robot control algorithm? system?), and what are the contrary situation without semantic technologies? What concrete challenges (more detailed than the general uncertainty and flexibility) need to be solved? These vital connections that close the last link of the storyline are missing or insufficiently clear.


Evaluation
The paper has elaborated on several industrial scenarios and give illustrative examples about how the proposed methods work in these scenarios. This is quite nice. What's unclear to me is how the knowledge representation of the scenarios with the proposed approach function as technological methods/systems/solutions, in another word, what ""tangible impact"" has the proposed approach brought onto the industrial cases, reflected by e.g., problem solving, KPI improvement, positive user feedbacks. The evaluation of the proposed approach could be improved by ontology quality verification, empirical quantitative comparison, success story telling, etc.

Presentation
The paper would enjoy the appreciation of a broader range of audience if its presentation is improved in several aspects:
Some terms are used before a clear explaination, like the aforementioned ""static model"" and ""dynamic production environments"". 
The term ""control model"" is crutial to understand the second major contribution of the paper, 
the ""knowledge reasoning mechanism that contextualize production knowledge and synthesize plan-based control models"", but it is unsufficiently explained.
Many figures (e.g., Fig. 4, Fig.6, Fig. 8, Fig. 10) have rich content but the text in them are too small. Besides, their main messages are also inadequately clarified and their contribution of the overall porpose of the paper is also not particularly clear.

In summary, the paper has overall interesting topic and content, but should be improved in various aspects before it is accepted. Thus, I recommend a ""Major Revision"".

","gpt-4o-2024-11-20, Jon Wasky",slightly negative
3029,"The article proposes an approach to build an I4.0 benchmark dataset by using KG, called I4.0 KG, to integrate data from sensors attached to machines in a production line for the manufacture of soccer balls.

As a general comment, the article lacks of a concise and clear description of the key terms characteristics of the dataset. This does not help to facilitate its usage for different purposes. No information about dataset maintenance, reported usage and known shortcomings or limitations is provided. Furthermore, the article does not demonstrate what is the advantage or the contribution of using a KG in addition to the ontological models that exist in the literature for the integration of heterogeneous data and the access to this data. I think the authors should emphasize this to make it clear to the readers. Also, the authors talk about Industry 4.0 in general but the KG they present is specific to soccer ball manufacturing. I strongly recommend the authors to dedicate a section to discuss the possibilities of adapting their approach and the proposed KG to be used in other manufacturing activities.


Here are some more specific comments:

The title of the article is not clear. I don't understand why KG appears at the end.

In the abstract:
""Our research helps the stakeholders to take timely decisions by exploting the information embedded in the KG."" - I do not see this verified in the article.

In the introduction:
Mass production was achieved in previous industrial revolutions, not in I4.0. 
I4.0 is more about the use of various technologies and also the use of artificial intelligence to make better use of resources to optimize production.

The paragraph between lines 12 and 24 in page 2 is not clear. Maybe, it would be better to give the necessary definitions and then describe the differences and how they complement each other or how they are linked. Ontology and KG are not defined, please add the definitions here with the corresponding references.

The authors enumerate the contributions of the article, however I do not think they are all contributions. For example, the last one is the validation of RGOM, a model that it is not sufficiently described in the article. I do not think the validation of this model is a contribution of this article. It is more to prove that the KG built from RGOM is in fact a contribution.

In the related work:
""... Internet of Things, Internet of Services, Cyber-Physical Systems, Digital twins, ..."" are not technologies. Please rephrase.

The state of the art is difficult to understand. I think the authors should work on this section and make clear the link between the existing works and what their limitations are so that it is clear how the proposed model addresses those limitations and to what extent.

In section 3:
""... acquisition and generation of the dataset."" This phrase is not clear for me. Maybe, data acquisition and dataset construction ?

There is no transition between the first paragraphs of this section with subsections 3.1 ... 3.9. At the beginning two types of attributes, static and variable, are mentioned and then it goes on to describe each machine without transition. Adding a transition here would make the text more readable and aid understanding.

Regarding the random creation of the variable attribute values, the authors give a reference [17] that explains how these values are generated, however I think it would be worth to give more details about this in the article to see how accurate these values are. Also, it would be interesting to know if in the creation of these values the relationship that exists between certain properties (for example, the temperatures of the different components of a machine) is taken into account. Is it possible to represent these relationships (perhaps physicals) in the proposed KG? I think this is a key point and would allow to further enrich the data of the model.


In section 4:
""IT silos"" is a very specific term, maybe give the definition or use another term like data storage.

""... usability of this data for, e.g., subsequent analyses and reasoning."" - I do not understand this phrase.

Linked Open Data was never mentioned in the article before. Maybe briefly explain what it is about.

""This goal can be ..."" - Which goal?

""The following describes the steps ..."" -> Maybe the layers and the interaction of the different components instead of ""the steps"".

""... at a certain timestamps ..."" -> at different timestamps.

""... unnconnected data"" - What does it mean? I think it is a very general statement, it would be better to be more specific and for the authors to make it clear what they mean by ""unconnected data"".

""... interaction of production staff with unconnected data is very difficult."" - I do not understand this phrase. It refers to access to information, interpretation, ... ?

The authors state that the RGOM model is inspired by the standards adopted by RAMI4.0. RGOM is also based on the model proposed in ""Giustozzi, F.; Saunier, J.; Zanni-Merk, C. Context modeling for industry 4.0: An ontology-based proposal. Procedia Comput. Sci. 2018, 126, 675–684"" and reuses other ontologies such as the Time ontology, SSN, among others. The reference [7] cites these models. The construction of the KG is not well described, it is difficult to see the link between the KG and these ontological models and how the KG is constructed from these modelsand the data. This is linked to my general comment that it is not easy to see what is the contribution of the KG with respect to using these ontological models.
I think this whole section should be rewritten and restructured to make it clear how the KG is constructed and why it is useful and necessary.

In section 5:
It is difficult to see the adaptability of RGOM through this example. The use case is very simple and does not demonstrate the usefulness of the KG. I think that the queries are too simple, maybe think about adding more complex queries that allow to see the real usefulness of the KG and the advantages it offers in terms of integration of heterogeneous data and with different temporal resolutions. 

In the query of listing 3 it would be helpful to give more details about the status of the engine of a machine. I do not see the concept Status in the model, maybe it should be described how this status is obtained or what it represents (for example, if it can be seen as an abnormal behavior). This would show that the KG offers this kind of semantic information that could be exploited by an operator and even obtain more information associated with this abnormal behavior to determine its causes.
Furthermore, maybe add third-party uses to provide evidence of the usefulness of the KG dataset. For example, a possible application (and not just say methods and tools) that can make use of KG to demonstrate its usefulness. For example, how KG could help to create suitables datasets to build machine learning models for predictive maintenance.

Can this KG be adapted to another case that is not the manufacture of soccer balls? I think that the authors could make a discussion about this and give some hints on how to do it beyond that they do not fully validate this adaptability.","gpt-4o-2024-11-20, Jon Wasky",'strongly negative'
3029,"In this “Dataset Description” article, the authors evaluate their previous work on the Reference Generalized Ontological Model with a use case dataset on production line processes in the football manufacturing industry.

In the introduction, the authors identify a lack of openly available knowledge graphs to be used in use cases for Industry 4.0. However, it should be clear that they are specifically targeting open datasets for production line processes – as Industry 4.0 is a very broad term, and for other industries than production lines there might exist datasets which are neither toy examples nor closed-access.

The authors mention this benchmark dataset allows to verify the adaptability of their RGOM data patterns. Although this article is a dataset description that focuses on the creation and availability of a particular KG, I think it is nevertheless beneficial to situate this in the larger picture: as the authors mention, one of the goals is to demonstrate/verify the RGOM framework’s adaptability. I think this is difficult with only a single dataset – in my opinion, flexibility can only be demonstrated by applying multiple datasets which contain significant differences. Also, it will help the reader to understand the value of the paper if it were made more explicit how to re-use the dataset in future use cases. In the conclusion, it is mentioned that RGOM and the underlying dataset will help the research community to validate their tools and techniques. However, it seems that the dataset will be more useful as an example for industry on how to set up a Linked Data-based production line, rather than for research purposes? As indicated in the submission type guidelines, it is recommended to illustrate adoption of the dataset by third parties.

The description of the dataset itself (parameters as mentioned in the guidelines for submission types, “Dataset Description”) is insufficient. Please check again the Journal requirements for a “Dataset Description” paper. E.g. the URL in footnote 2 leads to the Github repository that contains the code to generate the dataset. Although a URL to a (inaccessible) Google Drive file is given in the repository Readme, a dereferenceable URL to find and/or query the dataset itself is not given directly in the paper.
The RGOM framework itself is introduced in Section 4.2. However, I think this should happen earlier in the text, as it seems to be a core concept of the paper, and a main reason to write this contribution. Maybe it can be introduced briefly in Section 2: Related Work. Doing this will help the reader situate the work in the sections to come.

In Fig. 2, confusion may arise as there are some graph nodes which seem to be Literals – probably because of the use of quotation marks (e.g., “cutting TPU roll process number 51”). It appears the authors mean to identify IRI concepts in a more ‘human-readable’ way, but they should make sure to avoid this confusion – RDF Literal values cannot have outgoing edges.

When introducing the ontological structure of RGOM (Section 4.2.1), it is not clear in Fig. 3 whether the authors define new concepts or reuse existing RDF vocabularies. This becomes clear in the SPARQL examples of Section 5, but should already be mentioned when introducing the ontologies.

Some last notes: 
- A reference worth mentioning for mapping the non-RDF dataset to RDF (Section 4.2.2) is R2RML (or RML).
- In section 5: is it relevant to mention the used computer system when no performance benchmark is included in the paper?
- Also please check if all abbreviations are introduced fully on their first occurrence in the text (e.g. CPS, SIB) and check table 1 for consequent usage of punctuation marks in table cells.
- Finally, I strongly suggest to perform a profound spelling/grammatical check and correct existing typo’s in the text.
","gpt-4o-2024-11-20, Jon Wasky",'slightly negative'
3029,"The contribution of this paper is potentially significant. The lack of an open dataset for Industry 4.0 research is known and this article provides ontology and KG datasets. The paper is acceptable in general. One suggestion is that it will be clearer if the authors address how this football production case is actually an Industry 4.0 case instead of listing the sequence of the production process. I suggest utilizing Figure 1 to address this. And multiple typos and consistency (e.g., capitalization are noticed. I suggest reading and updating the manuscript thoroughly.  ","gpt-4o-2024-11-20, Jon Wasky",neutral
3034,"The author has submitted a thoroughly revised version of the original article, which all the minor issues I had pointed out. 

In particular, the description of the Sampo design principles has been improved considerably by adding futher explanations and by changing the order of presentation. More detail is given on the technology generations and it becomes much clearer how they relate to the design principles. Finally, the SemanticComputing Github, which provides the long-term stable link to the resources, has been updated as asked for to meet the journal's Open Science Data policy. 

I recommend the manuscript for publication.","gpt-4o-2024-11-20, Jon Wasky",'extremely positive'
3034,This is a joint review of Kai Eckert and Benjamin Schnabel. Benjamin Schnabel is a PhD student in the field of Digital Humanities (Jewish Studies). This revised version of the paper has been improved a lot over the previous version. All considerations have been taken into account. We therefore would recommend to accept the paper for publication.,"gpt-4o-2024-11-20, Jon Wasky",'slightly positive'
3037,"The paper provides an original and credible approach to the definition and population of a ontology for materials. The results of this approach would be of great benefit for the materials community since it would prepare data semantically facilitating further computational investigations. The implementation details in sections 5 (extension) and 6 (database access) clearly describe the power and limits of the approach.

In general, the paper is well written and able to effectively deliver the details of the approach. The suggestion is to accept the paper for publication.

Here is a couple of minor polishing suggestions and comments, leaving to the authors to decision to use them to make minor text changes/additions:

- page 1, lines 41-43: provide a better explanation about the role of material science in env-friendly energy technologies (a couple of sentences should be enough)

- page 5: really appreciated the usage of OWL2 DL to enable a full exploitation of semantic constraints provided by OWL, that would enable the usage of expressivity at ontology level (while usually such approaches are kept at taxonomical or thesaurus level), enabling the exploitation of existing Top Level Ontologies (TLO) (strongly promoted by the OntoCommons project, mentioned in the paper). However... (see next point)

- page 9-11: as it is now, the connection to the TLO is quite useless since its main concepts are not really used to implement the MDO modules. Unless a stronger commitment to the TLO classes and relations in the MDO core ontology the connection is only a conceptual orientation more than an architectural choice. However, the approach used here is already a relevant step forward and does not prevent future alignments that will provide stronger semantic extension of the MDO thanks to a stronger commitment to the EMMO TLO.

Data are easily accessed and well organized, and primarily based on a GitHub repository.","gpt-4o-2024-11-20, Jon Wasky",extremely positive
3037,"(1) originality: The paper provides and important and original contribution to the application of semantic web technologies in the field of materials science. It is very timely since there is a rapid development of databases and resources that could be better harnessed and harvested with ontologies, there is a strong interest in the field in using ontologies for such purposes but still a lack of semantic resources and ontologies available. The approach employed is very methodological and will also serve as s show-case for ontology development to others in the field. The ontology is well described in the paper, provided in github and all entities in the owl file are 'defined' via comments as is appropriate. As mentioned above, the Long-term stable URL included the complete (modular) ontologies  together with Readme etc

Extension of the ontology based on analysing a corpus consisting of journal databases is presentation and discussed in excellent detail. This will be a key step in overcoming current scope limitation of ontologies in the field. 
The application to materials databases, enabling integration of heterogeneous resources is also presented with detailed results, and includes the required GraphQL server setup details, as well as queries for reproducability.
Overall the results are highly significant and expemplary for the field in many aspects as discussed. The paper is well written and clear.
One minor criticism regards the discussion and use of top level ontologies. In section 2.1 it states that:
EMMO is ""a standard representational ontology framework based on knowledge of materials modeling and characterization."" That is not correct. As outlined in https://github.com/emmo-repo/EMMO, EMMO is based on physics and analytical philosophy, in particular mereotopology and semiotics. 
Given that DOLCE Top Level Ontology is then referred to in line 6 on page 3, it should be discussed also with the BFO and EMMO in the previous paragraph. It cannot be said, as seems to be implied now, that EMMO and BFO are somehow particularly 'materials' related top level ontologies.
Given the discussion of ontologies developed within TLO contexts and those indepedent of TLO context, it is not clear why MDO decided to develop outside of a TLO.Furthermore, it is not clear what the purpose of using a single concept (Material) from EMMO is in the context of MDO, in particular since 'atom', i.e. a type of 'material' is aligned with CHEBI (why not also with EMMO?). Strictly, this creates the issue that Material is a 4D (space-time) entity (due to EMMO being 4D) and atom is a 3D (space) entity. It would be better to keep the concepts without reference to other ontologies at this stage and leave any alignments to the future work mentioned in the conclusions.

 



","gpt-4o-2024-11-20, Jon Wasky",'extremely positive'
3037,"In this manuscript the Authors present the Materials Design Ontology (MDO), a domain ontology to support interoperability of databases for solid-state calculations.

Next, they address the MDO extension, applying a method (and a tool) that extracts candidates for additional concepts from a corpus of journal abstracts and enables domain experts to evaluate them. Finally, they present a proof-of-concept system using MDO to query multiple databases from the OPTIMADE initiative and evaluate its performance in comparison with other two systems.

The manuscript builds on previous work from the Authors, that is extended here; both the ontology and the information to set up the server are publicly available (https://w3id.org/mdo/full/1.0/ and https://github.com/LiUSemWeb/OBG-gen). 

The topic is timely, the manuscript is well written and rich of details to support reproducibility, therefore I do recommend it for publication in the Semantic Web journal.

Below I give some questions and comments I would like the Authors to consider in their revised version before publication.

-----------------------------------------------------------------------------------------------

Questions/Comments

1) Some ontologies for materials are listed in the manuscript (Table 1): how have these ontologies been found/selected? It would be useful to add a brief comment about this in the text, especially if they were found systematically. 

2) The ""xsd:string"" type is used also for cases that could be further specified, for example as URI (e.g., DOI and URL properties of ReferenceAgent in the Provenance module). Please add a comment about this choice.

3) In MDO, ""Physical property"" and ""calculated property"" are disjoint, it seems ""Physical"" stays for ""measured"" (experimentally). Note that other authors take the view that certain properties can be ""computed/calculated"" or ""measured"", but they are anyway in both cases
""physical"" quantities, as opposed, for example, to numerical parameters that do not affect the physics of the system. A note could be added to point out what is meant with ""physical"" in MDO.

4) On competency questions and restrictions.
4a) CQ11 vs CQ13? Does CQ13 refer to software that is compatible with the one used to obtain the results? Please clarify. 
4b) AR2 vs AR6? Does this exclude the possibility of using a combination of computational methods for materials? Please clarify. 

5) Table 3: It is not clear how the ""Original TopMine"" and ""TopMine without stemming"" methods (columns two and three) differ from each other. Please explain it briefly in the text and/or point to a reference for details.

6) The concept of ""High-quality frequent phrases"" could be clarified better and sooner. It is explained with an example, but I think a concise sentence could be added beforehand. For example, saying that a phrase that is also part of (contained in) a frequent one will not be counted.

7) Is the ontology extension tool (snapshots in Figs 2,3,4,5) available, or is there a plan to make it available? In case, please add a comment in the manuscript.

8) Current and future scope and connections
8a) As clarified by UC1, ""materials"" and ""materials calculations"" in MDO are intended for solids, in the context of solid-state physics and condensed-matter theory. A note on this could be added in the introduction and conclusions, to stress the focus of MDO also there. If, on the contrary, there is a plan for MDO to be extended to address also other phases and areas of physics/modelling, this should be said.

8b) For future work, in line with the mentioned OntoCommons demonstrator where connections to other ontologies will be explored, I would suggest, for the ""Calculation"" module, looking into recent and ongoing work within VIMMP Ontologies, in particular VISO-electronic (for definition of methods, parameters etc) [https://gitlab.com/vimmp-semantics/vimmp-ontologies/-/blob/master/viso/viso-electronic.ttl]. Similarly, another (work-in-progress) relevant ontology is EMMO-CIF [https://github.com/emmo-repo/CIF-ontology].

8c) Sec 3.4: The current connection to EMMO and CHEBI is only via two individual concepts. A comment could be added on the reason for these choices (e.g., since EMMO is a TLO, more concepts can probably be reused, but I am aware that EMMO is still being actively developed and it could make sense to wait for it to be stable before drawing more connections).

Minor points:

9) ""Topic model"" approach and ""Latent Dirichlet Allocation"": please add references.

10) Please add a sentence (and a reference) about Yago. Add a reference for MatML.

11) Unless they are universally known, please expand acronyms in the main text when they first appear (e.g., OQMD = Open Quantum Materials Database, NOMAD=Novel Materials Discovery, etc).

12) Introduction: EMMO name, please update ""Elemental"" -> ""Elementary"" 

13) Page 5, line 51: Additional Restrictions -> please add ""(AR)"", since it is used afterwards 

14) ""ODGSG"" is used in Fig 19, 20 and not defined. Obviously it refers to the OBG-gen server, but the acronym should be introduced in the text.

15) Section 5: ""The 37 concepts of MDO were used as search phrases ..."". Was the search run on titles AND abstracts? I guess so, but please clarify for reproducibility.

16) Typo: ""has exact one composition"" -> has exactly ...

17) Patterns: In Sec. 3 it is first said ""we did not use existing ontology design patterns (scenario 7), as the only one we are aware of in the materials science field is about materials
transformation [42] that is not covered by MDO."" then later on in the same section it is said
""We identified a pattern related to provenance information in the repository of Ontology Design Patterns (ODPs) that could be reused or re-engineered for MDO."" Please clarify. 

18) ""TopMine generates frequent phrases"": I understand it is meant to point out the ""output"" of the code, but ""identifies"" seems more appropriate.

19) Page 16: ""generated topics"": Please add a brief sentence to say what a ""topic"" is in this context (e.g, as a clusters of phrases).

-----------------------------------------------------------------------------------------------

Note: In Long-term Stable Link to Resources, the Authors provided this link: https://github.com/LiUSemWeb/Materials-Design-Ontology
Probably also this other link should be given https://github.com/LiUSemWeb/OBG-gen
","gpt-4o-2024-11-20, Jon Wasky",'slightly positive'
3043,"The paper presents an ontology to support automated manipulation of objects in a 3D environment while taking advantage of virtual applications. The topic is relevant and fitting to the special issue.
Since the manuscript is a ""full paper"", it is reviewed along the following dimensions.

(1) originality

The authors have paid attention to the existing literature (even though some clusters of authors can be identified), but it's hard to appreciate the actual novel contributions.
The introduction is lacking focus and missing to provide key motivations for the proposed ontology, in particular from an industrial perspective.

The need for a new ontology for geometric description is not properly motivated because the problem has already been addressed in the literature. STEP is mentioned, but it's not clear to what extent it was used. 
IFC (and its ontology version ifcOWL) covers most of what is discussed about geometry and topology. In addition, GeoSPARQL and Well-Known Text (WKT) are potentially relevant and should be assessed, since they provide also mechanisms for automatic calculations. 

(2) significance of the results

It is not clear why the ""automated extracted taxonomy of AP203 is meaningless as STEP standards..."".

The specifications (Section 3) could be better developed, because much space is dedicated to a specific little example. Competency Questions should be better explained. It would be beneficial to specify also requirements, goals, constraints, etc. The initial part of Section 4 would probably better fit Section 3.

If more than one ontology is used, then the addition of prefixes would help to avoid ambiguity. 

The authors are not addressing the problem of generating the large set of data that are needed to instantiate a 3D environment and run queries. Which would be a feasible workflow? Are you relying on CAD models? Point clouds?
Moreover, basic mathematical concepts (e.g., Vector3D, RotationMatrix3D, etc.) are defined in a verbose way that will pose challenges for the generation and querying of data.

The definition of Hole, Opening, and Container as context-dependent is quite surprising and not well motivated.

The presented simulation scenarios (Section 5.1) are very simple and have little relevance from an industrial perspective. In addition, the authors don't explain how the scenarios were instantiated, therefore it is assumed that this task was carried out manually, making it not scalable.  Figures like Figg.11 and 15 are little informative and could be replaced by a table and complete online resources.

Which are alternative technologies that could help to answer the competency questions defined in Table 10 and Table 8? Which is the actual advantage of using SPARQL queries?

SPARQL queries in Figg. 14 and 18 are customized to the specific scenario. Instead, general purpose queries would be expected as reusable contributions, receiving as input only the URI of the solid body or place to be analyzed.

Overall, the paper is focused on the geometry and topology data, so it is difficult to appreciate the advantage of using an ontology to integrate also other knowledge domains, since the contextual part is only marginally addressed.

Also, the conclusions (Section 6) fail to highlight the significance of results. The added value of aligning the proposed ontology with a top-level ontology is not clear in the scope of the paper.

(3) quality of writing

The use of English must be improved with a proper proofreading. There are typos and sentences that need to be rephrased.
e.g., ""to across"" is not a verb (Table 1).

Table 8 is referenced in page 12, but it is not included nearby. There is a Table 8 in page 21, but its content is not consistent. 


(A) The repository contains a README file, but it is basically empty, so the content is poorly documented. 


(B) it is not possible to identify the implementation of the two simulation scenarios. Moreover, the repository doesn't include relevant SPARQL queries. Therefore, the provided resources don't help to replicate the experiments.

(C) The authors provided the link to a GitHub repository.

(4) The provided data artifacts are not complete, as commented above.","gpt-4o-2024-11-20, Jon Wasky",'slightly negative'
3043,"The authors present a knowledge model for semantic maps that takes into account, and links, items of knowledge from geometric, topological, and ""context"" (ie application dependent) levels. 

I think the model is well presented and interesting, and I am keen to use it or its future developments at some point. I would only recommend minor revisions at this time.

A couple of more substantive points are below:

Context independent semantics (pgs. 12, 13): Environment Complexity and Congestion seem to be qualities that can vary with time. E.g., a room may be crowded during some hours but empty later, and a corridor that is passable now becomes impassable once some items are dropped there. However, the knowledge engineering techniques mentioned do not account for time variation. A little more discussion would help clarify matters here, e.g. by stipulating that the ontology is to be used for ""sufficiently short"" simulations that certain qualities can be thought of as time invariant. [I notice the Conclusion section discusses the fact that the ontology is at least so far intended to capture snapshots of the world, not the ongoing process of updating, but perhaps this should also be discussed previously]

Table 7, FunctionalObject definition, pg. 13: requiring a functional object to have exactly 1 function seems rather restrictive. In the industrial domain this seems like less of a limitation since tools and machinery seem to have one designated use and the users are strongly encouraged not to deviate from it, or outright forbidden from doing so. However the household domain is quite prone to item repurposing. As an example, an oven can be used to bake, but it can also be used for storage; a knife is typically used to cut, but it can also spread or collect butter. It doesn't seem though that ENVOn restricts its application domain to industry, so perhaps a relaxation of the ""exactly 1"" constraint may be in order.

The next items are mostly related to aspects of presentation.

State of the art, pg. 4: I cannot understand this sentence: ""Cailhol et al. [11] define a place as a topological graph connecting places borders built on octree decomposition."" Perhaps you could rephrase it?

Table 1, pg. 5 (note, there is another Table 1 on page 7): CQ1 and CQ8 appear to be the same question. CQ14: perhaps rephrase the text ""least complex to across"" -- do you mean, easiest to go across?

Figure 1, pg. 6: the hasGeometricModel arrows are not consistently oriented

Definition of AffineTransformationMatrix3D: as far as I know, 3D affine transformation matrices have (0 0 0 1) as their last row; relaxing this constraint allows projective transformations also (making these projective transformation matrices)

Definition of Axis3D, pg. 9: it is not clear from the definition given in table 3 that this is a subclass of Line3D

Definition of OrientedBoundingBox, pg. 8 (in Table 3) and pg. 9: it is not clear that the minimum and maximum points are Point3D, what is asserted is they are AxisPlacement3D (which also relates to a point, but may be overkill for specifying the boundaries of the box once the local coordinate system is also known)

Definition of topological graph elements, pgs. 11, 12: I would need some hand holding here to understand how, e.g., two rooms, the corridor between them, and the doors from the rooms into the corridor, become a topological graph. This is because intuitively it should be the borders that are associated to edges (since borders seem to be defined between two places) and the places themselves as vertices in a graph. Nonetheless, the definition in the paper is opposite: the nodes are borders (so can more places than 2 be incident on a border?), and an edge passes through a place: does this mean the edge describes a way to get into the place connected by passing, or out of it? [the figures given later for the evaluation scenarios don't help, I'd really like to have the graph drawn somewhere, with labelled vertices and edges]","gpt-4o-2024-11-20, Jon Wasky",'slightly positive'
3043,"The paper aims at proposing a new ontology, called ENVOn, to represent 3D virtual scenes and to support simulated manipulation task. The use of such ontology can be leveraged to serve a robotic system while planning motions or tasks. The paper provides a rather wide analysis of the state of the art considering works related to semantic information of environments and ontology for robotics. It is argued that past works focus on either domain specific applications or formal geometric descriptions. Also, distinct works have been presented considering topological or semantic map knowledge. Authors underscored that a comprehensive approach seems to be missing and a context-based knowledge structure should be considered. 
This is the aim of ENVOn with a three-tier knowledge architecture (i.e., geometric, topological and contextual layers) and inter-layers relations/constraints to maintain coherence. The paper provides a detailed presentation for layers and for concepts in each layer. At the end, it proposes two scenarios to validate and verify the ontology. 

The general approach pursued in the paper seems interesting and significant.
Even though the work relies on previous works, the proposed approach seems to me original. 

A first concern is related to the lack of strong motivations about the need of such new ontology in a robotic context. In fact, while I see a clear benefit in using ENVOn to represent virtual scenarios, I would expect to see a wider discussion (and relevant motivations) about the actual need for the ENVOn definition in robotics (e.g., connections with a motion planning module or a task planning system). As it is, the paper does not provide such strong connections with robotic applications.

The most appreciable point of the paper is represented by the fact of considering multiple layers. Considering different abstraction layers with suitable relations/constraints to maintain a valid general representation seems to me a really valuable point. In fact, this allows to provide richer representations of a same scenario and therefore serving different kind of queries. Indeed, ENVOn is shown to be able supporting reasoning services at different abstraction levels and this seems a clear advantage with respect to the state of the art.

Also, the simulation scenarios demonstrate the effectiveness of ENVOn in representing simulated manipulation examples with a set of competencies queries addressed through the defined representations. In general, this sounds as a good contribution.

In the final discussion, authors pointed out also some limitations (the most obvious one related to computation latency). In this regard, I would appreciate whether the authors also provide some insight about possible solutions to guarantee an easy deployment and sustainable operations in concrete scenarios. Again, I would expect to see more strong connections with robotic-related future perspectives.

Finally, I think the presentation should be improved. In some parts of the paper the text format is somehow strange. Often, there are large white spaces (e.g., pag 1 column 2, pag 4 column 2, pag 7 9 16 at the bottom) and in some cases text is not well formatted (e.g., pag 14 column 2, pag 18 column 1 and 2). Also some typos (pag 1 column 2 ""an geometrical"", pag 12 column 2 ""For example, The construction"", pag 22 ""[53]]"") or errors (pag 16) are recurring here and there. This may be due to the paper template (I guess authors are using a word template) but, in any case, a careful revision seems to be required. Also, Figure 11 is not so readable, Figures 14 and 18 are composed by rather low quality images. In general, authors should check that figures have a good resolution (esp. for 3D pictures). 

Moreover, I would suggest to consider a small example (or one of the considered simulated scenarios) as a motivating use case. So, I would consider to present such case at the beginning of the paper and using it as a motivating and running example throughout the paper. It could be used in section 3 to motivate why state of the art approaches are not enough, e.g., to support questions in Tab. 1. Then, it can be used to support the presentation of layers, concepts and constraints (in sections 4 and 5). In my opinion, this would make the paper easier to follow.

The resources provided with the paper (in a github repository) are well organized though they should be better described (as far as I can see, the README file is empty). More information should be also provided to better support utilization of KB for queries replication.

In general, the paper seems to me worth to be considered for publication but some more work is needed to improve its overall quality. More clear robotic related motivations should be provided, presentation should be enhanced and additional resources description should be improved.

For the above reasons, I would suggest to consider a major revision of the paper before considering it for actual publication. ","gpt-4o-2024-11-20, Jon Wasky",slightly positive
3049,"This submission proposes a new approach for explaining the results of a black-box classifier,
by providing a user with a query, or a rule, which describes the basic characteristics of the
resulting class. The approach is based on known techniques from the DL community: computing the
most specific query (that is, the query that most tightly describes a given individual from a 
dataset) and the least common subsumer (the tightest query that generalises other two). In addition
to the technical details of the approach, the authors provide an empirical analysis bases on three
experiments, which provides evidence for the usefulness of the approach to explain the results of
the classifier.

In general, I believe that the submission covers a very important and relevant topic in the modern
era of AI, where most current methods are based on deep learning methods, but explainability of
answers is not only desirable, but also begins to be required by law in some regions. The kinds of
explanations provided by the queries generated through this approach require some technical 
understanding, but are relatively easy to grasp by a trained user, as long as the number of 
conjuncts is not exceedingly large. Under this view, I believe that this work is worth exploring.

Still, I believe that there are some important points that need to be improved or clarified before
it can be accepted for publication.

My main concern is with the motivation of the paper, and how it applies to the developed formalism.
The authors argue that semantic labelings should be used instead of instance features to allow for
understandable explanations, given that the names have a meaning to users reading the explanation.
In the original example, the authors propose that the input of a diagnose is a recording of a cough,
which for training has been semantically labeled by an expert (as ""dry cough"", ""loose cough"", etc).
These labels characterise the recordings, and are used for the explanation. I see two obvious 
problems with this (which are actually connected):
1. the interest of explaining black-box classifiers is not in understanding the train or test 
   instances, but rather in understanding a new instance given. For those, the semantic annotations
   are not available---otherwise, we would not need the black-box classifier to begin with
2. there is no guarantee of any kind that the black-box classifier is using the ""semantics"" given
   by the annotations. Indeed, it could be that for the classifier other factors such as the 
   length of the recording, the frequency of cough, or its strength are more important. We would then
   be explaining through irrelevant labels (and hence, mislead the users).
The second point is particularly important in the context of explaining black-box classifiers, since
that is precisely the issue with these kinds of classifiers: the reasons why they select a class 
over another is not accessible. This well-known issue was already immortalised in science fiction
from the late 90s.

Another issue is with the classifier, and the rules extracted. As the authors observe, it is 
difficult to find correct rules for a classification, and hence one would like to find some with
exceptions. However, it is usually these exception that are interesting to explain (the difficult
cases to understand, not the simple instances). For these, there is no notion of explanation, except
about being exceptions to a rule. Very important details, such as bias, could be hidden in those
exceptions that cannot be understood. It would be good if the authors could discuss this issue.

Another less problematic, but still relevant issue regards presentation. I have recently observed
this trend where at first sight, papers seem more general than they actually are, and important
restrictions are hidden in some sentence in the middle of the paper. Although this should formally
not be an issue (a paper should be read in detail), in reality can cause confusions to superficial
readings. In this case, for example, the authors first define the general notion of a conjunctive
query, then (in line 47 or page 3) say that ""query"" stands for ""conjunctive query"", and a few lines
later (line 2 of page 4) say that ""query"" stands for ""instance query"". Someone who misses that 
last passage can easily think that it can handle conjunctive queries in general. Another case is
at the end of page 4: after defining rules in general, it is assumed that all rules are ""connected"".
This assumption is mentioned there, and nowhere else, but used throughout the paper. 

There are a few minor comments in addition to these, but at this point I believe that the larger
comments above justify a major revision of the submission.","gpt-4o-2024-11-20, Jon Wasky",'slightly negative'
3049,"It is a very, very interesting paper. Yet, I have the impression that it promises a bit more than what it delivers. Predominantly from two aspects:
-	No external knowledge graphs are used; a knowledge base is built based on the exemplary datasets, using 'features' extracted from it; it looks (at least based on the examples) that a knowledge bases are built and are fully related to datasets, right?
-	The examples are a bit disappointing; they should provide some 'exact' results so a reader can derive her own opinion regarding the method's benefits (a bit more below).
There is also the impression of over-complication in some places. And, the paper is long and not the easiest to follow. Understandably, you need to provide a number of concepts and notations – but it is a bit overwhelming – the idea behind your approach is somehow difficult to 'extract' (sorry, it was my impression). So, any simplification of the paper would provide the reader with an easier task to understand it and appreciate it.
Still, I like the paper and many aspects of it very much and would like to see its simplified and more 'goal-oriented' version.

Some comments:
-	you use the term ‘semantic query’ – any particular reason for that?
-	what do you mean by “certain answers” – are these definite ones? Do you even need to mention that? Could some answer be uncertain?
-	You use the term Exemplar as a concept and exemplar as datapoint (interpretation); if I could suggest something (because the only difference is a different font and a capital first letter) to change exemplar as datapoint into ‘exemplary datapoint’ – I think it would improve comprehension/readability of the paper
-	Page 4, line 14: le should be ExE ->2 …
-	Page 4, line 24: you use the term ‘substitution’ – a few words of explanation, please
-	Page 4, line 49: you have ‘D(x)’ – what D stands for? I assume it is different from C(x); does it mean class/category of a classification problem (sorry, I could not find any explanation in the text)
-	Page 6, Fig 1: it is a nice figure but could include a bit more details that explain the approach, or you could include a figure representing a ‘flow’ of the process (just a suggestion)
-	Page 7, line 9: is Exemplar a subset of EN? (the first part of the formula); in general, it would be nice to see a bit more explanation of this important formula
-	Page 7, Example 1 (and following up): I really like the idea of ‘running example’ that illustrates your proposed approach, yet, not everything is very clear – for example, s1, …, s8 as a part of IN – are these symptoms? Also,  it would be nice to mention how the explanation rules (top of page 8) were created (refer to Section 4, its particular parts)
-	Page 8, lines 18-20: it seems to me that it is the essence of your approach – to make the paper more understandable for a reader, it would be nice to define what is a query reverse engineering problem in this particular context 
-	Page 10, line 29 (and page 11, line 48): TBox eliminated? Could you please provide a short explanation?
-	Page 12, line 46 (Section 4.2) is it easier to determine ‘dissimilarity’ than ‘similarity’? any comment here?

Section 5: Experiments
As much as I enjoy three different datasets used as case studies, I am disappointed you have not provided examples of generated rules/explanations. It would be very interesting to see how we (readers) can ‘evaluate’ these explanations. Among the three datasets, the first two seem (at least for me) quite suitable for showing how classifiers work. For example, I could suggest putting some samples of classified (correctly/or many not) data points and explaining why this happened. The third dataset – MNIST – is very popular, as you indicated, but somehow less attractive for the topic of your paper.
","gpt-4o-2024-11-20, Jon Wasky",slightly negative
3052,"In this paper, the authors describe an approach (in 2 variants) to extend a well established chemistry ontology, Chebi, with new classes (more precisely, suggestions for new, atomic ’SubClassOf’ axioms) for un-seen chemicals from their SMILES strings. The approach uses transformers/deep learning models and domain-specific embeddings of the SMILES strings and is trained on the existing Chebi ontology (where the SMILES strings are captured as annotations of classes). The results are promising: on existing Chebi classes, they achieve good F1 scores and outperform the authors’ previous approach, and on those not yet covered in Chebi they look good but exact evaluation is part of future work. The new approach has the potential to be transparent: considering areas of attention of the model on (positive?) classifications can indicate reasons for the classifications. 

The paper reads well and the results are interesting, though I think the presentation can and should be made more clear by addressing the following points. Also some of the claims about this new approach should be made a little more carefully to fit the evidence gathered so far:  

- Explanations of diagrams and plots need to be clearer: all axes and colours used need to be explained, ideally in the caption (eg Fig 7, it’s unclear what the x-axis is and what the colours mean (perhaps the ‘blue/red’ for Fig 7(c) is also used for (a) and (b) but this can be made clearer. Eg Fig 8, what is enumerated on x-axis (this becomes sort of clear in the text but should also be clear from caption)). Also, please make sure that numbers on axes are readable (even with quite a lot of zooming in, this isn’t the case in Figure 2 - and it’s also lacking labels)

- Some of the claims made are not strongly  supported by the evidence provided in the paper: the interpretability/explainability is discussed by an interesting example, but a suitable evaluation is left for future work. Furthermore, it seems that explanations will only be available for positive classification: what would one do for false negatives? Similarly, the current approach addresses ontology learning in a very weak form as it is restricted to learning of atomic subclass-relationships. While the results are interesting, one could also call this ‘class localisation’ or ‘class insertion’. 

- Throughout the paper, the nature of the (structured) annotation used should be made more clear: it took me a while to realise that the SMILES strings were used (and without further statements around them) since the annotations used are described in quite a few different ways first. 

More detailed comments and suggestions: 

Page 2 
- line 5: rephrase ‘Cheb tries to’ to ‘Chebi engineers try to ‘ or such like 
- Line 12: is there a reference for Chebi’s workflow? Also ""navigating the ontology scaling dilemma “: is ‘navigating' really what you mean here? 
- Line 13: I don’t understand “ design decisions [..] analogously to new classes and relations, “ 

Page 3: 
- perhaps also explain what the *target* of these ontology extension approaches are (are they all aimed at atomic SubClassOf axioms?)
- Would the following be clearer? ""Given the *documented, structured* design decisions by the ontology developers, how would they extend their ontology to cover a novel entity? “
- Line 39: ""within these structures *whose* sub-graphs may themselves”? 
- Perhaps move the explanation of SMILES to an earlier point, eg a small section on ‘background on Chebi’? 

Page 5 
- line 16: can you be more precise on ""and the system as a whole was not explainable.” 
- Line 31: ""based on the design decisions that are implicitly reflected in the structure of ChEBI. ” one of the places that confused me (see above): isn’t your approach rather focussed on the (structured) annotation documenting/reflecting these design decisions? 

Page 6 
- line 34: ""One of these successful architectures is RoBERTa [44], *whose* architecture offers a learning paradigm “ 
- Line 33: here and later in the evaluation, it would be interesting to know this distribution of ‘several’ (direct) superclasses: how many classes have 1 superclass? How many 2?…
- ""with a plausible real-life dataset of chemicals” isn’t it that the related use case is realistic (rather than the dataset ‘real-life’)? 

Page 7: line 25  could you briefly sketch the algorithm used and/or explain the ‘class merging’ step (perhaps using an example)? 

Section 4.1: given that we’re looking at multi-label classification could you please briefly explain precision/recall: do we need to get the whole label set correct to be correct or is this counted on a ‘per label’ basis’ (and perhaps drop the explanation on page 11 of the usual precision and recall)?  Also, perhaps illustrate the different F1 scores using a small example?   

Page 10: why is Table 2 and Figure 7 restricted to Electra - or - how are these for Roberta? 

Page 11: briefly explain what a ‘smaller class’ is? Also, ""some of the predicted subclass relationships can be determined to be correct according to ChEBI, while others are incorrect.” - is confusing: some of these *are* correct according to Chebi - and that’s how the whole evaluation works, right? This needs clarifying, in particular, what is the verdict if, for chemical X, the predicted direct superclass is Y but it should be Y’s sub- (or super-)class Z?   

Page 13 line 47: how many are ‘several’ (see above - would a distribution be interesting?)? 

Section 4.3: did you eye-ball the extended ontology? It seems that one could relatively easily pick some of the new classes and (ask some chemists to) check the classification (even manually this should be feasible for quite a good sample)? 

Page 16 line 1: which part of the OWL DL expressivity is relevant for your approach? Isn’t it restricted to /focussed on/working with the (inferred) class hierarchy and treating the rest of the ontology as a black box? 

Page 16 line 32: "" Visualisations such as those in Figs. 9 and 10b can be used to explain decision made by the model, raise trust in the prediction system, a” this looks a bit like an over-statement to me.  ","gpt-4o-2024-11-20, Jon Wasky",'slightly positive'
3052,"(1) originality

The paper is reasonably original. It details the application of transformer networks to an ontology extension task in the domain of chemical classification.

Transformer networks are well studied by now and the paper does not make a technical contribution at that level. So are the tricks around using attention for some form of ""interpretation"" of results. The paper is really an application paper with some smaller interesting choices such as tokenization.

Primarily, I think the paper is interesting because of the domain it addresses and the progress the model is making. I think that this could be interesting for ML practicioners in the biomedical field.

The part that is most questionable in my view is the discussion about interpretability/explainability. I'd suggest to use the correct term here - which I think is ""interpretability"" and de-emphasize the contribution. It's not clear to me from the paper that the results really are intereprable in any meaningfull sense. For that argument to hold - authors would have to show additional data that this is indeed convincing experts. Otherwise, it's just a story about interpretability. Within the context of the paper the attention analysis makes sense - as it shows the model has learned something about the domain. But for it to be truly about interpretability or even explainability requires additional experiments of the model with experts (which I didn't read in the paper)


(2) significance of the results

Results present a statistically significant increase over the state of the art.

The state of the art though hasn't seen much coverage by other attempts. It seems to be mostly a niche that the authors are occupying. That's obviously not necessarily a bad thing or something the authors can change - but it does limit the overall significance of results.

It would be good to see more date about the choices - i.e. ablation studies w.r.t to tokenization and other choices

How were hyperparameters choosen?

(3) quality of writing

Generally the writing is good. It's relatively easy to understand and follow.

However in certain cases authors struggle to get to the point and explain why we need to understand a particular part of the system. I'd suggest to work more with tables and also subparagraphs to structure especially the technical sections. Separate the description of the current system from motivation. This is of course subjective - but I'd rather know quickly what the system is and then get a discussion/motivation of design choices. In part the paper reads like a story of what was tried - which is fine but that clouds the details of what was actually used in the paper. There are different parts where I thought the paper could be significantly clearer. 

a) the data part 3.1 is quite confusing. it might help to have a table - clearly delineating the different datasets used in the experiment and then the dataset constrcuted from these and then which model was trained with which OR at least organize the section to clearly explain one after the other

b) the whole tokenization part was very difficult to follow. not because the topic is difficult but because we are getting a lot of information about word vs character-level tokenization and then BPE that ultimately it was not very clear which was used. I suggest removing large part of the discussion or make it very clear what was chosen and then justify. 

c) the model part has a similar structure. I'd prefer to have a clear statement about what the models are and then a paragraph on each model. then motivation for different choices


(4) Other comments

almost all figures have very small legends and many are missing clear x,y legends

Section related work. the last paragraph of ""text-based"" seems to really be better placed in the subsequent section

I don't understand why Roberta is introduced if there are no results discussed

Since you really only have one baseline it might make sense to have that model available in an Appendix or at least offer a short description of the differences - e.g. tokenization if any

Figure 7 (a,b) what's x axis? I assume it's the classes but how did you order them?
Figure 7 (c) to me the color doesn't look red but orange
Figure 8 is confusing. what's on the x axis?
Figure 11 right. I am not sure a scatter plot without any ordering is really informative

Section 5 - unsupervised In ML terms you are not doing unsupervised learning - even if labels come from the ontology.
Section 5 - interpretability - see comments above. in this section it seems you are saying that indeed this doesn't actually work in terms of interpretability

Section Explainability - please rename (see comment about explainability/interpretability)





Assessment of  “Long-term stable URL for resources”

(A) Data file is well organized and in particular contains a README file which makes it easy for you to assess the data

(B) The provided resources appear to be complete for replication of experiments

(C) The chosen repository is Zenodo and appropriate for long-term repository discoverability

(D) Data artifacts seem complete","gpt-4o-2024-11-20, Jon Wasky",'slightly negative'
3052,"This submission presents a new approach to automatically extending the ChEBI ontology via deep learning models. 

Overall, this is a very well-written paper that is, I think, a good contribution to the current state-of-the-art in the area. The approach presented by the authors has its drawbacks and limits, which the authors themselves point out while nonetheless making a solid case for the value and potential of their approach. 

Therefore, I think that this contribution can be accepted. 

Some minor comments follow: 

- Page 7, line  37. just before Section 3.2: ""we created a second dataset without restricting to a maximum of 100 members..."" I think that here it was intended ""... to a *minimum* of 100 members""? Above (line 28 of the same page) we are requiring at least 100 individuals per class.

- Page 11, line 4: ""Yet, the model show overall good performance"". This seems to be very much class-dependent. For some classes, this model clearly does not seem go give very useful predictions at all, while for others it seems to be pretty reliable. It would be interesting to look further into for which kinds of classes this approach works and for which ones it doesn't (the authors mention structural features cycles being potentially problematic, for example). 

- Page 12: ""Figure 10 illustrates a selection of the attnetion weighs..."" if I'm not mistaken, both Figure 9 and Figure 10 do that, and the following discussion is about both. So I  would say ""Figures 9 and 10 illustrate...""

- Page 16, line 1: ""The system extends the given ontology using the same ontology language that has been used to build it"". This is true, but it does not use all the features of the language that are used by the original ontology: for example, the original ontology makes use of disjointness axioms, but the system cannot introduce novel disjointness axioms but (if I'm not misunderstanding it) merely assigning new molecules to given classes. Despite the approach of the authors being, I think, interesting and valuable, this is a limitation that I think is worth remarking.","gpt-4o-2024-11-20, Jon Wasky",'slightly positive'
3067,"The paper discusses how to improve data quality of maintenance working orders (MWO), through the comparison of the output of a (previously developed) NLP pipeline with the output of an inference process carried out using OWL and SWRL-rules reasoning in the context of an application ontology about maintenance activities.

This topic is appropriate for the journal “Semantic Web Journal”, as it fits the journal’s stated goal of publishing “descriptions of concrete ontologies and applications in all areas”, as well as the topics “Ontologies for knowledge representation and reasoning about topics relevant for industrial engineering” and “Experiences with research and application initiatives” of the Special Issue on Semantic Web for Industrial Engineering: Research and Applications.

* Main arguments in favor of accepting the paper

This is a nice paper with interesting observations and results. The authors show good knowledge of the domain and a clear understanding of the practical problems they aim to solve. The original results are relevant and useful to the community. Overall, the paper is clearly written, fairly well organized, and presented. There are some typos and some repetitions, which are highlighted in the attached pdf document.

The paper is fairly well-positioned among previous literature on the same topic, and clarifies which parts are innovative: the contributions of the paper are, essentially,
•	an ontology of maintenance activities based on ISO 14224, ISO 15926-14, and analysis and classification of verbs used in MWO records;
•	a methodology, based on the comparison between NLP and OWL/SWRL inference outputs (the latter depending on the maintenance activities ontology), that identifies data quality issues with MWO records.

The paper begins by describing the current situation in the literature and explains the use case data. Then the paper details the derivation of the maintenance activity ontology, how it can be used to check data quality of MWO records, and finally the results that were obtained by applying this methodology to the use case

The paper focuses on data quality assurance, which is an important topic in industrial engineering, and it does so by managing a use case in the maintenance domain. The result is a methodology that, although studied in the paper with a limited scope (the “Centrifugal Pump-Motor System”) and applied to a small number of records, seems to be generalizable to a wider number of engineering systems and report records.

*Main arguments that require a revision

There are some issues regarding the functional breakdown of the centrifugal pump: the centrifugal pump (http://www.semanticweb.org/asset-list-ontology#Pump) is considered a material artifact, which has, at all times, the various subsystems (Control and Monitor system, lubrication system, etc.) as parts. The subsystem themselves are still material entities, which have other material artifacts as parts, at some time. But,
•	The hierarchy is not reported in the OWL ontology “functional-breakdown-pump-ontology”, as the smaller material artifacts are not part of the proper subsystems, but only of the centrifugal pump. This results in a weaker ontology than stated in the paper, where the authors write: “The subunits, modelled as Engineered System classes under the BFO object aggregate hierarchy, have continuant parts at sometime, e.g., the Control and Monitoring System having further continuant parts such as a Pressure Switch and other types of instrumentation.”
•	In BFO material entities must have at all times some amount of matter as part. Using the authors’ ontology we know that, say, the Driver and Electrical subsystem has a Motor, Variable drive, and Power supply as parts at some time. The fact that the motor etc. are parts only at some time is due, presumably, to the attempt of the authors to consider the possibility that motors etc. are routinely replaced and can be missing from their “functional location” for some time. Thus, the ‘at some time’ phrase is important, and cannot be disposed of. Yet, material entities in BFO must have a material constituent at all times: if the motor etc. of the Driver subsystem is missing, what is the material constituent of the subsystem?
The authors are aware of this problem (they “acknowledge that the idea of replacement is a deeply philosophical problem and raises many questions about identity”) and that the problem of an asset’s functional breakdown is a “complex topic to address in BFO” but do not suggest a viable suggestion leaving the reader wondering if the proposed ontology would remain consistent in real scenarios.
•	The authors make heavy use of the concept of function, functional part, functional location, etc.. Formally, the authors take the concept of function from BFO, where it is a subcategory of dispositions. There are some issues with that, for example, “In BFO, there is no such concept [of functional object and functional part] and this makes it difficult to distinguish where the replaced asset sits in a functional breakdown (as opposed to a physical breakdown)”. That is, it seems that one cannot carry out a functional decomposition (“functional breakdown”) of an engineering system even though this is important for the approach to work.
•	Another difficulty linked to an underdeveloped function theory is the non-distinction between subtypes of functions. For example, the natural language definition of functional role (http://www.semanticweb.org/functional-breakdown-pump-ontology#FunctionalComponentRole) says: “Fhe[sic] role that a material artifact bears if it is a critical part of the larger system. For example, a pump cannot operate without a motor. Therefore, the motor will play a critical component role in the pump system”. This definition apparently is speaking of essential or primary function roles more than simply function roles. Indeed, in the functional-breakdown-pump-ontology the lubrication system has no functional role, which is counterintuitive. Maybe the authors aim to distinguish primary and secondary functions? The paper should provide some information on how this could be done.

In any case, it would be difficult for the authors to do better since the development of an ontological understanding of functionality (etc.) is still an ongoing research line. Moreover, today BFO shows still has relevant limitations on this aspect compared to, say, YAMATO and DOLCE.

The check vs. inspect problem (pg. 10) can be seen as a classification tension. On the one hand, there is the classification of an activity according to the postconditions it generates – the goal of a check is that the system’s state is known (call it the goal view), and this conflates check and inspection into a single activity. On the other hand, there is the classification of activities according to what motivates them - a check is motivated by, say, a fault; an inspection by, say, a temporal flag. In the latter case check and inspect are distinct activities. The existence of the two views could be presented in the introduction to clarify and motivate the classification choice made in the paper.
This is also related to the overhaul vs repair problem where in this case the authors choose the goal view (or to look at the “bare” action, which here amounts to the same result). 
To make the different choices clear and comparable (and possibly coherent), the authors could anticipate the flowchart of Fig. 2 at the beginning of the paper (perhaps simplified as at that point the different activities are not yet identified) and use it to list the criteria that emerge from the ideal use of the ontology.

“These 230 [actually 221, see pdf] terms were then clustered according to whether they describe similar activities.” Following the previous point, here one expects to know the adopted criteria for similarity. Are they the same across all the clusters? Since the decision was made by a subject matter expert, which standards was the subject matter expert familiar with? Did s/he use any? Did s/he participate in the term elucidation writing? Is this person a co-author? If yes, please identify her/him. If not, did the authors double check the quality (and coherence) of this clustering?

On defining “Inspect” (Table 7). We see two problems here. 
Problem 1: the elucidation essentially says that the inspection has the goal to observe the state s of the item. The semi-formal def. talks about capability. Do you assume that states and capabilities are the same thing? If so, explain. If not, fix the definition.
Problem 2: A preventative strategy ps is a procedure that describes a series of activities among which some can be inspections. The strategy itself may prescribe a type of inspection that should be executed (and usually also when), but it does not prescribe the specific activity p (which is a token). In other words, the last condition in the elucidation should say something like: p is an activity of type i and the strategy ps prescribes to execute an activity of that very type.
[this type/token problem applies to the def. of Service as well.]

On defining “Diagnose”: 
“Able to regain function by…” double check the expression in the original source.
The elucidation for diagnose does not seem to match the definition. In an electric machine the company might perform a diagnose activity to find out why it is not working (by def. this is not an inspection since it is caused by a failure, the machine is not working). After checking a few components, the technician my realise that there is no electricity in incoming line. So, the diagnose result is “no degraded state” and “no failed state” for the machine. Of course, there will be another diagnose activity for the electric system but that is, indeed, another activity since it applies to another system. The activity run on the machine is a diagnosis according to the given semi-formal def. of diagnose (and the SME Def. as well) but not according to the elucidation. This should be fixed.

According to sect. 4.2, the definitions in Table 7 aim to be mutually exclusive but the elucidations do not enforce this from the logical viewpoint. In particular, if a device has both a functional and a control role, can the same activity be both an adjust and a calibrate activity? Does one need to use the terminology only relatively to a specific granularity where one can distinguish a component devoted to control only and another to the function execution only? What if the system is an embedded system (meaning, it cannot be separated)?
(Some of these issues are discussed in sect. 7 as if they were problems related to application aspects. Yet, in some cases the cause can be traced back to these definitions and elucidations which, as shown above, are the source of a few “grey zones”.)

Additionally,
•	The elucidations of maintenance activities mention often the term “functioning process”. Is it a technical term in BFO? It should be clarified. Similarly for the other relevant terms recurring in the elucidations.
•	The authors state “As such, they [the classes used for automatic classification] should be broadly applicable to categories of equipment or, at the very least, the classes of pump other than centrifugal pump”. This is convincing for, e.g., the class “Inspectable Unit”, but less convincing for the class “Not Pump Unit System”. In fact, the presence of rules depending on whether an object belongs or not to a given class (in this case the object is not a pump unit) seems an obstacle to scaling up the number of rules.


Attached Software
-The rdf serialisation of the paper ontology is readily accessible on GitHub.
-The files and data present on GitHub are well-structured and documented.
-The steps necessary in order to replicate the paper results are clearly stated and easily carried out.

-We reproduced the population script. The script works fine, but on GitHub there is an additional file “populated-data.owl” that is not produced by the script. Additional comments below.

 -We reproduced the reasoning script. The script works fine. Additional comments in “Suggestion for revision”
 


Further issues

-See file “swj3067_comments_review.pdf” for typos.
-Abstract is too long
-The term artifact in the paper is misleading, it should be technical (or engineering) artifacts.
-In the maintenance activity ontology there is the following taxonomy excerpt:
'Maintenance Type'
	'Corrective Maintenance Type'
	'Preventative Maintenance Type'
where 'Maintenance Type' is the field of a MWO that takes as values either “preventive” or “corrective” depending on the Work Order Type. Presumably, when the authors inserted 'Corrective Maintenance Type' and 'Preventative Maintenance Type' subclasses they meant to say “all the Maintenance Type' fields that take as value only “corrective” ” and “all the Maintenance Type' fields that take as value only “preventive” ”. But, as it is, it seems that  'Corrective Maintenance Type' and 'Preventative Maintenance Type' are full-fledged fields themselves, on par with 'Maintenance Type', ‘Material Cost’, ‘Labour Cost’, etc..
We suggest simply removing the two subclasses.
-Add reference to the fact that an asset’s functional breakdown is a “complex topic to address in BFO”.
-typos/comments/errors for population_script/script.py:
•	Line 12: “funtional_breakdown_onto” –> “functional_breakdown_onto”
•	Line 17: delete
•	Line 75,110; “# sub_unit_indiv = select_sub_unit(row['NLP Identified Subunit'])” –> delete
•	Line 175: “# todo: figure out how to make a date type” –> We see that in the ontology the dates are labeled as xsd:dateTime, so maybe it’s ok.
•	The script was originally thought as having a loop in which each of the 36 MWOs was to be read from the master datasheet and copied into an owl ontology file. At some time the authors realized that there were problems with “owlready chaching” and had to take the loop to another python script (“population_script/runner.py”). The original loop is still in population_script/script.py, though, and a filter was added in the old loop in order to skip all loop iterations except for one. This is not ideal, and the authors could remove the old loop.
•	The authors state “To achieve this, we store the NLP Identified Activity from Table 4 as an annotation property on the Maintenance Work Order Description”, then proceed to show a figure (Figure 1) where “NLP identified Activity”, “NLP identified Item”, “NLP identified Subunit” are annotation properties. The authors highlight this by saying “We represent this information in annotation properties because they represent assumed knowledge resulting from an entity recognition algorithm applied to the specific field. Thus, we cannot assert this knowledge as individuals in the ontology without a detailed ontological analysis”. But in the owl serialisation present on GitHub (e.g., in populated-data-1.owl) “NLP identified Item” and “NLP identified Subunit” are object properties and “NLP identified Activity” is a data property. This is, at least in our understanding, a contradiction between the paper and the owl serialisation, and should be corrected.

-typos/comments for reasoning_script/runner.py:
•	Line 9: “PREFIX bfo:<http://purl.obolibrary.org/obo/>” –> prefix not needed by query
•	Line 20: “ # FILTER NOT EXISTS { ?activity a/rdfs:subClassOf* macr:UncertainActivity }” –> remove comment of old query version
•	Line 26: “ # a/rdfs:subClassOf* activity:replace .” –> remove comment of old query version
•	Line 39: “PREFIX bfo:<http://purl.obolibrary.org/obo/>” –> prefix not needed by query
•	In the first query the prefixes “macr” and “work” are used, while in the paper the same query uses only the prefix “rule”. This works fine, but it is a small inconsistency in notation.
•	The script runs fine, but it is very slow (about 1 minute per record, on my machine), and the reasoning is even done twice (“There is also some inconsistency in the output occasionally (exact cause is unknown) where the activity classifications are not inferred and so the script will run the pellet reasoner twice if it detects such an occurrence”, as per the authors’ explanation – which is correct, and we also have no idea about the cause). This is not unexpected, but perhaps should be mentioned, for, suppose that a company has, say, 500 machines, each one having a history of 30 records, then, assuming one minute per record, the verification of the 15000 records should take more than 10 full days. Additional records may be verified as they are added, so they shouldn’t be too much of a problem, still, if a company would attempt to use the reasoning algorithm outlined in the paper massively on its records, it may encounter severe computational requirements. Maybe this fact should be mentioned and/or some possible solutions suggested.
","gpt-4o-2024-11-20, Jon Wasky",'slightly positive'
3068,"The article aims to investigate how ontology design patterns for event representation impact the performance of natural language (NL) query interfaces. Given a complex event structure, we can expect that different event modelling patterns with various ontologies, such as those reviewed in Section 2.2, will impact the performance of semantic question answering systems that translate NL-queries into SPARQL. 

However, the article lacks a novel contribution. Evaluation metrics do not directly measure the effect of the event representation on the performance of the NL-2-SPARQL query mapping, which is a central aspect to investigate in this context. Instead, some proxy metrics such as SPARQL execution time are used. However, these proxy metrics refer to the SPARQL rather than NL-queries. 

Furthermore, the authors evaluate different event-modelling design decisions in a specific semantic search framework (SBVR). The discussion is tightly connected to the SBVR framework and limited by it, and experiments are limited to a few particular event types. It is unclear how generalisable the discussion of performance (e.g. SPARQL query execution time) is to other systems and event types. ","gpt-4o-2024-11-20, Jon Wasky",'slightly negative'
3068,"The paper presents an evaluation of different ontology representation patters and how they affect SPARQL query performance, data import operations, the size of a triple store and the size of a query. The results of the evaluations can be helpful to many but are limited to only the GraphDB graph database. I can see the contribution of this work for future research but there are some inconsistency regarding its presentation overall. Although the level of English is good, I found it hard following the content at times. See below for more details.

The introduction can be more consice (i.e. present the main research motivation, research questions and your contribution). It is mentioned that you focus on different events. Later, only the event of ""talking"" and its types is mentioned. It would be good to name the specific types of NLQI systems did your work focused on.

In related work section, sometimes it is unclear if you review someone else's work or your own previously published work. It should be made clearer what you present from already published work and what you present now as a new idea in this manuscript. For example, if the current proposed work is an extension of an existing solution. Several references are missing in this section. Namely of OWL, OWL2, OMG. Further, these abbreviations should be mentioned in the introduction and used consistently through the paper. 

The beginning of Section 2.2 sounds more suitable for the introduction. It would be good if it is integrated there. When reviewing the SEM ontology, it should be explained ""minimum semantic commitment"" means. The names of the reviewed ontologies, when having an abbreviation, should be spelled out first in full. See LODE and its full name in the references. In Section 2.3, what are the pros and cons that have been identified? 
In some places, when using ""both ontologies"", it is not clear to which two ontologies it is referred to. It would be good to see the actual namespaces of the defined concepts.

In Section 3.1, are the mentioned questions the actual competency questions that was refered to in the introduction of Section 3?

In Section 3.1, first sentence, what fragment of which ontology was reused?

Missing reference of GraphDB in Section 3.3. Further, it was not clear which version of GraphDB was used? The free or the licensed one as each supports different functionalities. Was it a local instance of it or on a server?

It would be good to present the recommendations in a more clearer way. Maybe a separate sections even or a table to refer to.

Section 5 has a paragraph with only one sentence. 

There is a dot after each reference number, which should not be there. Were references [12], [23], [28] all accessed in 2020 (almost 2 years ago)?

The provided GitHub repository is well structured and has a good description.


Overall, the proposed paper is interesting but needs improvements if it is to be published.","gpt-4o-2024-11-20, Jon Wasky",slightly negative
3081,"The authors have done very good work in analyzing different perspectives of functions and related concepts in engineering design, and presenting their own contributions in formalizing those notions. This is an important topic in engineering design that could definitely need help from the ontology and Semantic Web communities, as exemplified by the current work. Based on this merit, I think the work is worthy enough for publication. However, I have a few suggestions for improvement as follows. I'd especially request the authors to consider the last point made in this review.  

I have an issue with the phrase ""systemic functions"" because the term ""system"" is also a very broadly used term that is not easy to define nor formalize (as the authors also recognize later on). Hence, by including this term as part of the phrase, the authors are adding more ambiguity. Could the authors think of an alternative phrase? Otherwise, does a screw (typically not considered as a system) not have a ""systemic function""?

I quite enjoyed reading the literature review -- it is not simply a cursory review of the prior work but highlights the subtle differences between all the prior work and sets up nicely for the work presented by the authors. However, I suggest a couple of improvements:

1. Another important related notion to functions besides capabilities is ""affordance"" in engineering design (see Maier and Fadel's work). The authors should provide some literature review and discussion on this topic, and preferably incorporate the analysis of this notion in their work.

2. While BFO may lack axiomatization of functions, Barry Smith has a number of papers on functions that are worth reviewing here. How does BFO's perspective on functions differ from the authors?

In Section 3, the authors should justify why DOLCE was chosen as the top-level ontology. Also, an introductory paragraph of why the three concepts (qualities, perdurants, and roles) are related to functions would be beneficial for the reader to follow along with the details of each concept presented.

I don't think color is a good example of intrinsic qualities because it is measured indirectly using light and is still a contentious topic for ontological debates. One could argue that there is no color if there is no light, the same as there is no weight if there is no gravity. Why not stick to more obvious examples of physical or material properties?

The analysis of behavior and the justification given to classify it as a perdurant was very well done. The section also leads to an important discussion on states and state variables. I wonder if the authors could expand or elucidate the definitions of behaviors using these notions. The reviewer (with an engineering background) assumes that most engineers model physical behaviors with equations and the values of their variables determine a particular state. They then use these equations to analyze or predict how well the artifact would realize its functional requirements. I think if the authors could bridge these notions (behaviors, states, and modeling equations), it could be another important contribution of the paper.

I wonder if the distinction between ontological and systemic functions is simply prescribed vs. actual (realized) functions. I'd like to hear the authors' thoughts.

I'm a bit concerned that the notion of requirements is not discussed anywhere in the paper, especially in Section 4. In practice, engineers deal with requirements more than functions. Could the authors discuss or define how they are related?

Understanding that OWL has limited expressiveness, why did the authors choose OWL to implement their ontology? Provide some justification in the paper.

There is a bit of irony in the engineering design practice regarding functions. While many people in academia emphasize the importance of functional modeling and decomposition and teach their students related methods, almost no one in the industry practices them. In fact, most design practices do not start from a blank state where desired functions are thought out, but rather with an initial design from which to improve. I wonder if the authors have thought about how their efforts could resolve this issue or help in any way. One could perform this type of work for the sake of formalizing ""functions"" and related notions, but for what practical benefits does it serve? How do the authors envision that their work could actually improve the engineering design practice? Can the authors identify any use cases, motivating scenarios, or competency questions that their ontology could address in real engineering design practices? I believe this should be addressed to make this type of work much more meaningful and significant to the engineering community.
","gpt-4o-2024-11-20, Jon Wasky",slightly positive
3081,"The paper introduces an ontological description of functions, behavior, capability, and capacity. The proposal extends DOLCE ontology. 
The article clearly describes the concepts in natural language, formalizes them using first-order logic, and then implements these formalizations using OWL. 
Although many proposals define functions, the approach that is introduced in the article is original and is well ontologically founded. 
An interesting issue of the article is that its formalization allows the implementation of the concepts so they can be used in practice.  
The proposal is general enough to specialize in several domains in a straight fashion.
The article is well organized, and the quality of writing allows the reader to understand the proposal.
The authors provide an access to a Github repository with the proposed ontology. It is complete to replicate the queries introduced in the article (I have some problems running the first query).  ","gpt-4o-2024-11-20, Jon Wasky",'slightly positive'
3084,"This manuscript was submitted as 'Ontology Description' and should be reviewed along the following dimensions:  (1) Quality and relevance of the described ontology (convincing evidence must be provided). 

This paper provides an ontology for image schema using Framnet semantics. The ontology can be queried from Framnet's endpoint. Image schema is a common and important cognitive instrument for sentence making and creating language expressions. Hence, this newly created ontology can be highly useful and relevant to the readers of this journal. 


(2) Illustration, clarity and readability of the describing paper, which shall convey to the reader the key aspects of the described ontology. 

The revised version of this paper is clear and well written. 


Please also assess the data file provided by the authors under ""Long-term stable URL for resources"". In particular, assess (A) whether the data file is well organized and in particular contains a README file which makes it easy for you to assess the data, 
There is a readme file in the github repo with example queries. No data file is provided, but the entire ontology is accessible through Framester's hub. 

(B) whether the provided resources appear to be complete for replication of experiments, and if not, why, 
It is sufficient. 

(C) whether the chosen repository, if it is not GitHub, Figshare or Zenodo, is appropriate for long-term repository discoverability, 
yes. 

and (4) whether the provided data artifacts are complete. Please refer to the <a href=""/reviewers"" target=""_blank"">reviewer instructions</a> and the <a href=""/faq"" target=""_blank"">FAQ</a> for further information.

The current work only includes 6 types of image schemas. While there are other types of image schemas, at least we have a clear idea of what is covered. 


","gpt-4o-2024-11-20, Jon Wasky",'slightly positive'
3087,"This paper has made some improvements over its original form. Principally, it added more backgrounds of the way of proposing the model, e.g., involved institutions and survey table, and more technical discussions on interoperability. I read through authors' responses to all reviewers' comments as well as the revised paper. Frankly speaking, I do not think the authors fundamentally addressed my, as well as the other two reviewers'(especially the first reviewer's) comments. Plus, there are many duplicated responses across the letter, and they are very general, even though IMHO the comments asked different things.

More concretely, I am barely satisfied by the response to my comment related to how ""specified/special"" such a model is for precision agriculture and livestock farming. My comment was more to challenge the motivation and innovation of the model. Put differently, if one was asked to create a meta-model for, say urban-related data, I think a very similar model can be proposed. Then what is ""new"" about the proposed meta-model? In my humble opinion, the problem might boil down to the fact that the meta-model is largely a ""merging"" of different ontologies. There is nothing wrong about it. I totally support reusing rather than creating something new if there already exits something we can apply. But I am skeptical about the originality of such a paper if its main contribution is this meta-model. Simply put, I believe this paper can be regarded as (maybe) a ""best practice"" of using DCAT, PROV, and QB for metadata modeling in agriculture and livestock farming, with some minor model modifications (the SHACL shape to connect DCAT and QB for example). Then, the authors might consider to more focus on the use cases rather than the detailed technical modeling. Plus, I believe my concern echoes Reviewer 1's comment. The authors' very general responses using (1) the four categories of metadata (already widely known), (2) the support of ""data alignment"" (did not specifically discuss though), and (3) the support of ""interoperability"" (similar to alignment actually; too general that one can fairly say all semantic models have the same goal), are hardly satisfactory in my opinion.

This concern is also reflected in the fact that many contents (sections) in the paper are loosely connected with the proposed meta-model. For example,  even though the survey form is provided, how does it specifically help to build the model? For example, is there any useful answer that persuade you to make some specific modeling decisions in the domain of agriculture and livestock farming while using DCAT/PROV (the paper actually mentioned a little bit on it, but I am still curious how different it would be if the domain changes)?  How does the category of different data sources in Section 4.2 related to the modeling? The authors states that ""... to understand the nature and structure of the data... not intended to provide a taxonomy of the data"". Then how does such an understanding helps build the model? 

Plus, in related work, the four categories of metadata have already been discussed, why in Section 5.1, it is discussed again? Also, I disagree with the author that a diagram of the involved conceptual model is unnecessary and can already be seen from Fig. 2. I believe they should be quite different things. For example, what is the conceptual contribution of this paper rather than merging several different existing models (like Fig 2)? In section 4.3 (line 39-46), aren't the three identified scenario/cases to summarize the benefits/motivations of the proposed model the same (i.e., integrate and query heterogeneous data)?

In summary, this paper introduced many detailed and technical aspects of the project, which is exciting indeed. However, I really encourage the authors to think what they really want to highlight in this one paper and what is the major contribution they want to present to the community of agriculture and livestock farming specifically.  
","gpt-4o-2024-11-20, Jon Wasky",'slightly negative'
3090,"This document reports the second version of the work on planning and execution techniques for [R2]RML mapping rules. The proposed method relies on the partition of mapping rules. Evaluating the groups in the partition, reduces the duplicated generation of RDF triples and maximizes the parallel execution of the mapping rules. The proposed techniques are implemented in MorphKGC, an RML-compliant engine; the behavior of MorphKGC is assessed in three existing benchmarks. The reported results suggest that the proposed methods can accelerate the execution of mapping rules as the ones composing the studied benchmarks.

Overall, this second version addresses some comments on the previous version. The new experimental results provide evidence of the benefits that planning the execution of the mapping rules brings to the process of KG construction. Moreover, the competitive behavior with other engines puts the critical role played by logical and physical planning into perspective. Despite the improvements, this new version of this work still presents imprecise statements which need to be addressed in a new version of the paper.

Definitions:

Definitions 1 and 2 are still not well-formulated; please, reuse the notation and conventions in 2.2. The formal proof of whether given a data source when these two mapping documents will produce the same set of RDF triples is necessary; the correctness of the proposed approach dependents on that. 

I do not agree with this statement 
“Answer: We have included an additional function const(.) in Section 2.2 to solve this. Note that `if` here does not refer to logics, we have replaced it with `when` to avoid reader confusion. As the values of a term map must be {constant, template or reference}, the invariant is well-defined with the bullets that consider the three possibilities.” 
Since “when” and “if” represent logical conditionals, clarify the sufficient and necessary conditions in Definition 5.

Proposed Algorithms:

Please, clearly state the assumptions under which Algorithm 3 is able to generate the Maximal Mapping Partition of an [R2]RML document. 

Empirical Evaluation
Please, include absolute values and explain the impact of the selectivity of the joins in the performance of the compared engines. 


Minor comments
SDM-RDFizer v4.1.1 and Chimera v2.1. are interpreters of RML and not only parsers. Please, clarify this point. 
","gpt-4o-2024-11-20, Jon Wasky",'slightly negative'
3090,"The paper presents a method for materialization of triples that are used for knowledge graph construction. Two different algorithms are presented for mapping partitions, which group mapping rules that create triples. The presented algorithms are evaluated on three benchmark datasets and compared against four existing systems. The experimental results suggest that the presented sequential and parallel processing solutions perform better than the compared systems in terms of memory usage and execution time, respectively.

Originality: the paper addresses an important problem of reducing memory usage and execution time for knowledge graph construction. The novelty aspect of the paper lies in the presented two ways of using mapping partitions: parallel or sequential execution.

Improvement: The authors have taken into account the comments regarding the inclusion of additional configurations of the presented model for some experiments, inclusion of a discussion section to finalize the findings from the presented solution. The authors have shared the resources in Zenodo for the reproducibility.

Overall, the paper is well written with clear structures, after the requested changes from the initial reviews.



","gpt-4o-2024-11-20, Jon Wasky",extremely positive
3097,"The manuscript introduces an approach for timestamp-based versioning of RDF datasets. The approach uses RDF-star to annotate every triple of the dataset with a 'valid_from' timestamp and a 'valid_to' timestamp, where the latter may be in the far future to cover cases of triples that are ""valid until further notice.""

The authors describe how to insert and update triples with timestamp annotations by using SPARQL-star Update statements and they describe how to retrieve/materialize a version of the dataset at any given timestamp by using SPARQL-star queries. Additionally, as an evaluation of their approach, the authors have done a simple experiment by using a dataset of the BEAR RDF Archiving Benchmark with some triple pattern queries. The main observations of the experiment are i) that the tested systems (Jena TDB and GraphDB) achieve better query performance for some (unspecified) Named Graph representation of the timestamped data than for two variations of the authors' RDF-star approach and ii) that GraphDB achieves better performance than Jena TDB.

I do not consider the contributions of this manuscript sufficient for a journal article; in fact, I wouldn't even consider them sufficient for a conference paper in the main Semantic Web conferences. Instead, I would consider the contributions more as something for a workshop paper. The proposed approach is just a straightforward application of RDF-star and SPARQL-star, which is not even made transparent for users (instead, users are assumed to include the timestamp annotations and the timestamp-related query patterns manually), and the evaluation is very simplistic and only scratches the surface in terms of insights that it provides about the proposed approach. Moreover, the manuscript contains several small inaccuracies, and several details are missing or are not captured thoroughly.

Having said that, I am happy to see that the authors are attempting this work and I strongly encourage them to expand what they currently have. The remainder of this review elaborates more on the aforementioned issues and provides suggestions to improve and expand this work.

CONCEPTUAL CONTRIBUTIONS

To provide an actual conceptual contribution related to the proposed application of RDF-star and SPARQL-star, I would like to see a well-defined approach to make the RDF-star-based timestamping of RDF datasets transparent to the users. That is, I would like to see a foundation for *automatically* translating any given SPARQL Update statement into a SPARQL-star Update statement that adds or updates the relevant timestamp annotations. Similarly, for materialization-related queries, I would like to see a foundation for automatically translating a SPARQL query, together with a timestamp, into a SPARQL-star query that produces the result of the given SPARQL query over the version of the dataset at the given timestamp.

In addition to materialization-related queries, I would like to see a thorough discussion of how the proposed application of RDF-star can be leveraged for other types of data archive queries (timestamp retrieval, delta materialization, cross-version queries, etc). Related to that, I see that the authors wanted to focus on materialization queries, but I don't see any clearly-stated rationale for this focus; there are some vague references to some recommendations of a data citation working group but no concrete elaboration on these recommendations or no discussion of relevant requirements.

Smaller issues about the description of the proposal:

* While the idea to use the VALUES feature for inserting triples makes sense, I am certain that there is a practical limit to this idea. I mean, it is not possible to bulk load an unlimited number of triples in a single insert statement. This limit should be discussed and may also be something to be studied experimentally.

* The examples for the case of updates focuses only on updating a single triple. It may not be obvious to the reader how the updates are done when updating a combination of multiple triples or when bulk-updating multiple individual triples. Generally, as mentioned above, I would like to see a more generic treatment of how update statements need to be extended with the relevant timestamp-related patterns.

* The proposal for outdating a triple (Sec.3.6) requires that ""an artificial valid_until timestamp must exist on that triple."" While this requirement makes sense, there should also be a statement that specifies what happens, or what should happen, in cases in which the requirement is not satisfied.

* Section 3.7 claims that Tables 5 and 6 represent query results for the queries in Listings 9 and 10. That is incorrect because the values in the ""Predicate"" and the ""Object"" columns of the tables are not returned by the queries.

* The discussion related to DISTINCT at the end of Sec.3.7 does not make sense. Since the queries also contain the second FILTER condition (about ?valid_until), there would be no duplicates and no need for using DISTINCT (at least, if we assume that every data triple has only one vers:valid_from annotation and only one vers:valid_until annotation).

* In this discussion related to DISTINCT, the text mentions a condition with some ""system_timestamp"". It is not clear what that means.


EVALUATION

From a journal article I am expecting a much more comprehensive evaluation than what is provided in this manuscript.

* There is no study of the performance impact of the insert and update part of the proposal, although this part makes up 2/3 of the description of the proposal.

* The file-import part of the evaluation is somewhat unclear (and perhaps misleading?) because the baseline (some Named Graphs based approach) is not clearly defined.

* The evaluation is based on a single dataset and, additionally, there is no justification why only that dataset was used (considering that the BEAR benchmark consists of multiple datasets).

* The triple pattern lookup queries considered in the ""Query Performance"" experiment are a very simple form of queries. The authors do not provide any consideration of the practical relevance of such queries; how much can we actually learn about the approaches from such simple queries?

* There is no discussion whatsoever of the observations that can made from the measurements. Why is there such a huge reduction of the file size and the memory footprint when converting the Named Graphs representation into the RDF-star-based representations? Why do the systems achieve a better query performance for the Named Graphs representation of the timestamped data when compared to the RDF-star-based representations? Why does GraphDB achieve a better performance than Jena TDB? Is the hypothesis that ""we expect a better performance with former ones"" (i.e., predicate-lookup-queries)"" actually verified by the experiments? etc.

* The claim that ""using the proposed approach, even large scale and highly dynamic RDF datasets can be efficiently versioned"" is absolutely not justified by the presented evalution!
--> A few hundred MB are not ""large scale"" and neither are a few GB.
--> Also, there is nothing about ""highly dynamic RDF datasets"" in this evaluation; the authors just imported a file in which multiple dataset versions are represented. (see also my comment above about the lack of an evaluation of the insert and update part of the proposal)

Further smaller issues in the section about the evaluation:

* It needs to be specified which version of each of the systems was used exactly.

* Readers may not know what a "".ttl file"" is and how it may be used to serialize datasets with RDF-star triples. In fact, the last paragraph of Sec.5 makes even me wonder what exactly the authors have done (""Once the turtle-star RDF serialization format becomes widely adopted we will fit our datasets into this format."") Does this mean the authors have not actually used Turtle-star for the serialization, but plain Turtle? How was it possible to represent the nested RDF-star triples then??

* What is the purpose of the ""shell script"" mentioned in Sec.4.1?

* The authors use the terms ""compressed"" and ""compression"" in several places, which is highly misleading because they have not actually used any compression techniques. Instead, there is simply a reduction of the dataset size (measured in terms of file size) after converting data from the Named Graphs representation to the RDF-star-based representations.

* It is not clear how to read things such as ""173-256MB"" in Sec.4.2.

* The authors should clarify what they mean by ""storage consumption scaling factors"".

* Table 7 says ""mean ingestion time"" but I don't see any indication of the number of file-import runs have been tried to calculate a mean from.


RELATED WORK

The ""Related Work"" section needs to be improved as well. Currently, it appears as a semi-organized collection of some related work, mixed up with an introduction of the background of the presented work. Additionally, there is a paper that has two entries in the bibliography (namely, [2] and [17]), and there are entries for which it is not clear where these papers have been published (e.g., [3], [41], [42]).

Finally, I suggest to reference also the W3C Community Group Report about RDF-star and SPARQL-star as this is the most recent document about the approach, and in this context I also suggest to use the terms RDF-star and SPARQL-star instead of RDF* and SPARQL*.","gpt-4o-2024-11-20, Jon Wasky",'slightly negative'
